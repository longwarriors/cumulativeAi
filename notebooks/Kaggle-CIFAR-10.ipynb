{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T17:56:10.694367Z",
     "start_time": "2025-04-03T17:56:10.638009Z"
    }
   },
   "source": [
    "# 从原始图像文件开始读取\n",
    "# 并将它们转换为张量格式\n",
    "# https://www.kaggle.com/c/cifar-10\n",
    "# https://www.zhihu.com/question/54883612/answer/130707137363\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from collections import Counter\n",
    "from sched import scheduler\n",
    "\n",
    "# 数据文件路径\n",
    "data_files_dir = r'../data/kaggle-cifar-10'\n",
    "train_images_dir = os.path.join(data_files_dir, 'train')\n",
    "test_images_dir = os.path.join(data_files_dir, 'test')\n",
    "train_labels_file_path = os.path.join(data_files_dir, 'trainLabels.csv')\n",
    "submission_example_file_path = os.path.join(data_files_dir, 'sampleSubmission.csv')\n",
    "\n",
    "\n",
    "def copy_files(file_path, target_dir):\n",
    "    \"\"\"\n",
    "    将文件复制到目标目录\n",
    "    :param file_path: 源文件路径\n",
    "    :param target_dir: 目标文件夹\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)  # 确保目标目录存在，如果不存在则创建\n",
    "    shutil.copy(file_path, target_dir)\n",
    "\n",
    "\n",
    "def read_csv_labels(file_path) -> dict:\n",
    "    \"\"\"\n",
    "    读取CSV文件，返回文件名到标签的字典\n",
    "    :param file_path: CSV文件路径\n",
    "    :return: 字典\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[1:]  # 跳过第一行标题\n",
    "    # 遍历每一行，将其去掉末尾的换行符，并按逗号分割成列表\n",
    "    tokens = [line.strip().split(',') for line in lines]\n",
    "    # 创建字典，键是文件名，值是标签\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "\n",
    "train_labels = read_csv_labels(train_labels_file_path)\n",
    "print('# 训练样本 :', len(train_labels))\n",
    "print('# 类别数 :', len(set(train_labels.values())))\n",
    "dict(list(train_labels.items())[:10])  # 显示前10个样本的文件名和标签"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 训练样本 : 50000\n",
      "# 类别数 : 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 'frog',\n",
       " '2': 'truck',\n",
       " '3': 'truck',\n",
       " '4': 'deer',\n",
       " '5': 'automobile',\n",
       " '6': 'automobile',\n",
       " '7': 'bird',\n",
       " '8': 'horse',\n",
       " '9': 'ship',\n",
       " '10': 'cat'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T17:56:14.638931Z",
     "start_time": "2025-04-03T17:56:14.629437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 显示标签种类分布\n",
    "Counter(train_labels.values())"
   ],
   "id": "b3679a30619a640",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'frog': 5000,\n",
       "         'truck': 5000,\n",
       "         'deer': 5000,\n",
       "         'automobile': 5000,\n",
       "         'bird': 5000,\n",
       "         'horse': 5000,\n",
       "         'ship': 5000,\n",
       "         'cat': 5000,\n",
       "         'dog': 5000,\n",
       "         'airplane': 5000})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "def reorganize_train_valid(whole_train_dir: str,\n",
    "                           labels: dict,\n",
    "                           valid_ratio: float = 0.2):\n",
    "    \"\"\"\n",
    "    将训练集划分为训练集和验证集\n",
    "    :param whole_train_dir: 训练集目录\n",
    "    :param labels: 标签字典 {文件名: 类别}\n",
    "    :param valid_ratio: 验证集比例\n",
    "    \"\"\"\n",
    "    label_counts = Counter(labels.values())  # 统计训练数据集中每个类别的样本数\n",
    "    min_count = min(label_counts.values())  # 获取样本最少的类别的样本数\n",
    "    valid_count_per_label = max(1, int(min_count * valid_ratio))  # 每个类比在验证集中至少要有的样本数\n",
    "    files_by_label = {}  # 按类组织文件\n",
    "    for name, label in labels.items():\n",
    "        if label not in files_by_label:\n",
    "            files_by_label[label] = []\n",
    "        else:\n",
    "            files_by_label[label].append(name)\n",
    "\n",
    "    for label, names in files_by_label.items():\n",
    "        random.shuffle(names)\n",
    "        valid_files = names[:valid_count_per_label]\n",
    "        train_files = names[valid_count_per_label:]\n",
    "\n"
   ],
   "id": "db5da387e8c72f23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:28:16.822917Z",
     "start_time": "2025-04-03T18:14:23.845953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    \"\"\"\n",
    "    将验证集从原始的训练集中拆分出来\n",
    "    最终会把数据分成以下三类：\n",
    "    - train_valid_test/train_valid/类别（全部数据，包括训练 + 验证）\n",
    "    - train_valid_test/train/类别（训练数据）\n",
    "    - train_valid_test/valid/类别（验证数据）\n",
    "    \"\"\"\n",
    "    n = Counter(labels.values()).most_common()[-1][1]  # 样本最少的类别的样本数\n",
    "    n_valid_per_label = max(1, int(n * valid_ratio))  # 每个类别放入验证集的样本数\n",
    "    label_count = {}  # 用于记录已分配到验证集的样本数\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]  # 去掉文件扩展名，查找标签\n",
    "        fname = os.path.join(data_dir, 'train', train_file)  # 生成文件的完整路径\n",
    "        copy_files(fname, os.path.join(data_dir, 'train_valid_test', 'train_valid', label))\n",
    "        # 如果该类别的验证集样本数未达到`n_valid_per_label`，则放入 `valid` 目录\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            copy_files(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1  # 更新该类别的计数\n",
    "        else:\n",
    "            copy_files(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))\n",
    "    return n_valid_per_label  # 返回每个类别被划分到验证集的样本数量\n",
    "\n",
    "\n",
    "def reorg_test(data_dir):\n",
    "    \"\"\"\n",
    "    将测试集中的文件按照指定的目录结构复制到新的位置\n",
    "    unknown 文件夹表示这些测试样本是未标记的\n",
    "    \"\"\"\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copy_files(os.path.join(data_dir, 'test', test_file),\n",
    "                   os.path.join(data_dir, 'train_valid_test', 'test', 'unknown'))\n",
    "\n",
    "\n",
    "def reorg_cifar10(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "VALID_RATIO = 0.1\n",
    "reorg_cifar10(data_files_dir, VALID_RATIO)"
   ],
   "id": "1789fa06108c6357",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:32:56.101792Z",
     "start_time": "2025-04-03T18:32:49.311325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 图像增强防止过拟合\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomResizedCrop(size=32,\n",
    "                                 scale=(0.64, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "train_ds, train_valid_ds = [\n",
    "    ImageFolder(os.path.join(data_files_dir, 'train_valid_test', folder), transform=transform_train) for folder in\n",
    "    ['train', 'valid']]\n",
    "\n",
    "valid_ds, test_ds = [ImageFolder(os.path.join(data_files_dir, 'train_valid_test', folder), transform=transform_test) for\n",
    "                     folder in ['test', 'valid']]"
   ],
   "id": "7683d26a359304d8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:35.639073Z",
     "start_time": "2025-04-03T18:36:35.630697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iter, train_valid_iter = [DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) for ds in\n",
    "                                [train_ds, valid_ds]]\n",
    "valid_iter = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_iter = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ],
   "id": "a650843e7810e85a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 模型\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "def get_net():\n",
    "    net = models.resnet18(pretrained=False)  # 获取标准的ResNet18模型\n",
    "    net.fc = nn.Linear(net.fc.in_features, 10)\n",
    "    # CIFAR-10图像是32x32分辨率\n",
    "    # 而标准ResNet期望的输入分辨率为224x224\n",
    "    # 需要调整第一个卷积层，将其从7x7卷积（适合ImageNet的大图像）改为3x3卷积（更适合CIFAR-10的小图像）\n",
    "    net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # 移除最大池化层，因为CIFAR-10图像太小\n",
    "    net.maxpool = nn.Identity()\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net,\n",
    "          train_iter,\n",
    "          valid_iter,\n",
    "          num_epochs,\n",
    "          learning_rate,\n",
    "          wd,\n",
    "          devices,\n",
    "          lr_period,\n",
    "          lr_decay):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    # 每隔 lr_period 轮对学习率进行衰减（乘以 lr_decay 的值）\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, lr_period,lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()"
   ],
   "id": "9b472a07705a6c92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
