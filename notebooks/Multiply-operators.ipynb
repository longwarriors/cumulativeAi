{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T09:25:20.395937Z",
     "start_time": "2025-04-17T09:25:17.288298Z"
    }
   },
   "source": [
    "# https://zhuanlan.zhihu.com/p/559824020\n",
    "import torch\n",
    "\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'torch version: {torch.__version__}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "torch version: 2.6.0+cu126\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T12:46:25.087720Z",
     "start_time": "2025-04-16T12:46:24.759791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''tensordot\n",
    "用于缩并/扩充多个维度\n",
    "'''\n",
    "# 1. 最理想的情况 = 张量积：X^{abcd} * Y^{ef} = Z^{abcdef}\n",
    "x = torch.rand((3, 4, 5, 6, 7))\n",
    "y = torch.rand((8, 9, 10, 11))\n",
    "assert torch.tensordot(x, y, dims=0).shape == (3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
    "\n",
    "# 2. 只缩并一个维度 X^{abc d} * Y^{d ef} = Z^{abcef}\n",
    "a = torch.rand((7, 8, 9, 10, 11))\n",
    "assert torch.tensordot(x, a, dims=1).shape == (3, 4, 5, 6, 8, 9, 10, 11)\n",
    "\n",
    "# 3. 只缩并两个维度 X^{ab cd} * Y^{cd ef} = Z^{abef}\n",
    "b = torch.rand((6, 7, 10, 11))\n",
    "assert torch.tensordot(x, b, dims=2).shape == (3, 4, 5, 10, 11)\n",
    "\n",
    "# 4. 只缩并三个维度 X^{abc def} * Y^{def g} = Z^{abcg}\n",
    "c = torch.rand((5, 6, 7, 32))\n",
    "assert torch.tensordot(x, c, dims=3).shape == (3, 4, 32)\n",
    "\n",
    "# 5. 无限推广后缘维度缩并\n",
    "\n",
    "# 6. 指定维度角标缩并\n",
    "g = torch.rand((3, 10, 4, 12, 11, 5))\n",
    "h = torch.rand((12, 2, 6, 11, 10, 7, 8))\n",
    "\n",
    "g_dims = [3, 4, 1]\n",
    "h_dims = [0, 3, 4]\n",
    "assert torch.tensordot(g, h, dims=(g_dims, h_dims)).shape == (3, 4, 5, 2, 6, 7, 8)\n",
    "\n",
    "g_dims = [3, 4]\n",
    "h_dims = [0, 3]\n",
    "assert torch.tensordot(g, h, dims=(g_dims, h_dims)).shape == (3, 10, 4, 5, 2, 6, 10, 7, 8)\n",
    "\n",
    "g_dims = [3]\n",
    "h_dims = [0]\n",
    "assert torch.tensordot(g, h, dims=(g_dims, h_dims)).shape == (3, 10, 4, 11, 5, 2, 6, 11, 10, 7, 8)\n"
   ],
   "id": "4d4ba023c03ad1a4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:07:13.669760Z",
     "start_time": "2025-04-17T10:07:13.661287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''multiply (__mul__ / *)\n",
    "最自然的操作：\n",
    "- 1. 对应位置元素相乘\n",
    "- 2. 一个数乘以数组的每个元素\n",
    "\n",
    "广播机制下的操作：\n",
    "- 3. 扩维拷贝后相乘\n",
    "'''\n",
    "# 1. 对应位置元素相乘\n",
    "x = torch.randn(6, 4, 13, 5)\n",
    "a = torch.randn(6, 4, 13, 5)\n",
    "assert torch.mul(x, a).shape == (6, 4, 13, 5)\n",
    "\n",
    "# 2. 广播机制是从最后一个维度开始对齐\n",
    "x = torch.randn(6, 4, 13, 5)\n",
    "a = torch.randn(      13, 5)\n",
    "assert torch.mul(a, x).shape == (6, 4, 13, 5)\n",
    "\n",
    "# 3. 检查到不匹配的维度为1就会在这个维度上复制到匹配\n",
    "x = torch.rand((3, 1, 2, 5))\n",
    "a = torch.rand((3, 6, 2, 1))\n",
    "assert torch.mul(x, a).shape == (3, 6, 2, 5)\n",
    "\n",
    "# 4. multiply广播机制等价的循环\n",
    "a = torch.rand((3, 1))\n",
    "b = torch.rand((1, 6))\n",
    "broadcast_mul = a * b\n",
    "loop_mul = []\n",
    "for i in a.squeeze().tolist():\n",
    "    for j in b.squeeze().tolist():\n",
    "        loop_mul.append(i*j)\n",
    "loop_mul = torch.as_tensor(loop_mul).reshape(a.numel(), b.numel())\n",
    "torch.equal(broadcast_mul, loop_mul)"
   ],
   "id": "81b7dacd0d073515",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:27:53.915825Z",
     "start_time": "2025-04-17T10:27:53.912044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''matmul\n",
    "广播机制下的操作：\n",
    "- 1. 高维张量相乘，大于倒数两个维度的维度就是batch维度，不参与运算，只要求对齐\n",
    "- 2. 需要保证前一个张量的最后一个维度`shape[-1]`和后一个张量的倒数第二个维度`shape[-2]`保持一致\n",
    "- 3. 如果后一个张量是向量就是倒数第一个维度\n",
    "\n",
    "情况分析：\n",
    "- 1. 矩阵乘矩阵：left.shape[-1] = right.shape[-2]\n",
    "- 2. 向量乘矩阵：left.shape[-1] = right.shape[-2]\n",
    "- 3. 矩阵乘向量：left.shape[-1] = right.shape[-1]\n",
    "- 4. 向量乘向量：left.shape[-1] = right.shape[-1]\n",
    "\n",
    "matmul广播机制存在的缺点：\n",
    "matmul广播机制是采用copy data来expand到相同维度尺寸\n",
    "因此存在broadcast的matmul性能还不如einsum\n",
    "尽量自己手动broadcast一下，避免触发matmul的广播\n",
    "'''\n",
    "# 1. 向量乘矩阵的维度缩并\n",
    "vec = torch.rand(3)\n",
    "mat = torch.rand((3, 5))\n",
    "assert vec.matmul(mat).shape == (5,)"
   ],
   "id": "1c48b5c54fa12e98",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 矩阵乘向量\n",
    "print(mat.t().matmul(vec))\n",
    "# 矩阵乘矩阵\n",
    "row = vec.unsqueeze(0)\n",
    "print(f'row matrix shape: {row.shape}')\n",
    "print(row.matmul(mat))\n",
    "\n",
    "col = vec.unsqueeze(1)\n",
    "print(f'col matrix shape: {col.shape}')\n",
    "print(mat.t().matmul(col))\n",
    "\n",
    "# 向量乘向量\n",
    "arr = torch.randn(3)\n",
    "print(vec.matmul(arr))"
   ],
   "id": "c1620e24efdb054"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
