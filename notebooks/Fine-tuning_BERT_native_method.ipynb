{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T19:20:37.823905Z",
     "start_time": "2025-06-17T19:20:37.795158Z"
    }
   },
   "source": [
    "# https://medium.com/@raoashish10/fine-tuning-a-pre-trained-bert-model-for-classification-using-native-pytorch-c5f33e87616e\n",
    "# https://www.kaggle.com/datasets/gauravduttakiit/covid-19-tweet-classification\n",
    "# https://zindi.africa/competitions/covid-19-tweet-classification/data\n",
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('../data/COVID-19-Tweet/updated_train.csv')\n",
    "data_test = pd.read_csv('../data/COVID-19-Tweet/updated_test.csv')\n",
    "submission_example = pd.read_csv('../data/COVID-19-Tweet/updated_ss.csv')\n",
    "\n",
    "data_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        ID                                               text  target\n",
       "0  train_0            The bitcoin halving is cancelled due to       1\n",
       "1  train_1  MercyOfAllah In good times wrapped in its gran...       0\n",
       "2  train_2  266 Days No Digital India No Murder of e learn...       1\n",
       "3  train_3  India is likely to run out of the remaining RN...       1\n",
       "4  train_4  In these tough times the best way to grow is t...       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>The bitcoin halving is cancelled due to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>MercyOfAllah In good times wrapped in its gran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>266 Days No Digital India No Murder of e learn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>India is likely to run out of the remaining RN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>In these tough times the best way to grow is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:20:43.747792Z",
     "start_time": "2025-06-17T19:20:43.740972Z"
    }
   },
   "cell_type": "code",
   "source": "data_test.head()",
   "id": "35b336c5946813e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       ID                                               text\n",
       "0  test_2         Why is  explained in the video take a look\n",
       "1  test_3            Ed Davey fasting for Ramadan No contest\n",
       "2  test_4   Is Doja Cat good or do you just miss Nicki Minaj\n",
       "3  test_8  How Boris Johnson s cheery wounded in action p...\n",
       "4  test_9  Man it s terrible Not even a reason to get on ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Why is  explained in the video take a look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4</td>\n",
       "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_8</td>\n",
       "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_9</td>\n",
       "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:46:36.104792Z",
     "start_time": "2025-06-18T15:46:36.083188Z"
    }
   },
   "cell_type": "code",
   "source": "submission_example.head()",
   "id": "5dd99e48f1f7a65d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       ID  target\n",
       "0  test_2       0\n",
       "1  test_3       0\n",
       "2  test_4       0\n",
       "3  test_8       0\n",
       "4  test_9       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:20:47.025202Z",
     "start_time": "2025-06-17T19:20:46.873355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    data_train['text'], data_train['target'],\n",
    "    test_size=0.3, shuffle=True, random_state=42)"
   ],
   "id": "30c996318305f285",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:21:04.030642Z",
     "start_time": "2025-06-17T19:20:55.833665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "train_tokens = tokenizer(list(X_train), padding=True, truncation=True)\n",
    "valid_tokens = tokenizer(list(X_valid), padding=True, truncation=True)\n",
    "train_tokens.keys(), valid_tokens.keys()"
   ],
   "id": "623eb01b5a143f68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'token_type_ids', 'attention_mask']),\n",
       " dict_keys(['input_ids', 'token_type_ids', 'attention_mask']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:21:06.124939Z",
     "start_time": "2025-06-17T19:21:06.118939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tk0 = train_tokens['input_ids'][0]\n",
    "print(tk0)\n",
    "print(tokenizer.decode(tk0))"
   ],
   "id": "7708eec9250b2ef3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3725, 9216, 8653, 2740, 124, 5187, 1559, 5465, 1545, 1367, 5966, 1580, 9493, 2740, 122, 5539, 1571, 26516, 4859, 1545, 4735, 1116, 21606, 5117, 1477, 3993, 1604, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] Update Total cases 3 017 766 12 879 Current cases 1 915 580 856 Deaths 207 722 468 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:21:13.381954Z",
     "start_time": "2025-06-17T19:21:13.374883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, is_train=False):\n",
    "        if is_train:\n",
    "            self.text = X_train\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(y_train)\n",
    "        else:\n",
    "            self.text = X_valid\n",
    "            self.tokens = valid_tokens\n",
    "            self.labels = list(y_valid)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "train_dataset = TweetDataset(is_train=True)\n",
    "valid_dataset = TweetDataset(is_train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "id": "5331e7cd3ed9b195",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:21:19.885316Z",
     "start_time": "2025-06-17T19:21:19.875454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "verify_batch = next(iter(train_loader))\n",
    "print(verify_batch['input_ids'].shape)\n",
    "print(verify_batch['attention_mask'].shape)\n",
    "print(verify_batch['labels'].shape)\n",
    "print(verify_batch['input_ids'][0])\n",
    "print(tokenizer.decode(verify_batch['input_ids'][0]))\n",
    "print(verify_batch['labels'])"
   ],
   "id": "7b9a0fd6d17444a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 97])\n",
      "torch.Size([40, 97])\n",
      "torch.Size([40])\n",
      "tensor([  101, 14130,  2346,  2087,  1592, 17734, 15078,  6778,  1649,  1110,\n",
      "         1750,   170,  1669,  1104,  1120,  4798,  1880,  1190,  1122,  1110,\n",
      "          170,  1159,  1111,  6979,  1106,  2415,  2191, 21769,  1107,  3709,\n",
      "         1114,   170,  2246,  1306,  4944,  1106, 24296,  1141,  1104,  1103,\n",
      "        15592,  1104,  6489,  1103,  1421,  3501,  1995,  6248,  1104,  1103,\n",
      "         4360,  4483,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "[CLS] MercyOfAllah Ramadan however is less a period of atonement than it is a time for Muslims to practice self restraint in keeping with awm Arabic to refrain one of the pillars of Islam the five basic tenets of the Muslim religion [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:34:12.008603Z",
     "start_time": "2025-06-17T19:26:12.730932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrained_model = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model.to(DEVICE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    pretrained_model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        y_hat = outputs.logits\n",
    "        loss = loss_fn(y_hat, batch['labels'])\n",
    "        loss.backward()\n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / BATCH_SIZE\n",
    "        print(f'Training batch {idx + 1}, last loss: {train_last_loss:.4f}')\n",
    "    print(f'\\nTraining epoch {epoch + 1} completed, last loss: {train_last_loss:.4f}\\n')\n",
    "\n",
    "    pretrained_model.eval()\n",
    "    num_correct = 0\n",
    "    y_pred = []\n",
    "    for idx, batch in enumerate(valid_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = pretrained_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            y_hat = outputs.logits\n",
    "            loss = loss_fn(y_hat, batch['labels'])\n",
    "            valid_batch_loss = loss.item()\n",
    "            valid_last_loss = valid_batch_loss / BATCH_SIZE\n",
    "            print(f'Validation batch {idx + 1}, last loss: {valid_last_loss:.4f}')\n",
    "\n",
    "            y_pred.extend(y_hat.argmax(dim=1).cpu().numpy())\n",
    "            num_correct += (y_hat.argmax(dim=1) == batch['labels']).sum().item()\n",
    "            print(f'Validation accuracy: {num_correct / ((idx + 1) * BATCH_SIZE):.4f}')\n",
    "    print(f'\\nValidation epoch {epoch + 1} completed, last loss: {valid_last_loss:.4f}, accuracy: {num_correct / len(valid_dataset):.4f}\\n')"
   ],
   "id": "ffb4e714b4d9dfc2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Training batch 1, last loss: 0.0222\n",
      "Training batch 2, last loss: 0.0210\n",
      "Training batch 3, last loss: 0.0184\n",
      "Training batch 4, last loss: 0.0217\n",
      "Training batch 5, last loss: 0.0215\n",
      "Training batch 6, last loss: 0.0217\n",
      "Training batch 7, last loss: 0.0229\n",
      "Training batch 8, last loss: 0.0183\n",
      "Training batch 9, last loss: 0.0219\n",
      "Training batch 10, last loss: 0.0174\n",
      "Training batch 11, last loss: 0.0222\n",
      "Training batch 12, last loss: 0.0188\n",
      "Training batch 13, last loss: 0.0239\n",
      "Training batch 14, last loss: 0.0202\n",
      "Training batch 15, last loss: 0.0217\n",
      "Training batch 16, last loss: 0.0245\n",
      "Training batch 17, last loss: 0.0224\n",
      "Training batch 18, last loss: 0.0204\n",
      "Training batch 19, last loss: 0.0203\n",
      "Training batch 20, last loss: 0.0243\n",
      "Training batch 21, last loss: 0.0214\n",
      "Training batch 22, last loss: 0.0185\n",
      "Training batch 23, last loss: 0.0193\n",
      "Training batch 24, last loss: 0.0256\n",
      "Training batch 25, last loss: 0.0173\n",
      "Training batch 26, last loss: 0.0218\n",
      "Training batch 27, last loss: 0.0201\n",
      "Training batch 28, last loss: 0.0217\n",
      "Training batch 29, last loss: 0.0265\n",
      "Training batch 30, last loss: 0.0207\n",
      "Training batch 31, last loss: 0.0238\n",
      "Training batch 32, last loss: 0.0189\n",
      "Training batch 33, last loss: 0.0221\n",
      "Training batch 34, last loss: 0.0253\n",
      "Training batch 35, last loss: 0.0247\n",
      "Training batch 36, last loss: 0.0227\n",
      "Training batch 37, last loss: 0.0226\n",
      "Training batch 38, last loss: 0.0193\n",
      "Training batch 39, last loss: 0.0215\n",
      "Training batch 40, last loss: 0.0201\n",
      "Training batch 41, last loss: 0.0208\n",
      "Training batch 42, last loss: 0.0192\n",
      "Training batch 43, last loss: 0.0215\n",
      "Training batch 44, last loss: 0.0232\n",
      "Training batch 45, last loss: 0.0252\n",
      "Training batch 46, last loss: 0.0228\n",
      "Training batch 47, last loss: 0.0203\n",
      "Training batch 48, last loss: 0.0231\n",
      "Training batch 49, last loss: 0.0235\n",
      "Training batch 50, last loss: 0.0214\n",
      "Training batch 51, last loss: 0.0218\n",
      "Training batch 52, last loss: 0.0238\n",
      "Training batch 53, last loss: 0.0220\n",
      "Training batch 54, last loss: 0.0206\n",
      "Training batch 55, last loss: 0.0239\n",
      "Training batch 56, last loss: 0.0220\n",
      "Training batch 57, last loss: 0.0229\n",
      "Training batch 58, last loss: 0.0196\n",
      "Training batch 59, last loss: 0.0189\n",
      "Training batch 60, last loss: 0.0203\n",
      "Training batch 61, last loss: 0.0209\n",
      "Training batch 62, last loss: 0.0212\n",
      "Training batch 63, last loss: 0.0247\n",
      "Training batch 64, last loss: 0.0231\n",
      "Training batch 65, last loss: 0.0199\n",
      "Training batch 66, last loss: 0.0216\n",
      "Training batch 67, last loss: 0.0178\n",
      "Training batch 68, last loss: 0.0223\n",
      "Training batch 69, last loss: 0.0200\n",
      "Training batch 70, last loss: 0.0188\n",
      "Training batch 71, last loss: 0.0246\n",
      "Training batch 72, last loss: 0.0178\n",
      "Training batch 73, last loss: 0.0227\n",
      "Training batch 74, last loss: 0.0233\n",
      "Training batch 75, last loss: 0.0196\n",
      "Training batch 76, last loss: 0.0180\n",
      "Training batch 77, last loss: 0.0212\n",
      "Training batch 78, last loss: 0.0217\n",
      "Training batch 79, last loss: 0.0208\n",
      "Training batch 80, last loss: 0.0192\n",
      "Training batch 81, last loss: 0.0186\n",
      "Training batch 82, last loss: 0.0191\n",
      "Training batch 83, last loss: 0.0219\n",
      "Training batch 84, last loss: 0.0161\n",
      "Training batch 85, last loss: 0.0203\n",
      "Training batch 86, last loss: 0.0213\n",
      "Training batch 87, last loss: 0.0251\n",
      "Training batch 88, last loss: 0.0191\n",
      "Training batch 89, last loss: 0.0241\n",
      "Training batch 90, last loss: 0.0209\n",
      "Training batch 91, last loss: 0.0188\n",
      "Training batch 92, last loss: 0.0203\n",
      "Training batch 93, last loss: 0.0239\n",
      "\n",
      "Training epoch 1 completed, last loss: 0.0239\n",
      "\n",
      "Validation batch 1, last loss: 0.0210\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 2, last loss: 0.0230\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 3, last loss: 0.0161\n",
      "Validation accuracy: 0.5333\n",
      "Validation batch 4, last loss: 0.0235\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 5, last loss: 0.0231\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 6, last loss: 0.0213\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 7, last loss: 0.0194\n",
      "Validation accuracy: 0.4929\n",
      "Validation batch 8, last loss: 0.0225\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0185\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 10, last loss: 0.0226\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 11, last loss: 0.0223\n",
      "Validation accuracy: 0.4818\n",
      "Validation batch 12, last loss: 0.0208\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 13, last loss: 0.0184\n",
      "Validation accuracy: 0.4904\n",
      "Validation batch 14, last loss: 0.0215\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 15, last loss: 0.0224\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 16, last loss: 0.0227\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 17, last loss: 0.0197\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 18, last loss: 0.0242\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 19, last loss: 0.0216\n",
      "Validation accuracy: 0.4789\n",
      "Validation batch 20, last loss: 0.0195\n",
      "Validation accuracy: 0.4825\n",
      "Validation batch 21, last loss: 0.0241\n",
      "Validation accuracy: 0.4762\n",
      "Validation batch 22, last loss: 0.0207\n",
      "Validation accuracy: 0.4773\n",
      "Validation batch 23, last loss: 0.0211\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 24, last loss: 0.0216\n",
      "Validation accuracy: 0.4781\n",
      "Validation batch 25, last loss: 0.0201\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 26, last loss: 0.0181\n",
      "Validation accuracy: 0.4846\n",
      "Validation batch 27, last loss: 0.0238\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 28, last loss: 0.0230\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 29, last loss: 0.0226\n",
      "Validation accuracy: 0.4767\n",
      "Validation batch 30, last loss: 0.0189\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 31, last loss: 0.0184\n",
      "Validation accuracy: 0.4831\n",
      "Validation batch 32, last loss: 0.0222\n",
      "Validation accuracy: 0.4820\n",
      "Validation batch 33, last loss: 0.0243\n",
      "Validation accuracy: 0.4788\n",
      "Validation batch 34, last loss: 0.0219\n",
      "Validation accuracy: 0.4787\n",
      "Validation batch 35, last loss: 0.0188\n",
      "Validation accuracy: 0.4814\n",
      "Validation batch 36, last loss: 0.0220\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 37, last loss: 0.0207\n",
      "Validation accuracy: 0.4818\n",
      "Validation batch 38, last loss: 0.0188\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 39, last loss: 0.0176\n",
      "Validation accuracy: 0.4878\n",
      "Validation batch 40, last loss: 0.0225\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 1 completed, last loss: 0.0225, accuracy: 0.4871\n",
      "\n",
      "Epoch 2/30\n",
      "Training batch 1, last loss: 0.0230\n",
      "Training batch 2, last loss: 0.0251\n",
      "Training batch 3, last loss: 0.0232\n",
      "Training batch 4, last loss: 0.0217\n",
      "Training batch 5, last loss: 0.0204\n",
      "Training batch 6, last loss: 0.0211\n",
      "Training batch 7, last loss: 0.0175\n",
      "Training batch 8, last loss: 0.0216\n",
      "Training batch 9, last loss: 0.0181\n",
      "Training batch 10, last loss: 0.0217\n",
      "Training batch 11, last loss: 0.0242\n",
      "Training batch 12, last loss: 0.0228\n",
      "Training batch 13, last loss: 0.0222\n",
      "Training batch 14, last loss: 0.0243\n",
      "Training batch 15, last loss: 0.0194\n",
      "Training batch 16, last loss: 0.0226\n",
      "Training batch 17, last loss: 0.0203\n",
      "Training batch 18, last loss: 0.0181\n",
      "Training batch 19, last loss: 0.0221\n",
      "Training batch 20, last loss: 0.0225\n",
      "Training batch 21, last loss: 0.0216\n",
      "Training batch 22, last loss: 0.0198\n",
      "Training batch 23, last loss: 0.0199\n",
      "Training batch 24, last loss: 0.0220\n",
      "Training batch 25, last loss: 0.0225\n",
      "Training batch 26, last loss: 0.0208\n",
      "Training batch 27, last loss: 0.0234\n",
      "Training batch 28, last loss: 0.0208\n",
      "Training batch 29, last loss: 0.0252\n",
      "Training batch 30, last loss: 0.0233\n",
      "Training batch 31, last loss: 0.0189\n",
      "Training batch 32, last loss: 0.0193\n",
      "Training batch 33, last loss: 0.0194\n",
      "Training batch 34, last loss: 0.0252\n",
      "Training batch 35, last loss: 0.0210\n",
      "Training batch 36, last loss: 0.0198\n",
      "Training batch 37, last loss: 0.0193\n",
      "Training batch 38, last loss: 0.0181\n",
      "Training batch 39, last loss: 0.0212\n",
      "Training batch 40, last loss: 0.0213\n",
      "Training batch 41, last loss: 0.0235\n",
      "Training batch 42, last loss: 0.0186\n",
      "Training batch 43, last loss: 0.0198\n",
      "Training batch 44, last loss: 0.0214\n",
      "Training batch 45, last loss: 0.0254\n",
      "Training batch 46, last loss: 0.0242\n",
      "Training batch 47, last loss: 0.0192\n",
      "Training batch 48, last loss: 0.0225\n",
      "Training batch 49, last loss: 0.0214\n",
      "Training batch 50, last loss: 0.0214\n",
      "Training batch 51, last loss: 0.0204\n",
      "Training batch 52, last loss: 0.0223\n",
      "Training batch 53, last loss: 0.0229\n",
      "Training batch 54, last loss: 0.0247\n",
      "Training batch 55, last loss: 0.0225\n",
      "Training batch 56, last loss: 0.0198\n",
      "Training batch 57, last loss: 0.0235\n",
      "Training batch 58, last loss: 0.0215\n",
      "Training batch 59, last loss: 0.0172\n",
      "Training batch 60, last loss: 0.0233\n",
      "Training batch 61, last loss: 0.0211\n",
      "Training batch 62, last loss: 0.0207\n",
      "Training batch 63, last loss: 0.0230\n",
      "Training batch 64, last loss: 0.0220\n",
      "Training batch 65, last loss: 0.0206\n",
      "Training batch 66, last loss: 0.0193\n",
      "Training batch 67, last loss: 0.0203\n",
      "Training batch 68, last loss: 0.0204\n",
      "Training batch 69, last loss: 0.0242\n",
      "Training batch 70, last loss: 0.0224\n",
      "Training batch 71, last loss: 0.0177\n",
      "Training batch 72, last loss: 0.0211\n",
      "Training batch 73, last loss: 0.0199\n",
      "Training batch 74, last loss: 0.0193\n",
      "Training batch 75, last loss: 0.0197\n",
      "Training batch 76, last loss: 0.0191\n",
      "Training batch 77, last loss: 0.0196\n",
      "Training batch 78, last loss: 0.0196\n",
      "Training batch 79, last loss: 0.0211\n",
      "Training batch 80, last loss: 0.0220\n",
      "Training batch 81, last loss: 0.0242\n",
      "Training batch 82, last loss: 0.0214\n",
      "Training batch 83, last loss: 0.0240\n",
      "Training batch 84, last loss: 0.0192\n",
      "Training batch 85, last loss: 0.0203\n",
      "Training batch 86, last loss: 0.0225\n",
      "Training batch 87, last loss: 0.0223\n",
      "Training batch 88, last loss: 0.0237\n",
      "Training batch 89, last loss: 0.0233\n",
      "Training batch 90, last loss: 0.0199\n",
      "Training batch 91, last loss: 0.0216\n",
      "Training batch 92, last loss: 0.0194\n",
      "Training batch 93, last loss: 0.0244\n",
      "\n",
      "Training epoch 2 completed, last loss: 0.0244\n",
      "\n",
      "Validation batch 1, last loss: 0.0200\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 2, last loss: 0.0214\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 3, last loss: 0.0216\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 4, last loss: 0.0222\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 5, last loss: 0.0187\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 6, last loss: 0.0202\n",
      "Validation accuracy: 0.5042\n",
      "Validation batch 7, last loss: 0.0206\n",
      "Validation accuracy: 0.5071\n",
      "Validation batch 8, last loss: 0.0237\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 9, last loss: 0.0209\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 10, last loss: 0.0206\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 11, last loss: 0.0187\n",
      "Validation accuracy: 0.5023\n",
      "Validation batch 12, last loss: 0.0216\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 13, last loss: 0.0208\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 14, last loss: 0.0245\n",
      "Validation accuracy: 0.4911\n",
      "Validation batch 15, last loss: 0.0226\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 16, last loss: 0.0206\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 17, last loss: 0.0206\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 18, last loss: 0.0259\n",
      "Validation accuracy: 0.4778\n",
      "Validation batch 19, last loss: 0.0217\n",
      "Validation accuracy: 0.4763\n",
      "Validation batch 20, last loss: 0.0213\n",
      "Validation accuracy: 0.4763\n",
      "Validation batch 21, last loss: 0.0189\n",
      "Validation accuracy: 0.4810\n",
      "Validation batch 22, last loss: 0.0238\n",
      "Validation accuracy: 0.4773\n",
      "Validation batch 23, last loss: 0.0217\n",
      "Validation accuracy: 0.4761\n",
      "Validation batch 24, last loss: 0.0204\n",
      "Validation accuracy: 0.4781\n",
      "Validation batch 25, last loss: 0.0208\n",
      "Validation accuracy: 0.4790\n",
      "Validation batch 26, last loss: 0.0223\n",
      "Validation accuracy: 0.4769\n",
      "Validation batch 27, last loss: 0.0190\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 28, last loss: 0.0171\n",
      "Validation accuracy: 0.4866\n",
      "Validation batch 29, last loss: 0.0183\n",
      "Validation accuracy: 0.4905\n",
      "Validation batch 30, last loss: 0.0176\n",
      "Validation accuracy: 0.4942\n",
      "Validation batch 31, last loss: 0.0181\n",
      "Validation accuracy: 0.4976\n",
      "Validation batch 32, last loss: 0.0214\n",
      "Validation accuracy: 0.4969\n",
      "Validation batch 33, last loss: 0.0226\n",
      "Validation accuracy: 0.4947\n",
      "Validation batch 34, last loss: 0.0216\n",
      "Validation accuracy: 0.4941\n",
      "Validation batch 35, last loss: 0.0213\n",
      "Validation accuracy: 0.4936\n",
      "Validation batch 36, last loss: 0.0236\n",
      "Validation accuracy: 0.4910\n",
      "Validation batch 37, last loss: 0.0208\n",
      "Validation accuracy: 0.4912\n",
      "Validation batch 38, last loss: 0.0218\n",
      "Validation accuracy: 0.4908\n",
      "Validation batch 39, last loss: 0.0253\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0199\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 2 completed, last loss: 0.0199, accuracy: 0.4871\n",
      "\n",
      "Epoch 3/30\n",
      "Training batch 1, last loss: 0.0223\n",
      "Training batch 2, last loss: 0.0228\n",
      "Training batch 3, last loss: 0.0221\n",
      "Training batch 4, last loss: 0.0211\n",
      "Training batch 5, last loss: 0.0242\n",
      "Training batch 6, last loss: 0.0213\n",
      "Training batch 7, last loss: 0.0199\n",
      "Training batch 8, last loss: 0.0180\n",
      "Training batch 9, last loss: 0.0207\n",
      "Training batch 10, last loss: 0.0213\n",
      "Training batch 11, last loss: 0.0223\n",
      "Training batch 12, last loss: 0.0207\n",
      "Training batch 13, last loss: 0.0196\n",
      "Training batch 14, last loss: 0.0219\n",
      "Training batch 15, last loss: 0.0234\n",
      "Training batch 16, last loss: 0.0241\n",
      "Training batch 17, last loss: 0.0206\n",
      "Training batch 18, last loss: 0.0209\n",
      "Training batch 19, last loss: 0.0218\n",
      "Training batch 20, last loss: 0.0227\n",
      "Training batch 21, last loss: 0.0237\n",
      "Training batch 22, last loss: 0.0228\n",
      "Training batch 23, last loss: 0.0213\n",
      "Training batch 24, last loss: 0.0244\n",
      "Training batch 25, last loss: 0.0247\n",
      "Training batch 26, last loss: 0.0170\n",
      "Training batch 27, last loss: 0.0201\n",
      "Training batch 28, last loss: 0.0199\n",
      "Training batch 29, last loss: 0.0255\n",
      "Training batch 30, last loss: 0.0196\n",
      "Training batch 31, last loss: 0.0180\n",
      "Training batch 32, last loss: 0.0225\n",
      "Training batch 33, last loss: 0.0217\n",
      "Training batch 34, last loss: 0.0176\n",
      "Training batch 35, last loss: 0.0218\n",
      "Training batch 36, last loss: 0.0237\n",
      "Training batch 37, last loss: 0.0240\n",
      "Training batch 38, last loss: 0.0189\n",
      "Training batch 39, last loss: 0.0224\n",
      "Training batch 40, last loss: 0.0195\n",
      "Training batch 41, last loss: 0.0230\n",
      "Training batch 42, last loss: 0.0210\n",
      "Training batch 43, last loss: 0.0234\n",
      "Training batch 44, last loss: 0.0231\n",
      "Training batch 45, last loss: 0.0235\n",
      "Training batch 46, last loss: 0.0194\n",
      "Training batch 47, last loss: 0.0202\n",
      "Training batch 48, last loss: 0.0231\n",
      "Training batch 49, last loss: 0.0203\n",
      "Training batch 50, last loss: 0.0209\n",
      "Training batch 51, last loss: 0.0197\n",
      "Training batch 52, last loss: 0.0190\n",
      "Training batch 53, last loss: 0.0176\n",
      "Training batch 54, last loss: 0.0218\n",
      "Training batch 55, last loss: 0.0192\n",
      "Training batch 56, last loss: 0.0169\n",
      "Training batch 57, last loss: 0.0220\n",
      "Training batch 58, last loss: 0.0167\n",
      "Training batch 59, last loss: 0.0237\n",
      "Training batch 60, last loss: 0.0246\n",
      "Training batch 61, last loss: 0.0202\n",
      "Training batch 62, last loss: 0.0190\n",
      "Training batch 63, last loss: 0.0193\n",
      "Training batch 64, last loss: 0.0242\n",
      "Training batch 65, last loss: 0.0210\n",
      "Training batch 66, last loss: 0.0215\n",
      "Training batch 67, last loss: 0.0248\n",
      "Training batch 68, last loss: 0.0181\n",
      "Training batch 69, last loss: 0.0242\n",
      "Training batch 70, last loss: 0.0239\n",
      "Training batch 71, last loss: 0.0199\n",
      "Training batch 72, last loss: 0.0226\n",
      "Training batch 73, last loss: 0.0228\n",
      "Training batch 74, last loss: 0.0158\n",
      "Training batch 75, last loss: 0.0181\n",
      "Training batch 76, last loss: 0.0222\n",
      "Training batch 77, last loss: 0.0219\n",
      "Training batch 78, last loss: 0.0196\n",
      "Training batch 79, last loss: 0.0213\n",
      "Training batch 80, last loss: 0.0197\n",
      "Training batch 81, last loss: 0.0218\n",
      "Training batch 82, last loss: 0.0240\n",
      "Training batch 83, last loss: 0.0192\n",
      "Training batch 84, last loss: 0.0239\n",
      "Training batch 85, last loss: 0.0215\n",
      "Training batch 86, last loss: 0.0228\n",
      "Training batch 87, last loss: 0.0221\n",
      "Training batch 88, last loss: 0.0247\n",
      "Training batch 89, last loss: 0.0192\n",
      "Training batch 90, last loss: 0.0188\n",
      "Training batch 91, last loss: 0.0201\n",
      "Training batch 92, last loss: 0.0228\n",
      "Training batch 93, last loss: 0.0234\n",
      "\n",
      "Training epoch 3 completed, last loss: 0.0234\n",
      "\n",
      "Validation batch 1, last loss: 0.0192\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0188\n",
      "Validation accuracy: 0.5625\n",
      "Validation batch 3, last loss: 0.0226\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 4, last loss: 0.0217\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 5, last loss: 0.0189\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 6, last loss: 0.0196\n",
      "Validation accuracy: 0.5292\n",
      "Validation batch 7, last loss: 0.0238\n",
      "Validation accuracy: 0.5107\n",
      "Validation batch 8, last loss: 0.0214\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 9, last loss: 0.0221\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 10, last loss: 0.0237\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 11, last loss: 0.0224\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 12, last loss: 0.0246\n",
      "Validation accuracy: 0.4729\n",
      "Validation batch 13, last loss: 0.0215\n",
      "Validation accuracy: 0.4731\n",
      "Validation batch 14, last loss: 0.0234\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 15, last loss: 0.0189\n",
      "Validation accuracy: 0.4733\n",
      "Validation batch 16, last loss: 0.0224\n",
      "Validation accuracy: 0.4719\n",
      "Validation batch 17, last loss: 0.0208\n",
      "Validation accuracy: 0.4735\n",
      "Validation batch 18, last loss: 0.0172\n",
      "Validation accuracy: 0.4819\n",
      "Validation batch 19, last loss: 0.0192\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 20, last loss: 0.0220\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 21, last loss: 0.0226\n",
      "Validation accuracy: 0.4810\n",
      "Validation batch 22, last loss: 0.0192\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 23, last loss: 0.0203\n",
      "Validation accuracy: 0.4848\n",
      "Validation batch 24, last loss: 0.0224\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 25, last loss: 0.0172\n",
      "Validation accuracy: 0.4890\n",
      "Validation batch 26, last loss: 0.0191\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 27, last loss: 0.0213\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 28, last loss: 0.0250\n",
      "Validation accuracy: 0.4866\n",
      "Validation batch 29, last loss: 0.0201\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 30, last loss: 0.0216\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 31, last loss: 0.0222\n",
      "Validation accuracy: 0.4863\n",
      "Validation batch 32, last loss: 0.0194\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 33, last loss: 0.0274\n",
      "Validation accuracy: 0.4811\n",
      "Validation batch 34, last loss: 0.0205\n",
      "Validation accuracy: 0.4824\n",
      "Validation batch 35, last loss: 0.0184\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 36, last loss: 0.0209\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 37, last loss: 0.0218\n",
      "Validation accuracy: 0.4851\n",
      "Validation batch 38, last loss: 0.0210\n",
      "Validation accuracy: 0.4849\n",
      "Validation batch 39, last loss: 0.0189\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0213\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 3 completed, last loss: 0.0213, accuracy: 0.4871\n",
      "\n",
      "Epoch 4/30\n",
      "Training batch 1, last loss: 0.0222\n",
      "Training batch 2, last loss: 0.0208\n",
      "Training batch 3, last loss: 0.0205\n",
      "Training batch 4, last loss: 0.0210\n",
      "Training batch 5, last loss: 0.0243\n",
      "Training batch 6, last loss: 0.0199\n",
      "Training batch 7, last loss: 0.0245\n",
      "Training batch 8, last loss: 0.0186\n",
      "Training batch 9, last loss: 0.0230\n",
      "Training batch 10, last loss: 0.0209\n",
      "Training batch 11, last loss: 0.0197\n",
      "Training batch 12, last loss: 0.0200\n",
      "Training batch 13, last loss: 0.0222\n",
      "Training batch 14, last loss: 0.0246\n",
      "Training batch 15, last loss: 0.0194\n",
      "Training batch 16, last loss: 0.0214\n",
      "Training batch 17, last loss: 0.0211\n",
      "Training batch 18, last loss: 0.0255\n",
      "Training batch 19, last loss: 0.0211\n",
      "Training batch 20, last loss: 0.0185\n",
      "Training batch 21, last loss: 0.0215\n",
      "Training batch 22, last loss: 0.0200\n",
      "Training batch 23, last loss: 0.0209\n",
      "Training batch 24, last loss: 0.0210\n",
      "Training batch 25, last loss: 0.0245\n",
      "Training batch 26, last loss: 0.0243\n",
      "Training batch 27, last loss: 0.0221\n",
      "Training batch 28, last loss: 0.0233\n",
      "Training batch 29, last loss: 0.0189\n",
      "Training batch 30, last loss: 0.0217\n",
      "Training batch 31, last loss: 0.0214\n",
      "Training batch 32, last loss: 0.0212\n",
      "Training batch 33, last loss: 0.0193\n",
      "Training batch 34, last loss: 0.0219\n",
      "Training batch 35, last loss: 0.0221\n",
      "Training batch 36, last loss: 0.0201\n",
      "Training batch 37, last loss: 0.0200\n",
      "Training batch 38, last loss: 0.0206\n",
      "Training batch 39, last loss: 0.0222\n",
      "Training batch 40, last loss: 0.0192\n",
      "Training batch 41, last loss: 0.0252\n",
      "Training batch 42, last loss: 0.0236\n",
      "Training batch 43, last loss: 0.0194\n",
      "Training batch 44, last loss: 0.0205\n",
      "Training batch 45, last loss: 0.0196\n",
      "Training batch 46, last loss: 0.0156\n",
      "Training batch 47, last loss: 0.0185\n",
      "Training batch 48, last loss: 0.0237\n",
      "Training batch 49, last loss: 0.0217\n",
      "Training batch 50, last loss: 0.0218\n",
      "Training batch 51, last loss: 0.0197\n",
      "Training batch 52, last loss: 0.0224\n",
      "Training batch 53, last loss: 0.0225\n",
      "Training batch 54, last loss: 0.0239\n",
      "Training batch 55, last loss: 0.0262\n",
      "Training batch 56, last loss: 0.0206\n",
      "Training batch 57, last loss: 0.0197\n",
      "Training batch 58, last loss: 0.0225\n",
      "Training batch 59, last loss: 0.0218\n",
      "Training batch 60, last loss: 0.0214\n",
      "Training batch 61, last loss: 0.0195\n",
      "Training batch 62, last loss: 0.0251\n",
      "Training batch 63, last loss: 0.0197\n",
      "Training batch 64, last loss: 0.0228\n",
      "Training batch 65, last loss: 0.0177\n",
      "Training batch 66, last loss: 0.0207\n",
      "Training batch 67, last loss: 0.0199\n",
      "Training batch 68, last loss: 0.0213\n",
      "Training batch 69, last loss: 0.0206\n",
      "Training batch 70, last loss: 0.0226\n",
      "Training batch 71, last loss: 0.0222\n",
      "Training batch 72, last loss: 0.0230\n",
      "Training batch 73, last loss: 0.0228\n",
      "Training batch 74, last loss: 0.0226\n",
      "Training batch 75, last loss: 0.0230\n",
      "Training batch 76, last loss: 0.0219\n",
      "Training batch 77, last loss: 0.0215\n",
      "Training batch 78, last loss: 0.0218\n",
      "Training batch 79, last loss: 0.0227\n",
      "Training batch 80, last loss: 0.0207\n",
      "Training batch 81, last loss: 0.0236\n",
      "Training batch 82, last loss: 0.0203\n",
      "Training batch 83, last loss: 0.0235\n",
      "Training batch 84, last loss: 0.0226\n",
      "Training batch 85, last loss: 0.0204\n",
      "Training batch 86, last loss: 0.0217\n",
      "Training batch 87, last loss: 0.0218\n",
      "Training batch 88, last loss: 0.0206\n",
      "Training batch 89, last loss: 0.0209\n",
      "Training batch 90, last loss: 0.0169\n",
      "Training batch 91, last loss: 0.0192\n",
      "Training batch 92, last loss: 0.0215\n",
      "Training batch 93, last loss: 0.0240\n",
      "\n",
      "Training epoch 4 completed, last loss: 0.0240\n",
      "\n",
      "Validation batch 1, last loss: 0.0182\n",
      "Validation accuracy: 0.6000\n",
      "Validation batch 2, last loss: 0.0166\n",
      "Validation accuracy: 0.6250\n",
      "Validation batch 3, last loss: 0.0208\n",
      "Validation accuracy: 0.5833\n",
      "Validation batch 4, last loss: 0.0218\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 5, last loss: 0.0208\n",
      "Validation accuracy: 0.5400\n",
      "Validation batch 6, last loss: 0.0212\n",
      "Validation accuracy: 0.5292\n",
      "Validation batch 7, last loss: 0.0229\n",
      "Validation accuracy: 0.5143\n",
      "Validation batch 8, last loss: 0.0226\n",
      "Validation accuracy: 0.5031\n",
      "Validation batch 9, last loss: 0.0235\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 10, last loss: 0.0226\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 11, last loss: 0.0182\n",
      "Validation accuracy: 0.4955\n",
      "Validation batch 12, last loss: 0.0223\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 13, last loss: 0.0230\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 14, last loss: 0.0206\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 15, last loss: 0.0236\n",
      "Validation accuracy: 0.4817\n",
      "Validation batch 16, last loss: 0.0165\n",
      "Validation accuracy: 0.4922\n",
      "Validation batch 17, last loss: 0.0191\n",
      "Validation accuracy: 0.4971\n",
      "Validation batch 18, last loss: 0.0171\n",
      "Validation accuracy: 0.5042\n",
      "Validation batch 19, last loss: 0.0229\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 20, last loss: 0.0182\n",
      "Validation accuracy: 0.5050\n",
      "Validation batch 21, last loss: 0.0194\n",
      "Validation accuracy: 0.5071\n",
      "Validation batch 22, last loss: 0.0220\n",
      "Validation accuracy: 0.5045\n",
      "Validation batch 23, last loss: 0.0266\n",
      "Validation accuracy: 0.4957\n",
      "Validation batch 24, last loss: 0.0244\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 25, last loss: 0.0197\n",
      "Validation accuracy: 0.4920\n",
      "Validation batch 26, last loss: 0.0208\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 27, last loss: 0.0204\n",
      "Validation accuracy: 0.4926\n",
      "Validation batch 28, last loss: 0.0234\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 29, last loss: 0.0214\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 30, last loss: 0.0231\n",
      "Validation accuracy: 0.4858\n",
      "Validation batch 31, last loss: 0.0160\n",
      "Validation accuracy: 0.4919\n",
      "Validation batch 32, last loss: 0.0202\n",
      "Validation accuracy: 0.4930\n",
      "Validation batch 33, last loss: 0.0233\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 34, last loss: 0.0207\n",
      "Validation accuracy: 0.4904\n",
      "Validation batch 35, last loss: 0.0238\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 36, last loss: 0.0212\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 37, last loss: 0.0199\n",
      "Validation accuracy: 0.4899\n",
      "Validation batch 38, last loss: 0.0205\n",
      "Validation accuracy: 0.4901\n",
      "Validation batch 39, last loss: 0.0249\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0206\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 4 completed, last loss: 0.0206, accuracy: 0.4871\n",
      "\n",
      "Epoch 5/30\n",
      "Training batch 1, last loss: 0.0203\n",
      "Training batch 2, last loss: 0.0203\n",
      "Training batch 3, last loss: 0.0215\n",
      "Training batch 4, last loss: 0.0205\n",
      "Training batch 5, last loss: 0.0224\n",
      "Training batch 6, last loss: 0.0238\n",
      "Training batch 7, last loss: 0.0215\n",
      "Training batch 8, last loss: 0.0171\n",
      "Training batch 9, last loss: 0.0216\n",
      "Training batch 10, last loss: 0.0214\n",
      "Training batch 11, last loss: 0.0220\n",
      "Training batch 12, last loss: 0.0198\n",
      "Training batch 13, last loss: 0.0206\n",
      "Training batch 14, last loss: 0.0182\n",
      "Training batch 15, last loss: 0.0165\n",
      "Training batch 16, last loss: 0.0186\n",
      "Training batch 17, last loss: 0.0195\n",
      "Training batch 18, last loss: 0.0240\n",
      "Training batch 19, last loss: 0.0200\n",
      "Training batch 20, last loss: 0.0229\n",
      "Training batch 21, last loss: 0.0182\n",
      "Training batch 22, last loss: 0.0193\n",
      "Training batch 23, last loss: 0.0227\n",
      "Training batch 24, last loss: 0.0217\n",
      "Training batch 25, last loss: 0.0234\n",
      "Training batch 26, last loss: 0.0213\n",
      "Training batch 27, last loss: 0.0260\n",
      "Training batch 28, last loss: 0.0224\n",
      "Training batch 29, last loss: 0.0191\n",
      "Training batch 30, last loss: 0.0248\n",
      "Training batch 31, last loss: 0.0228\n",
      "Training batch 32, last loss: 0.0226\n",
      "Training batch 33, last loss: 0.0234\n",
      "Training batch 34, last loss: 0.0210\n",
      "Training batch 35, last loss: 0.0225\n",
      "Training batch 36, last loss: 0.0224\n",
      "Training batch 37, last loss: 0.0198\n",
      "Training batch 38, last loss: 0.0236\n",
      "Training batch 39, last loss: 0.0196\n",
      "Training batch 40, last loss: 0.0213\n",
      "Training batch 41, last loss: 0.0218\n",
      "Training batch 42, last loss: 0.0217\n",
      "Training batch 43, last loss: 0.0225\n",
      "Training batch 44, last loss: 0.0233\n",
      "Training batch 45, last loss: 0.0227\n",
      "Training batch 46, last loss: 0.0204\n",
      "Training batch 47, last loss: 0.0201\n",
      "Training batch 48, last loss: 0.0153\n",
      "Training batch 49, last loss: 0.0193\n",
      "Training batch 50, last loss: 0.0226\n",
      "Training batch 51, last loss: 0.0235\n",
      "Training batch 52, last loss: 0.0219\n",
      "Training batch 53, last loss: 0.0201\n",
      "Training batch 54, last loss: 0.0241\n",
      "Training batch 55, last loss: 0.0203\n",
      "Training batch 56, last loss: 0.0187\n",
      "Training batch 57, last loss: 0.0211\n",
      "Training batch 58, last loss: 0.0216\n",
      "Training batch 59, last loss: 0.0237\n",
      "Training batch 60, last loss: 0.0220\n",
      "Training batch 61, last loss: 0.0230\n",
      "Training batch 62, last loss: 0.0188\n",
      "Training batch 63, last loss: 0.0250\n",
      "Training batch 64, last loss: 0.0198\n",
      "Training batch 65, last loss: 0.0214\n",
      "Training batch 66, last loss: 0.0234\n",
      "Training batch 67, last loss: 0.0218\n",
      "Training batch 68, last loss: 0.0244\n",
      "Training batch 69, last loss: 0.0205\n",
      "Training batch 70, last loss: 0.0197\n",
      "Training batch 71, last loss: 0.0202\n",
      "Training batch 72, last loss: 0.0253\n",
      "Training batch 73, last loss: 0.0179\n",
      "Training batch 74, last loss: 0.0203\n",
      "Training batch 75, last loss: 0.0201\n",
      "Training batch 76, last loss: 0.0214\n",
      "Training batch 77, last loss: 0.0200\n",
      "Training batch 78, last loss: 0.0224\n",
      "Training batch 79, last loss: 0.0197\n",
      "Training batch 80, last loss: 0.0220\n",
      "Training batch 81, last loss: 0.0202\n",
      "Training batch 82, last loss: 0.0186\n",
      "Training batch 83, last loss: 0.0177\n",
      "Training batch 84, last loss: 0.0250\n",
      "Training batch 85, last loss: 0.0212\n",
      "Training batch 86, last loss: 0.0210\n",
      "Training batch 87, last loss: 0.0223\n",
      "Training batch 88, last loss: 0.0226\n",
      "Training batch 89, last loss: 0.0242\n",
      "Training batch 90, last loss: 0.0238\n",
      "Training batch 91, last loss: 0.0229\n",
      "Training batch 92, last loss: 0.0225\n",
      "Training batch 93, last loss: 0.0193\n",
      "\n",
      "Training epoch 5 completed, last loss: 0.0193\n",
      "\n",
      "Validation batch 1, last loss: 0.0236\n",
      "Validation accuracy: 0.4000\n",
      "Validation batch 2, last loss: 0.0179\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 3, last loss: 0.0174\n",
      "Validation accuracy: 0.5417\n",
      "Validation batch 4, last loss: 0.0234\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 5, last loss: 0.0252\n",
      "Validation accuracy: 0.4700\n",
      "Validation batch 6, last loss: 0.0204\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 7, last loss: 0.0231\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 8, last loss: 0.0190\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 9, last loss: 0.0176\n",
      "Validation accuracy: 0.4972\n",
      "Validation batch 10, last loss: 0.0165\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 11, last loss: 0.0208\n",
      "Validation accuracy: 0.5114\n",
      "Validation batch 12, last loss: 0.0234\n",
      "Validation accuracy: 0.5021\n",
      "Validation batch 13, last loss: 0.0243\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 14, last loss: 0.0215\n",
      "Validation accuracy: 0.4911\n",
      "Validation batch 15, last loss: 0.0226\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 16, last loss: 0.0233\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 17, last loss: 0.0190\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 18, last loss: 0.0223\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 19, last loss: 0.0236\n",
      "Validation accuracy: 0.4816\n",
      "Validation batch 20, last loss: 0.0239\n",
      "Validation accuracy: 0.4763\n",
      "Validation batch 21, last loss: 0.0217\n",
      "Validation accuracy: 0.4762\n",
      "Validation batch 22, last loss: 0.0237\n",
      "Validation accuracy: 0.4727\n",
      "Validation batch 23, last loss: 0.0191\n",
      "Validation accuracy: 0.4761\n",
      "Validation batch 24, last loss: 0.0223\n",
      "Validation accuracy: 0.4740\n",
      "Validation batch 25, last loss: 0.0227\n",
      "Validation accuracy: 0.4720\n",
      "Validation batch 26, last loss: 0.0228\n",
      "Validation accuracy: 0.4702\n",
      "Validation batch 27, last loss: 0.0211\n",
      "Validation accuracy: 0.4704\n",
      "Validation batch 28, last loss: 0.0226\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 29, last loss: 0.0180\n",
      "Validation accuracy: 0.4733\n",
      "Validation batch 30, last loss: 0.0191\n",
      "Validation accuracy: 0.4758\n",
      "Validation batch 31, last loss: 0.0191\n",
      "Validation accuracy: 0.4782\n",
      "Validation batch 32, last loss: 0.0193\n",
      "Validation accuracy: 0.4805\n",
      "Validation batch 33, last loss: 0.0190\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 34, last loss: 0.0208\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 35, last loss: 0.0214\n",
      "Validation accuracy: 0.4836\n",
      "Validation batch 36, last loss: 0.0183\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 37, last loss: 0.0212\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 38, last loss: 0.0205\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 39, last loss: 0.0204\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 40, last loss: 0.0239\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 5 completed, last loss: 0.0239, accuracy: 0.4871\n",
      "\n",
      "Epoch 6/30\n",
      "Training batch 1, last loss: 0.0198\n",
      "Training batch 2, last loss: 0.0240\n",
      "Training batch 3, last loss: 0.0204\n",
      "Training batch 4, last loss: 0.0189\n",
      "Training batch 5, last loss: 0.0209\n",
      "Training batch 6, last loss: 0.0190\n",
      "Training batch 7, last loss: 0.0188\n",
      "Training batch 8, last loss: 0.0200\n",
      "Training batch 9, last loss: 0.0210\n",
      "Training batch 10, last loss: 0.0193\n",
      "Training batch 11, last loss: 0.0200\n",
      "Training batch 12, last loss: 0.0214\n",
      "Training batch 13, last loss: 0.0188\n",
      "Training batch 14, last loss: 0.0220\n",
      "Training batch 15, last loss: 0.0201\n",
      "Training batch 16, last loss: 0.0232\n",
      "Training batch 17, last loss: 0.0213\n",
      "Training batch 18, last loss: 0.0172\n",
      "Training batch 19, last loss: 0.0235\n",
      "Training batch 20, last loss: 0.0215\n",
      "Training batch 21, last loss: 0.0200\n",
      "Training batch 22, last loss: 0.0191\n",
      "Training batch 23, last loss: 0.0212\n",
      "Training batch 24, last loss: 0.0242\n",
      "Training batch 25, last loss: 0.0218\n",
      "Training batch 26, last loss: 0.0201\n",
      "Training batch 27, last loss: 0.0206\n",
      "Training batch 28, last loss: 0.0211\n",
      "Training batch 29, last loss: 0.0261\n",
      "Training batch 30, last loss: 0.0191\n",
      "Training batch 31, last loss: 0.0197\n",
      "Training batch 32, last loss: 0.0213\n",
      "Training batch 33, last loss: 0.0206\n",
      "Training batch 34, last loss: 0.0195\n",
      "Training batch 35, last loss: 0.0190\n",
      "Training batch 36, last loss: 0.0221\n",
      "Training batch 37, last loss: 0.0206\n",
      "Training batch 38, last loss: 0.0244\n",
      "Training batch 39, last loss: 0.0251\n",
      "Training batch 40, last loss: 0.0239\n",
      "Training batch 41, last loss: 0.0193\n",
      "Training batch 42, last loss: 0.0231\n",
      "Training batch 43, last loss: 0.0196\n",
      "Training batch 44, last loss: 0.0203\n",
      "Training batch 45, last loss: 0.0193\n",
      "Training batch 46, last loss: 0.0218\n",
      "Training batch 47, last loss: 0.0213\n",
      "Training batch 48, last loss: 0.0208\n",
      "Training batch 49, last loss: 0.0247\n",
      "Training batch 50, last loss: 0.0176\n",
      "Training batch 51, last loss: 0.0214\n",
      "Training batch 52, last loss: 0.0233\n",
      "Training batch 53, last loss: 0.0239\n",
      "Training batch 54, last loss: 0.0247\n",
      "Training batch 55, last loss: 0.0231\n",
      "Training batch 56, last loss: 0.0200\n",
      "Training batch 57, last loss: 0.0207\n",
      "Training batch 58, last loss: 0.0173\n",
      "Training batch 59, last loss: 0.0185\n",
      "Training batch 60, last loss: 0.0203\n",
      "Training batch 61, last loss: 0.0204\n",
      "Training batch 62, last loss: 0.0237\n",
      "Training batch 63, last loss: 0.0230\n",
      "Training batch 64, last loss: 0.0240\n",
      "Training batch 65, last loss: 0.0262\n",
      "Training batch 66, last loss: 0.0231\n",
      "Training batch 67, last loss: 0.0191\n",
      "Training batch 68, last loss: 0.0189\n",
      "Training batch 69, last loss: 0.0160\n",
      "Training batch 70, last loss: 0.0219\n",
      "Training batch 71, last loss: 0.0205\n",
      "Training batch 72, last loss: 0.0195\n",
      "Training batch 73, last loss: 0.0247\n",
      "Training batch 74, last loss: 0.0246\n",
      "Training batch 75, last loss: 0.0225\n",
      "Training batch 76, last loss: 0.0220\n",
      "Training batch 77, last loss: 0.0217\n",
      "Training batch 78, last loss: 0.0223\n",
      "Training batch 79, last loss: 0.0264\n",
      "Training batch 80, last loss: 0.0193\n",
      "Training batch 81, last loss: 0.0227\n",
      "Training batch 82, last loss: 0.0261\n",
      "Training batch 83, last loss: 0.0199\n",
      "Training batch 84, last loss: 0.0217\n",
      "Training batch 85, last loss: 0.0208\n",
      "Training batch 86, last loss: 0.0193\n",
      "Training batch 87, last loss: 0.0181\n",
      "Training batch 88, last loss: 0.0223\n",
      "Training batch 89, last loss: 0.0222\n",
      "Training batch 90, last loss: 0.0220\n",
      "Training batch 91, last loss: 0.0232\n",
      "Training batch 92, last loss: 0.0205\n",
      "Training batch 93, last loss: 0.0211\n",
      "\n",
      "Training epoch 6 completed, last loss: 0.0211\n",
      "\n",
      "Validation batch 1, last loss: 0.0182\n",
      "Validation accuracy: 0.6000\n",
      "Validation batch 2, last loss: 0.0230\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 3, last loss: 0.0206\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 4, last loss: 0.0212\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 5, last loss: 0.0212\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 6, last loss: 0.0232\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 7, last loss: 0.0222\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 8, last loss: 0.0191\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0197\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 10, last loss: 0.0207\n",
      "Validation accuracy: 0.4925\n",
      "Validation batch 11, last loss: 0.0226\n",
      "Validation accuracy: 0.4864\n",
      "Validation batch 12, last loss: 0.0194\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 13, last loss: 0.0204\n",
      "Validation accuracy: 0.4942\n",
      "Validation batch 14, last loss: 0.0166\n",
      "Validation accuracy: 0.5054\n",
      "Validation batch 15, last loss: 0.0204\n",
      "Validation accuracy: 0.5067\n",
      "Validation batch 16, last loss: 0.0211\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 17, last loss: 0.0227\n",
      "Validation accuracy: 0.5015\n",
      "Validation batch 18, last loss: 0.0219\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 19, last loss: 0.0221\n",
      "Validation accuracy: 0.4974\n",
      "Validation batch 20, last loss: 0.0203\n",
      "Validation accuracy: 0.4988\n",
      "Validation batch 21, last loss: 0.0226\n",
      "Validation accuracy: 0.4952\n",
      "Validation batch 22, last loss: 0.0209\n",
      "Validation accuracy: 0.4955\n",
      "Validation batch 23, last loss: 0.0223\n",
      "Validation accuracy: 0.4935\n",
      "Validation batch 24, last loss: 0.0160\n",
      "Validation accuracy: 0.5010\n",
      "Validation batch 25, last loss: 0.0243\n",
      "Validation accuracy: 0.4960\n",
      "Validation batch 26, last loss: 0.0227\n",
      "Validation accuracy: 0.4933\n",
      "Validation batch 27, last loss: 0.0243\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 28, last loss: 0.0206\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 29, last loss: 0.0233\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 30, last loss: 0.0204\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 31, last loss: 0.0218\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 32, last loss: 0.0215\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 33, last loss: 0.0205\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 34, last loss: 0.0185\n",
      "Validation accuracy: 0.4897\n",
      "Validation batch 35, last loss: 0.0198\n",
      "Validation accuracy: 0.4907\n",
      "Validation batch 36, last loss: 0.0232\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 37, last loss: 0.0185\n",
      "Validation accuracy: 0.4912\n",
      "Validation batch 38, last loss: 0.0183\n",
      "Validation accuracy: 0.4934\n",
      "Validation batch 39, last loss: 0.0257\n",
      "Validation accuracy: 0.4891\n",
      "Validation batch 40, last loss: 0.0246\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 6 completed, last loss: 0.0246, accuracy: 0.4871\n",
      "\n",
      "Epoch 7/30\n",
      "Training batch 1, last loss: 0.0209\n",
      "Training batch 2, last loss: 0.0249\n",
      "Training batch 3, last loss: 0.0220\n",
      "Training batch 4, last loss: 0.0242\n",
      "Training batch 5, last loss: 0.0277\n",
      "Training batch 6, last loss: 0.0195\n",
      "Training batch 7, last loss: 0.0251\n",
      "Training batch 8, last loss: 0.0230\n",
      "Training batch 9, last loss: 0.0267\n",
      "Training batch 10, last loss: 0.0206\n",
      "Training batch 11, last loss: 0.0218\n",
      "Training batch 12, last loss: 0.0197\n",
      "Training batch 13, last loss: 0.0142\n",
      "Training batch 14, last loss: 0.0180\n",
      "Training batch 15, last loss: 0.0215\n",
      "Training batch 16, last loss: 0.0298\n",
      "Training batch 17, last loss: 0.0209\n",
      "Training batch 18, last loss: 0.0234\n",
      "Training batch 19, last loss: 0.0210\n",
      "Training batch 20, last loss: 0.0218\n",
      "Training batch 21, last loss: 0.0246\n",
      "Training batch 22, last loss: 0.0213\n",
      "Training batch 23, last loss: 0.0220\n",
      "Training batch 24, last loss: 0.0193\n",
      "Training batch 25, last loss: 0.0201\n",
      "Training batch 26, last loss: 0.0185\n",
      "Training batch 27, last loss: 0.0182\n",
      "Training batch 28, last loss: 0.0225\n",
      "Training batch 29, last loss: 0.0260\n",
      "Training batch 30, last loss: 0.0225\n",
      "Training batch 31, last loss: 0.0172\n",
      "Training batch 32, last loss: 0.0218\n",
      "Training batch 33, last loss: 0.0212\n",
      "Training batch 34, last loss: 0.0188\n",
      "Training batch 35, last loss: 0.0220\n",
      "Training batch 36, last loss: 0.0218\n",
      "Training batch 37, last loss: 0.0231\n",
      "Training batch 38, last loss: 0.0217\n",
      "Training batch 39, last loss: 0.0194\n",
      "Training batch 40, last loss: 0.0205\n",
      "Training batch 41, last loss: 0.0246\n",
      "Training batch 42, last loss: 0.0195\n",
      "Training batch 43, last loss: 0.0205\n",
      "Training batch 44, last loss: 0.0203\n",
      "Training batch 45, last loss: 0.0246\n",
      "Training batch 46, last loss: 0.0214\n",
      "Training batch 47, last loss: 0.0233\n",
      "Training batch 48, last loss: 0.0234\n",
      "Training batch 49, last loss: 0.0212\n",
      "Training batch 50, last loss: 0.0194\n",
      "Training batch 51, last loss: 0.0209\n",
      "Training batch 52, last loss: 0.0235\n",
      "Training batch 53, last loss: 0.0208\n",
      "Training batch 54, last loss: 0.0217\n",
      "Training batch 55, last loss: 0.0204\n",
      "Training batch 56, last loss: 0.0221\n",
      "Training batch 57, last loss: 0.0207\n",
      "Training batch 58, last loss: 0.0240\n",
      "Training batch 59, last loss: 0.0275\n",
      "Training batch 60, last loss: 0.0162\n",
      "Training batch 61, last loss: 0.0182\n",
      "Training batch 62, last loss: 0.0181\n",
      "Training batch 63, last loss: 0.0170\n",
      "Training batch 64, last loss: 0.0186\n",
      "Training batch 65, last loss: 0.0221\n",
      "Training batch 66, last loss: 0.0237\n",
      "Training batch 67, last loss: 0.0193\n",
      "Training batch 68, last loss: 0.0222\n",
      "Training batch 69, last loss: 0.0187\n",
      "Training batch 70, last loss: 0.0193\n",
      "Training batch 71, last loss: 0.0236\n",
      "Training batch 72, last loss: 0.0200\n",
      "Training batch 73, last loss: 0.0181\n",
      "Training batch 74, last loss: 0.0201\n",
      "Training batch 75, last loss: 0.0226\n",
      "Training batch 76, last loss: 0.0178\n",
      "Training batch 77, last loss: 0.0242\n",
      "Training batch 78, last loss: 0.0250\n",
      "Training batch 79, last loss: 0.0215\n",
      "Training batch 80, last loss: 0.0224\n",
      "Training batch 81, last loss: 0.0194\n",
      "Training batch 82, last loss: 0.0225\n",
      "Training batch 83, last loss: 0.0162\n",
      "Training batch 84, last loss: 0.0220\n",
      "Training batch 85, last loss: 0.0193\n",
      "Training batch 86, last loss: 0.0205\n",
      "Training batch 87, last loss: 0.0225\n",
      "Training batch 88, last loss: 0.0250\n",
      "Training batch 89, last loss: 0.0226\n",
      "Training batch 90, last loss: 0.0229\n",
      "Training batch 91, last loss: 0.0238\n",
      "Training batch 92, last loss: 0.0204\n",
      "Training batch 93, last loss: 0.0201\n",
      "\n",
      "Training epoch 7 completed, last loss: 0.0201\n",
      "\n",
      "Validation batch 1, last loss: 0.0220\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 2, last loss: 0.0206\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 3, last loss: 0.0206\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 4, last loss: 0.0218\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 5, last loss: 0.0223\n",
      "Validation accuracy: 0.4700\n",
      "Validation batch 6, last loss: 0.0219\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 7, last loss: 0.0191\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 8, last loss: 0.0190\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 9, last loss: 0.0230\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 10, last loss: 0.0257\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 11, last loss: 0.0210\n",
      "Validation accuracy: 0.4659\n",
      "Validation batch 12, last loss: 0.0200\n",
      "Validation accuracy: 0.4708\n",
      "Validation batch 13, last loss: 0.0220\n",
      "Validation accuracy: 0.4692\n",
      "Validation batch 14, last loss: 0.0195\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 15, last loss: 0.0214\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 16, last loss: 0.0236\n",
      "Validation accuracy: 0.4703\n",
      "Validation batch 17, last loss: 0.0196\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 18, last loss: 0.0186\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 19, last loss: 0.0191\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 20, last loss: 0.0205\n",
      "Validation accuracy: 0.4863\n",
      "Validation batch 21, last loss: 0.0200\n",
      "Validation accuracy: 0.4881\n",
      "Validation batch 22, last loss: 0.0233\n",
      "Validation accuracy: 0.4852\n",
      "Validation batch 23, last loss: 0.0248\n",
      "Validation accuracy: 0.4793\n",
      "Validation batch 24, last loss: 0.0206\n",
      "Validation accuracy: 0.4802\n",
      "Validation batch 25, last loss: 0.0224\n",
      "Validation accuracy: 0.4790\n",
      "Validation batch 26, last loss: 0.0180\n",
      "Validation accuracy: 0.4837\n",
      "Validation batch 27, last loss: 0.0176\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 28, last loss: 0.0202\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 29, last loss: 0.0231\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 30, last loss: 0.0200\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 31, last loss: 0.0207\n",
      "Validation accuracy: 0.4887\n",
      "Validation batch 32, last loss: 0.0203\n",
      "Validation accuracy: 0.4898\n",
      "Validation batch 33, last loss: 0.0227\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 34, last loss: 0.0219\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 35, last loss: 0.0190\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 36, last loss: 0.0206\n",
      "Validation accuracy: 0.4903\n",
      "Validation batch 37, last loss: 0.0233\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 38, last loss: 0.0192\n",
      "Validation accuracy: 0.4901\n",
      "Validation batch 39, last loss: 0.0243\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0217\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 7 completed, last loss: 0.0217, accuracy: 0.4871\n",
      "\n",
      "Epoch 8/30\n",
      "Training batch 1, last loss: 0.0203\n",
      "Training batch 2, last loss: 0.0234\n",
      "Training batch 3, last loss: 0.0217\n",
      "Training batch 4, last loss: 0.0208\n",
      "Training batch 5, last loss: 0.0242\n",
      "Training batch 6, last loss: 0.0193\n",
      "Training batch 7, last loss: 0.0221\n",
      "Training batch 8, last loss: 0.0177\n",
      "Training batch 9, last loss: 0.0188\n",
      "Training batch 10, last loss: 0.0209\n",
      "Training batch 11, last loss: 0.0207\n",
      "Training batch 12, last loss: 0.0208\n",
      "Training batch 13, last loss: 0.0203\n",
      "Training batch 14, last loss: 0.0223\n",
      "Training batch 15, last loss: 0.0182\n",
      "Training batch 16, last loss: 0.0177\n",
      "Training batch 17, last loss: 0.0200\n",
      "Training batch 18, last loss: 0.0218\n",
      "Training batch 19, last loss: 0.0197\n",
      "Training batch 20, last loss: 0.0228\n",
      "Training batch 21, last loss: 0.0179\n",
      "Training batch 22, last loss: 0.0211\n",
      "Training batch 23, last loss: 0.0217\n",
      "Training batch 24, last loss: 0.0193\n",
      "Training batch 25, last loss: 0.0185\n",
      "Training batch 26, last loss: 0.0201\n",
      "Training batch 27, last loss: 0.0198\n",
      "Training batch 28, last loss: 0.0202\n",
      "Training batch 29, last loss: 0.0232\n",
      "Training batch 30, last loss: 0.0231\n",
      "Training batch 31, last loss: 0.0192\n",
      "Training batch 32, last loss: 0.0208\n",
      "Training batch 33, last loss: 0.0205\n",
      "Training batch 34, last loss: 0.0201\n",
      "Training batch 35, last loss: 0.0219\n",
      "Training batch 36, last loss: 0.0192\n",
      "Training batch 37, last loss: 0.0174\n",
      "Training batch 38, last loss: 0.0239\n",
      "Training batch 39, last loss: 0.0185\n",
      "Training batch 40, last loss: 0.0226\n",
      "Training batch 41, last loss: 0.0211\n",
      "Training batch 42, last loss: 0.0239\n",
      "Training batch 43, last loss: 0.0181\n",
      "Training batch 44, last loss: 0.0226\n",
      "Training batch 45, last loss: 0.0221\n",
      "Training batch 46, last loss: 0.0176\n",
      "Training batch 47, last loss: 0.0218\n",
      "Training batch 48, last loss: 0.0198\n",
      "Training batch 49, last loss: 0.0236\n",
      "Training batch 50, last loss: 0.0221\n",
      "Training batch 51, last loss: 0.0232\n",
      "Training batch 52, last loss: 0.0224\n",
      "Training batch 53, last loss: 0.0229\n",
      "Training batch 54, last loss: 0.0232\n",
      "Training batch 55, last loss: 0.0233\n",
      "Training batch 56, last loss: 0.0200\n",
      "Training batch 57, last loss: 0.0206\n",
      "Training batch 58, last loss: 0.0220\n",
      "Training batch 59, last loss: 0.0219\n",
      "Training batch 60, last loss: 0.0197\n",
      "Training batch 61, last loss: 0.0226\n",
      "Training batch 62, last loss: 0.0221\n",
      "Training batch 63, last loss: 0.0258\n",
      "Training batch 64, last loss: 0.0184\n",
      "Training batch 65, last loss: 0.0213\n",
      "Training batch 66, last loss: 0.0258\n",
      "Training batch 67, last loss: 0.0226\n",
      "Training batch 68, last loss: 0.0252\n",
      "Training batch 69, last loss: 0.0215\n",
      "Training batch 70, last loss: 0.0188\n",
      "Training batch 71, last loss: 0.0204\n",
      "Training batch 72, last loss: 0.0201\n",
      "Training batch 73, last loss: 0.0237\n",
      "Training batch 74, last loss: 0.0223\n",
      "Training batch 75, last loss: 0.0234\n",
      "Training batch 76, last loss: 0.0230\n",
      "Training batch 77, last loss: 0.0199\n",
      "Training batch 78, last loss: 0.0234\n",
      "Training batch 79, last loss: 0.0251\n",
      "Training batch 80, last loss: 0.0199\n",
      "Training batch 81, last loss: 0.0228\n",
      "Training batch 82, last loss: 0.0244\n",
      "Training batch 83, last loss: 0.0233\n",
      "Training batch 84, last loss: 0.0279\n",
      "Training batch 85, last loss: 0.0199\n",
      "Training batch 86, last loss: 0.0211\n",
      "Training batch 87, last loss: 0.0184\n",
      "Training batch 88, last loss: 0.0200\n",
      "Training batch 89, last loss: 0.0223\n",
      "Training batch 90, last loss: 0.0233\n",
      "Training batch 91, last loss: 0.0190\n",
      "Training batch 92, last loss: 0.0198\n",
      "Training batch 93, last loss: 0.0197\n",
      "\n",
      "Training epoch 8 completed, last loss: 0.0197\n",
      "\n",
      "Validation batch 1, last loss: 0.0234\n",
      "Validation accuracy: 0.4000\n",
      "Validation batch 2, last loss: 0.0208\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 3, last loss: 0.0228\n",
      "Validation accuracy: 0.4417\n",
      "Validation batch 4, last loss: 0.0208\n",
      "Validation accuracy: 0.4562\n",
      "Validation batch 5, last loss: 0.0188\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 6, last loss: 0.0196\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 7, last loss: 0.0213\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 8, last loss: 0.0229\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 9, last loss: 0.0238\n",
      "Validation accuracy: 0.4694\n",
      "Validation batch 10, last loss: 0.0244\n",
      "Validation accuracy: 0.4600\n",
      "Validation batch 11, last loss: 0.0214\n",
      "Validation accuracy: 0.4614\n",
      "Validation batch 12, last loss: 0.0227\n",
      "Validation accuracy: 0.4583\n",
      "Validation batch 13, last loss: 0.0200\n",
      "Validation accuracy: 0.4635\n",
      "Validation batch 14, last loss: 0.0188\n",
      "Validation accuracy: 0.4714\n",
      "Validation batch 15, last loss: 0.0198\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 16, last loss: 0.0189\n",
      "Validation accuracy: 0.4797\n",
      "Validation batch 17, last loss: 0.0210\n",
      "Validation accuracy: 0.4809\n",
      "Validation batch 18, last loss: 0.0192\n",
      "Validation accuracy: 0.4847\n",
      "Validation batch 19, last loss: 0.0197\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 20, last loss: 0.0211\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 21, last loss: 0.0223\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 22, last loss: 0.0202\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 23, last loss: 0.0219\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 24, last loss: 0.0181\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 25, last loss: 0.0240\n",
      "Validation accuracy: 0.4860\n",
      "Validation batch 26, last loss: 0.0217\n",
      "Validation accuracy: 0.4856\n",
      "Validation batch 27, last loss: 0.0241\n",
      "Validation accuracy: 0.4815\n",
      "Validation batch 28, last loss: 0.0205\n",
      "Validation accuracy: 0.4830\n",
      "Validation batch 29, last loss: 0.0224\n",
      "Validation accuracy: 0.4819\n",
      "Validation batch 30, last loss: 0.0234\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 31, last loss: 0.0211\n",
      "Validation accuracy: 0.4798\n",
      "Validation batch 32, last loss: 0.0221\n",
      "Validation accuracy: 0.4789\n",
      "Validation batch 33, last loss: 0.0218\n",
      "Validation accuracy: 0.4780\n",
      "Validation batch 34, last loss: 0.0193\n",
      "Validation accuracy: 0.4801\n",
      "Validation batch 35, last loss: 0.0189\n",
      "Validation accuracy: 0.4829\n",
      "Validation batch 36, last loss: 0.0207\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 37, last loss: 0.0211\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 38, last loss: 0.0207\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 39, last loss: 0.0182\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0212\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 8 completed, last loss: 0.0212, accuracy: 0.4871\n",
      "\n",
      "Epoch 9/30\n",
      "Training batch 1, last loss: 0.0238\n",
      "Training batch 2, last loss: 0.0224\n",
      "Training batch 3, last loss: 0.0182\n",
      "Training batch 4, last loss: 0.0194\n",
      "Training batch 5, last loss: 0.0215\n",
      "Training batch 6, last loss: 0.0218\n",
      "Training batch 7, last loss: 0.0243\n",
      "Training batch 8, last loss: 0.0185\n",
      "Training batch 9, last loss: 0.0232\n",
      "Training batch 10, last loss: 0.0179\n",
      "Training batch 11, last loss: 0.0209\n",
      "Training batch 12, last loss: 0.0213\n",
      "Training batch 13, last loss: 0.0167\n",
      "Training batch 14, last loss: 0.0237\n",
      "Training batch 15, last loss: 0.0207\n",
      "Training batch 16, last loss: 0.0245\n",
      "Training batch 17, last loss: 0.0179\n",
      "Training batch 18, last loss: 0.0230\n",
      "Training batch 19, last loss: 0.0232\n",
      "Training batch 20, last loss: 0.0167\n",
      "Training batch 21, last loss: 0.0184\n",
      "Training batch 22, last loss: 0.0246\n",
      "Training batch 23, last loss: 0.0193\n",
      "Training batch 24, last loss: 0.0201\n",
      "Training batch 25, last loss: 0.0231\n",
      "Training batch 26, last loss: 0.0208\n",
      "Training batch 27, last loss: 0.0222\n",
      "Training batch 28, last loss: 0.0188\n",
      "Training batch 29, last loss: 0.0199\n",
      "Training batch 30, last loss: 0.0215\n",
      "Training batch 31, last loss: 0.0224\n",
      "Training batch 32, last loss: 0.0212\n",
      "Training batch 33, last loss: 0.0228\n",
      "Training batch 34, last loss: 0.0191\n",
      "Training batch 35, last loss: 0.0215\n",
      "Training batch 36, last loss: 0.0217\n",
      "Training batch 37, last loss: 0.0209\n",
      "Training batch 38, last loss: 0.0236\n",
      "Training batch 39, last loss: 0.0164\n",
      "Training batch 40, last loss: 0.0197\n",
      "Training batch 41, last loss: 0.0211\n",
      "Training batch 42, last loss: 0.0216\n",
      "Training batch 43, last loss: 0.0259\n",
      "Training batch 44, last loss: 0.0224\n",
      "Training batch 45, last loss: 0.0254\n",
      "Training batch 46, last loss: 0.0230\n",
      "Training batch 47, last loss: 0.0213\n",
      "Training batch 48, last loss: 0.0192\n",
      "Training batch 49, last loss: 0.0200\n",
      "Training batch 50, last loss: 0.0187\n",
      "Training batch 51, last loss: 0.0209\n",
      "Training batch 52, last loss: 0.0239\n",
      "Training batch 53, last loss: 0.0228\n",
      "Training batch 54, last loss: 0.0275\n",
      "Training batch 55, last loss: 0.0228\n",
      "Training batch 56, last loss: 0.0185\n",
      "Training batch 57, last loss: 0.0244\n",
      "Training batch 58, last loss: 0.0253\n",
      "Training batch 59, last loss: 0.0212\n",
      "Training batch 60, last loss: 0.0183\n",
      "Training batch 61, last loss: 0.0221\n",
      "Training batch 62, last loss: 0.0246\n",
      "Training batch 63, last loss: 0.0235\n",
      "Training batch 64, last loss: 0.0198\n",
      "Training batch 65, last loss: 0.0247\n",
      "Training batch 66, last loss: 0.0222\n",
      "Training batch 67, last loss: 0.0217\n",
      "Training batch 68, last loss: 0.0218\n",
      "Training batch 69, last loss: 0.0200\n",
      "Training batch 70, last loss: 0.0242\n",
      "Training batch 71, last loss: 0.0193\n",
      "Training batch 72, last loss: 0.0215\n",
      "Training batch 73, last loss: 0.0199\n",
      "Training batch 74, last loss: 0.0215\n",
      "Training batch 75, last loss: 0.0206\n",
      "Training batch 76, last loss: 0.0236\n",
      "Training batch 77, last loss: 0.0182\n",
      "Training batch 78, last loss: 0.0206\n",
      "Training batch 79, last loss: 0.0214\n",
      "Training batch 80, last loss: 0.0233\n",
      "Training batch 81, last loss: 0.0177\n",
      "Training batch 82, last loss: 0.0200\n",
      "Training batch 83, last loss: 0.0211\n",
      "Training batch 84, last loss: 0.0211\n",
      "Training batch 85, last loss: 0.0205\n",
      "Training batch 86, last loss: 0.0214\n",
      "Training batch 87, last loss: 0.0229\n",
      "Training batch 88, last loss: 0.0197\n",
      "Training batch 89, last loss: 0.0237\n",
      "Training batch 90, last loss: 0.0202\n",
      "Training batch 91, last loss: 0.0217\n",
      "Training batch 92, last loss: 0.0204\n",
      "Training batch 93, last loss: 0.0233\n",
      "\n",
      "Training epoch 9 completed, last loss: 0.0233\n",
      "\n",
      "Validation batch 1, last loss: 0.0199\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0231\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 3, last loss: 0.0207\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 4, last loss: 0.0236\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 5, last loss: 0.0184\n",
      "Validation accuracy: 0.4900\n",
      "Validation batch 6, last loss: 0.0231\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 7, last loss: 0.0236\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 8, last loss: 0.0164\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 9, last loss: 0.0194\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 10, last loss: 0.0207\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 11, last loss: 0.0223\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 12, last loss: 0.0204\n",
      "Validation accuracy: 0.4896\n",
      "Validation batch 13, last loss: 0.0198\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 14, last loss: 0.0210\n",
      "Validation accuracy: 0.4929\n",
      "Validation batch 15, last loss: 0.0232\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 16, last loss: 0.0208\n",
      "Validation accuracy: 0.4891\n",
      "Validation batch 17, last loss: 0.0216\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 18, last loss: 0.0228\n",
      "Validation accuracy: 0.4847\n",
      "Validation batch 19, last loss: 0.0245\n",
      "Validation accuracy: 0.4776\n",
      "Validation batch 20, last loss: 0.0221\n",
      "Validation accuracy: 0.4763\n",
      "Validation batch 21, last loss: 0.0248\n",
      "Validation accuracy: 0.4702\n",
      "Validation batch 22, last loss: 0.0169\n",
      "Validation accuracy: 0.4784\n",
      "Validation batch 23, last loss: 0.0161\n",
      "Validation accuracy: 0.4870\n",
      "Validation batch 24, last loss: 0.0222\n",
      "Validation accuracy: 0.4854\n",
      "Validation batch 25, last loss: 0.0157\n",
      "Validation accuracy: 0.4940\n",
      "Validation batch 26, last loss: 0.0200\n",
      "Validation accuracy: 0.4952\n",
      "Validation batch 27, last loss: 0.0246\n",
      "Validation accuracy: 0.4907\n",
      "Validation batch 28, last loss: 0.0215\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 29, last loss: 0.0251\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 30, last loss: 0.0233\n",
      "Validation accuracy: 0.4825\n",
      "Validation batch 31, last loss: 0.0209\n",
      "Validation accuracy: 0.4831\n",
      "Validation batch 32, last loss: 0.0219\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 33, last loss: 0.0235\n",
      "Validation accuracy: 0.4811\n",
      "Validation batch 34, last loss: 0.0233\n",
      "Validation accuracy: 0.4787\n",
      "Validation batch 35, last loss: 0.0178\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 36, last loss: 0.0210\n",
      "Validation accuracy: 0.4819\n",
      "Validation batch 37, last loss: 0.0204\n",
      "Validation accuracy: 0.4824\n",
      "Validation batch 38, last loss: 0.0203\n",
      "Validation accuracy: 0.4836\n",
      "Validation batch 39, last loss: 0.0172\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0208\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 9 completed, last loss: 0.0208, accuracy: 0.4871\n",
      "\n",
      "Epoch 10/30\n",
      "Training batch 1, last loss: 0.0201\n",
      "Training batch 2, last loss: 0.0221\n",
      "Training batch 3, last loss: 0.0215\n",
      "Training batch 4, last loss: 0.0243\n",
      "Training batch 5, last loss: 0.0209\n",
      "Training batch 6, last loss: 0.0186\n",
      "Training batch 7, last loss: 0.0244\n",
      "Training batch 8, last loss: 0.0214\n",
      "Training batch 9, last loss: 0.0229\n",
      "Training batch 10, last loss: 0.0178\n",
      "Training batch 11, last loss: 0.0195\n",
      "Training batch 12, last loss: 0.0174\n",
      "Training batch 13, last loss: 0.0229\n",
      "Training batch 14, last loss: 0.0201\n",
      "Training batch 15, last loss: 0.0218\n",
      "Training batch 16, last loss: 0.0189\n",
      "Training batch 17, last loss: 0.0224\n",
      "Training batch 18, last loss: 0.0259\n",
      "Training batch 19, last loss: 0.0235\n",
      "Training batch 20, last loss: 0.0235\n",
      "Training batch 21, last loss: 0.0206\n",
      "Training batch 22, last loss: 0.0208\n",
      "Training batch 23, last loss: 0.0198\n",
      "Training batch 24, last loss: 0.0190\n",
      "Training batch 25, last loss: 0.0174\n",
      "Training batch 26, last loss: 0.0194\n",
      "Training batch 27, last loss: 0.0201\n",
      "Training batch 28, last loss: 0.0221\n",
      "Training batch 29, last loss: 0.0229\n",
      "Training batch 30, last loss: 0.0186\n",
      "Training batch 31, last loss: 0.0169\n",
      "Training batch 32, last loss: 0.0187\n",
      "Training batch 33, last loss: 0.0202\n",
      "Training batch 34, last loss: 0.0226\n",
      "Training batch 35, last loss: 0.0203\n",
      "Training batch 36, last loss: 0.0231\n",
      "Training batch 37, last loss: 0.0224\n",
      "Training batch 38, last loss: 0.0213\n",
      "Training batch 39, last loss: 0.0214\n",
      "Training batch 40, last loss: 0.0201\n",
      "Training batch 41, last loss: 0.0250\n",
      "Training batch 42, last loss: 0.0228\n",
      "Training batch 43, last loss: 0.0229\n",
      "Training batch 44, last loss: 0.0186\n",
      "Training batch 45, last loss: 0.0214\n",
      "Training batch 46, last loss: 0.0228\n",
      "Training batch 47, last loss: 0.0215\n",
      "Training batch 48, last loss: 0.0148\n",
      "Training batch 49, last loss: 0.0251\n",
      "Training batch 50, last loss: 0.0221\n",
      "Training batch 51, last loss: 0.0215\n",
      "Training batch 52, last loss: 0.0203\n",
      "Training batch 53, last loss: 0.0223\n",
      "Training batch 54, last loss: 0.0224\n",
      "Training batch 55, last loss: 0.0190\n",
      "Training batch 56, last loss: 0.0215\n",
      "Training batch 57, last loss: 0.0228\n",
      "Training batch 58, last loss: 0.0248\n",
      "Training batch 59, last loss: 0.0175\n",
      "Training batch 60, last loss: 0.0241\n",
      "Training batch 61, last loss: 0.0204\n",
      "Training batch 62, last loss: 0.0247\n",
      "Training batch 63, last loss: 0.0186\n",
      "Training batch 64, last loss: 0.0180\n",
      "Training batch 65, last loss: 0.0179\n",
      "Training batch 66, last loss: 0.0187\n",
      "Training batch 67, last loss: 0.0238\n",
      "Training batch 68, last loss: 0.0182\n",
      "Training batch 69, last loss: 0.0179\n",
      "Training batch 70, last loss: 0.0203\n",
      "Training batch 71, last loss: 0.0217\n",
      "Training batch 72, last loss: 0.0270\n",
      "Training batch 73, last loss: 0.0186\n",
      "Training batch 74, last loss: 0.0233\n",
      "Training batch 75, last loss: 0.0261\n",
      "Training batch 76, last loss: 0.0244\n",
      "Training batch 77, last loss: 0.0223\n",
      "Training batch 78, last loss: 0.0235\n",
      "Training batch 79, last loss: 0.0211\n",
      "Training batch 80, last loss: 0.0210\n",
      "Training batch 81, last loss: 0.0218\n",
      "Training batch 82, last loss: 0.0217\n",
      "Training batch 83, last loss: 0.0203\n",
      "Training batch 84, last loss: 0.0252\n",
      "Training batch 85, last loss: 0.0192\n",
      "Training batch 86, last loss: 0.0240\n",
      "Training batch 87, last loss: 0.0215\n",
      "Training batch 88, last loss: 0.0220\n",
      "Training batch 89, last loss: 0.0228\n",
      "Training batch 90, last loss: 0.0185\n",
      "Training batch 91, last loss: 0.0233\n",
      "Training batch 92, last loss: 0.0232\n",
      "Training batch 93, last loss: 0.0236\n",
      "\n",
      "Training epoch 10 completed, last loss: 0.0236\n",
      "\n",
      "Validation batch 1, last loss: 0.0196\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0233\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 3, last loss: 0.0237\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 4, last loss: 0.0203\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 5, last loss: 0.0229\n",
      "Validation accuracy: 0.4600\n",
      "Validation batch 6, last loss: 0.0221\n",
      "Validation accuracy: 0.4583\n",
      "Validation batch 7, last loss: 0.0178\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 8, last loss: 0.0213\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0216\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 10, last loss: 0.0236\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 11, last loss: 0.0203\n",
      "Validation accuracy: 0.4795\n",
      "Validation batch 12, last loss: 0.0202\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 13, last loss: 0.0212\n",
      "Validation accuracy: 0.4808\n",
      "Validation batch 14, last loss: 0.0196\n",
      "Validation accuracy: 0.4839\n",
      "Validation batch 15, last loss: 0.0176\n",
      "Validation accuracy: 0.4933\n",
      "Validation batch 16, last loss: 0.0218\n",
      "Validation accuracy: 0.4922\n",
      "Validation batch 17, last loss: 0.0208\n",
      "Validation accuracy: 0.4926\n",
      "Validation batch 18, last loss: 0.0167\n",
      "Validation accuracy: 0.5014\n",
      "Validation batch 19, last loss: 0.0221\n",
      "Validation accuracy: 0.4987\n",
      "Validation batch 20, last loss: 0.0210\n",
      "Validation accuracy: 0.4988\n",
      "Validation batch 21, last loss: 0.0224\n",
      "Validation accuracy: 0.4952\n",
      "Validation batch 22, last loss: 0.0238\n",
      "Validation accuracy: 0.4909\n",
      "Validation batch 23, last loss: 0.0213\n",
      "Validation accuracy: 0.4913\n",
      "Validation batch 24, last loss: 0.0247\n",
      "Validation accuracy: 0.4854\n",
      "Validation batch 25, last loss: 0.0201\n",
      "Validation accuracy: 0.4870\n",
      "Validation batch 26, last loss: 0.0206\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 27, last loss: 0.0219\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 28, last loss: 0.0216\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 29, last loss: 0.0202\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 30, last loss: 0.0197\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 31, last loss: 0.0233\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 32, last loss: 0.0205\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 33, last loss: 0.0209\n",
      "Validation accuracy: 0.4864\n",
      "Validation batch 34, last loss: 0.0234\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 35, last loss: 0.0173\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 36, last loss: 0.0221\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 37, last loss: 0.0208\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 38, last loss: 0.0194\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 39, last loss: 0.0229\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0200\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 10 completed, last loss: 0.0200, accuracy: 0.4871\n",
      "\n",
      "Epoch 11/30\n",
      "Training batch 1, last loss: 0.0241\n",
      "Training batch 2, last loss: 0.0253\n",
      "Training batch 3, last loss: 0.0220\n",
      "Training batch 4, last loss: 0.0242\n",
      "Training batch 5, last loss: 0.0189\n",
      "Training batch 6, last loss: 0.0241\n",
      "Training batch 7, last loss: 0.0178\n",
      "Training batch 8, last loss: 0.0226\n",
      "Training batch 9, last loss: 0.0247\n",
      "Training batch 10, last loss: 0.0241\n",
      "Training batch 11, last loss: 0.0248\n",
      "Training batch 12, last loss: 0.0182\n",
      "Training batch 13, last loss: 0.0224\n",
      "Training batch 14, last loss: 0.0221\n",
      "Training batch 15, last loss: 0.0254\n",
      "Training batch 16, last loss: 0.0204\n",
      "Training batch 17, last loss: 0.0253\n",
      "Training batch 18, last loss: 0.0215\n",
      "Training batch 19, last loss: 0.0217\n",
      "Training batch 20, last loss: 0.0185\n",
      "Training batch 21, last loss: 0.0201\n",
      "Training batch 22, last loss: 0.0261\n",
      "Training batch 23, last loss: 0.0230\n",
      "Training batch 24, last loss: 0.0192\n",
      "Training batch 25, last loss: 0.0231\n",
      "Training batch 26, last loss: 0.0231\n",
      "Training batch 27, last loss: 0.0198\n",
      "Training batch 28, last loss: 0.0217\n",
      "Training batch 29, last loss: 0.0193\n",
      "Training batch 30, last loss: 0.0230\n",
      "Training batch 31, last loss: 0.0235\n",
      "Training batch 32, last loss: 0.0217\n",
      "Training batch 33, last loss: 0.0237\n",
      "Training batch 34, last loss: 0.0178\n",
      "Training batch 35, last loss: 0.0224\n",
      "Training batch 36, last loss: 0.0208\n",
      "Training batch 37, last loss: 0.0189\n",
      "Training batch 38, last loss: 0.0189\n",
      "Training batch 39, last loss: 0.0257\n",
      "Training batch 40, last loss: 0.0226\n",
      "Training batch 41, last loss: 0.0202\n",
      "Training batch 42, last loss: 0.0229\n",
      "Training batch 43, last loss: 0.0167\n",
      "Training batch 44, last loss: 0.0182\n",
      "Training batch 45, last loss: 0.0236\n",
      "Training batch 46, last loss: 0.0203\n",
      "Training batch 47, last loss: 0.0199\n",
      "Training batch 48, last loss: 0.0190\n",
      "Training batch 49, last loss: 0.0192\n",
      "Training batch 50, last loss: 0.0217\n",
      "Training batch 51, last loss: 0.0197\n",
      "Training batch 52, last loss: 0.0223\n",
      "Training batch 53, last loss: 0.0251\n",
      "Training batch 54, last loss: 0.0222\n",
      "Training batch 55, last loss: 0.0209\n",
      "Training batch 56, last loss: 0.0202\n",
      "Training batch 57, last loss: 0.0199\n",
      "Training batch 58, last loss: 0.0233\n",
      "Training batch 59, last loss: 0.0211\n",
      "Training batch 60, last loss: 0.0220\n",
      "Training batch 61, last loss: 0.0218\n",
      "Training batch 62, last loss: 0.0210\n",
      "Training batch 63, last loss: 0.0214\n",
      "Training batch 64, last loss: 0.0215\n",
      "Training batch 65, last loss: 0.0188\n",
      "Training batch 66, last loss: 0.0219\n",
      "Training batch 67, last loss: 0.0224\n",
      "Training batch 68, last loss: 0.0188\n",
      "Training batch 69, last loss: 0.0211\n",
      "Training batch 70, last loss: 0.0207\n",
      "Training batch 71, last loss: 0.0224\n",
      "Training batch 72, last loss: 0.0191\n",
      "Training batch 73, last loss: 0.0215\n",
      "Training batch 74, last loss: 0.0194\n",
      "Training batch 75, last loss: 0.0202\n",
      "Training batch 76, last loss: 0.0224\n",
      "Training batch 77, last loss: 0.0189\n",
      "Training batch 78, last loss: 0.0217\n",
      "Training batch 79, last loss: 0.0200\n",
      "Training batch 80, last loss: 0.0214\n",
      "Training batch 81, last loss: 0.0193\n",
      "Training batch 82, last loss: 0.0193\n",
      "Training batch 83, last loss: 0.0240\n",
      "Training batch 84, last loss: 0.0215\n",
      "Training batch 85, last loss: 0.0227\n",
      "Training batch 86, last loss: 0.0172\n",
      "Training batch 87, last loss: 0.0220\n",
      "Training batch 88, last loss: 0.0192\n",
      "Training batch 89, last loss: 0.0230\n",
      "Training batch 90, last loss: 0.0200\n",
      "Training batch 91, last loss: 0.0218\n",
      "Training batch 92, last loss: 0.0247\n",
      "Training batch 93, last loss: 0.0279\n",
      "\n",
      "Training epoch 11 completed, last loss: 0.0279\n",
      "\n",
      "Validation batch 1, last loss: 0.0221\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 2, last loss: 0.0211\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 3, last loss: 0.0225\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 4, last loss: 0.0248\n",
      "Validation accuracy: 0.4188\n",
      "Validation batch 5, last loss: 0.0203\n",
      "Validation accuracy: 0.4350\n",
      "Validation batch 6, last loss: 0.0153\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 7, last loss: 0.0209\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 8, last loss: 0.0199\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 9, last loss: 0.0222\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 10, last loss: 0.0182\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 11, last loss: 0.0230\n",
      "Validation accuracy: 0.4864\n",
      "Validation batch 12, last loss: 0.0158\n",
      "Validation accuracy: 0.5042\n",
      "Validation batch 13, last loss: 0.0218\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 14, last loss: 0.0196\n",
      "Validation accuracy: 0.5036\n",
      "Validation batch 15, last loss: 0.0237\n",
      "Validation accuracy: 0.4967\n",
      "Validation batch 16, last loss: 0.0191\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 17, last loss: 0.0188\n",
      "Validation accuracy: 0.5044\n",
      "Validation batch 18, last loss: 0.0195\n",
      "Validation accuracy: 0.5069\n",
      "Validation batch 19, last loss: 0.0250\n",
      "Validation accuracy: 0.4987\n",
      "Validation batch 20, last loss: 0.0171\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 21, last loss: 0.0233\n",
      "Validation accuracy: 0.5024\n",
      "Validation batch 22, last loss: 0.0215\n",
      "Validation accuracy: 0.5011\n",
      "Validation batch 23, last loss: 0.0173\n",
      "Validation accuracy: 0.5065\n",
      "Validation batch 24, last loss: 0.0195\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 25, last loss: 0.0223\n",
      "Validation accuracy: 0.5060\n",
      "Validation batch 26, last loss: 0.0146\n",
      "Validation accuracy: 0.5144\n",
      "Validation batch 27, last loss: 0.0265\n",
      "Validation accuracy: 0.5065\n",
      "Validation batch 28, last loss: 0.0225\n",
      "Validation accuracy: 0.5036\n",
      "Validation batch 29, last loss: 0.0206\n",
      "Validation accuracy: 0.5034\n",
      "Validation batch 30, last loss: 0.0248\n",
      "Validation accuracy: 0.4983\n",
      "Validation batch 31, last loss: 0.0249\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 32, last loss: 0.0216\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 33, last loss: 0.0222\n",
      "Validation accuracy: 0.4924\n",
      "Validation batch 34, last loss: 0.0205\n",
      "Validation accuracy: 0.4926\n",
      "Validation batch 35, last loss: 0.0225\n",
      "Validation accuracy: 0.4907\n",
      "Validation batch 36, last loss: 0.0218\n",
      "Validation accuracy: 0.4896\n",
      "Validation batch 37, last loss: 0.0233\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 38, last loss: 0.0210\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 39, last loss: 0.0203\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 40, last loss: 0.0244\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 11 completed, last loss: 0.0244, accuracy: 0.4871\n",
      "\n",
      "Epoch 12/30\n",
      "Training batch 1, last loss: 0.0235\n",
      "Training batch 2, last loss: 0.0201\n",
      "Training batch 3, last loss: 0.0208\n",
      "Training batch 4, last loss: 0.0239\n",
      "Training batch 5, last loss: 0.0223\n",
      "Training batch 6, last loss: 0.0209\n",
      "Training batch 7, last loss: 0.0211\n",
      "Training batch 8, last loss: 0.0206\n",
      "Training batch 9, last loss: 0.0189\n",
      "Training batch 10, last loss: 0.0209\n",
      "Training batch 11, last loss: 0.0177\n",
      "Training batch 12, last loss: 0.0195\n",
      "Training batch 13, last loss: 0.0206\n",
      "Training batch 14, last loss: 0.0229\n",
      "Training batch 15, last loss: 0.0209\n",
      "Training batch 16, last loss: 0.0221\n",
      "Training batch 17, last loss: 0.0189\n",
      "Training batch 18, last loss: 0.0206\n",
      "Training batch 19, last loss: 0.0252\n",
      "Training batch 20, last loss: 0.0219\n",
      "Training batch 21, last loss: 0.0181\n",
      "Training batch 22, last loss: 0.0223\n",
      "Training batch 23, last loss: 0.0217\n",
      "Training batch 24, last loss: 0.0191\n",
      "Training batch 25, last loss: 0.0236\n",
      "Training batch 26, last loss: 0.0184\n",
      "Training batch 27, last loss: 0.0209\n",
      "Training batch 28, last loss: 0.0218\n",
      "Training batch 29, last loss: 0.0168\n",
      "Training batch 30, last loss: 0.0204\n",
      "Training batch 31, last loss: 0.0194\n",
      "Training batch 32, last loss: 0.0263\n",
      "Training batch 33, last loss: 0.0217\n",
      "Training batch 34, last loss: 0.0252\n",
      "Training batch 35, last loss: 0.0203\n",
      "Training batch 36, last loss: 0.0255\n",
      "Training batch 37, last loss: 0.0214\n",
      "Training batch 38, last loss: 0.0206\n",
      "Training batch 39, last loss: 0.0216\n",
      "Training batch 40, last loss: 0.0224\n",
      "Training batch 41, last loss: 0.0204\n",
      "Training batch 42, last loss: 0.0198\n",
      "Training batch 43, last loss: 0.0240\n",
      "Training batch 44, last loss: 0.0171\n",
      "Training batch 45, last loss: 0.0185\n",
      "Training batch 46, last loss: 0.0200\n",
      "Training batch 47, last loss: 0.0233\n",
      "Training batch 48, last loss: 0.0189\n",
      "Training batch 49, last loss: 0.0225\n",
      "Training batch 50, last loss: 0.0207\n",
      "Training batch 51, last loss: 0.0195\n",
      "Training batch 52, last loss: 0.0196\n",
      "Training batch 53, last loss: 0.0217\n",
      "Training batch 54, last loss: 0.0233\n",
      "Training batch 55, last loss: 0.0205\n",
      "Training batch 56, last loss: 0.0222\n",
      "Training batch 57, last loss: 0.0185\n",
      "Training batch 58, last loss: 0.0255\n",
      "Training batch 59, last loss: 0.0234\n",
      "Training batch 60, last loss: 0.0193\n",
      "Training batch 61, last loss: 0.0176\n",
      "Training batch 62, last loss: 0.0214\n",
      "Training batch 63, last loss: 0.0232\n",
      "Training batch 64, last loss: 0.0210\n",
      "Training batch 65, last loss: 0.0229\n",
      "Training batch 66, last loss: 0.0257\n",
      "Training batch 67, last loss: 0.0198\n",
      "Training batch 68, last loss: 0.0220\n",
      "Training batch 69, last loss: 0.0220\n",
      "Training batch 70, last loss: 0.0223\n",
      "Training batch 71, last loss: 0.0226\n",
      "Training batch 72, last loss: 0.0249\n",
      "Training batch 73, last loss: 0.0216\n",
      "Training batch 74, last loss: 0.0211\n",
      "Training batch 75, last loss: 0.0227\n",
      "Training batch 76, last loss: 0.0175\n",
      "Training batch 77, last loss: 0.0219\n",
      "Training batch 78, last loss: 0.0233\n",
      "Training batch 79, last loss: 0.0236\n",
      "Training batch 80, last loss: 0.0213\n",
      "Training batch 81, last loss: 0.0235\n",
      "Training batch 82, last loss: 0.0195\n",
      "Training batch 83, last loss: 0.0214\n",
      "Training batch 84, last loss: 0.0203\n",
      "Training batch 85, last loss: 0.0215\n",
      "Training batch 86, last loss: 0.0225\n",
      "Training batch 87, last loss: 0.0217\n",
      "Training batch 88, last loss: 0.0237\n",
      "Training batch 89, last loss: 0.0180\n",
      "Training batch 90, last loss: 0.0196\n",
      "Training batch 91, last loss: 0.0226\n",
      "Training batch 92, last loss: 0.0239\n",
      "Training batch 93, last loss: 0.0239\n",
      "\n",
      "Training epoch 12 completed, last loss: 0.0239\n",
      "\n",
      "Validation batch 1, last loss: 0.0194\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0249\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 3, last loss: 0.0213\n",
      "Validation accuracy: 0.4583\n",
      "Validation batch 4, last loss: 0.0222\n",
      "Validation accuracy: 0.4562\n",
      "Validation batch 5, last loss: 0.0175\n",
      "Validation accuracy: 0.4900\n",
      "Validation batch 6, last loss: 0.0205\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 7, last loss: 0.0205\n",
      "Validation accuracy: 0.4929\n",
      "Validation batch 8, last loss: 0.0231\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0176\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 10, last loss: 0.0221\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 11, last loss: 0.0222\n",
      "Validation accuracy: 0.4909\n",
      "Validation batch 12, last loss: 0.0218\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 13, last loss: 0.0224\n",
      "Validation accuracy: 0.4827\n",
      "Validation batch 14, last loss: 0.0196\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 15, last loss: 0.0210\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 16, last loss: 0.0175\n",
      "Validation accuracy: 0.4969\n",
      "Validation batch 17, last loss: 0.0215\n",
      "Validation accuracy: 0.4956\n",
      "Validation batch 18, last loss: 0.0197\n",
      "Validation accuracy: 0.4986\n",
      "Validation batch 19, last loss: 0.0205\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 20, last loss: 0.0250\n",
      "Validation accuracy: 0.4925\n",
      "Validation batch 21, last loss: 0.0185\n",
      "Validation accuracy: 0.4964\n",
      "Validation batch 22, last loss: 0.0234\n",
      "Validation accuracy: 0.4920\n",
      "Validation batch 23, last loss: 0.0230\n",
      "Validation accuracy: 0.4891\n",
      "Validation batch 24, last loss: 0.0206\n",
      "Validation accuracy: 0.4896\n",
      "Validation batch 25, last loss: 0.0234\n",
      "Validation accuracy: 0.4860\n",
      "Validation batch 26, last loss: 0.0196\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 27, last loss: 0.0199\n",
      "Validation accuracy: 0.4898\n",
      "Validation batch 28, last loss: 0.0181\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 29, last loss: 0.0193\n",
      "Validation accuracy: 0.4957\n",
      "Validation batch 30, last loss: 0.0228\n",
      "Validation accuracy: 0.4933\n",
      "Validation batch 31, last loss: 0.0255\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 32, last loss: 0.0216\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 33, last loss: 0.0204\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 34, last loss: 0.0218\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 35, last loss: 0.0195\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 36, last loss: 0.0196\n",
      "Validation accuracy: 0.4896\n",
      "Validation batch 37, last loss: 0.0202\n",
      "Validation accuracy: 0.4905\n",
      "Validation batch 38, last loss: 0.0238\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 39, last loss: 0.0191\n",
      "Validation accuracy: 0.4897\n",
      "Validation batch 40, last loss: 0.0254\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 12 completed, last loss: 0.0254, accuracy: 0.4871\n",
      "\n",
      "Epoch 13/30\n",
      "Training batch 1, last loss: 0.0194\n",
      "Training batch 2, last loss: 0.0163\n",
      "Training batch 3, last loss: 0.0220\n",
      "Training batch 4, last loss: 0.0235\n",
      "Training batch 5, last loss: 0.0223\n",
      "Training batch 6, last loss: 0.0209\n",
      "Training batch 7, last loss: 0.0242\n",
      "Training batch 8, last loss: 0.0225\n",
      "Training batch 9, last loss: 0.0207\n",
      "Training batch 10, last loss: 0.0176\n",
      "Training batch 11, last loss: 0.0226\n",
      "Training batch 12, last loss: 0.0184\n",
      "Training batch 13, last loss: 0.0261\n",
      "Training batch 14, last loss: 0.0221\n",
      "Training batch 15, last loss: 0.0229\n",
      "Training batch 16, last loss: 0.0213\n",
      "Training batch 17, last loss: 0.0211\n",
      "Training batch 18, last loss: 0.0204\n",
      "Training batch 19, last loss: 0.0191\n",
      "Training batch 20, last loss: 0.0228\n",
      "Training batch 21, last loss: 0.0203\n",
      "Training batch 22, last loss: 0.0220\n",
      "Training batch 23, last loss: 0.0221\n",
      "Training batch 24, last loss: 0.0202\n",
      "Training batch 25, last loss: 0.0193\n",
      "Training batch 26, last loss: 0.0195\n",
      "Training batch 27, last loss: 0.0235\n",
      "Training batch 28, last loss: 0.0256\n",
      "Training batch 29, last loss: 0.0176\n",
      "Training batch 30, last loss: 0.0239\n",
      "Training batch 31, last loss: 0.0208\n",
      "Training batch 32, last loss: 0.0208\n",
      "Training batch 33, last loss: 0.0224\n",
      "Training batch 34, last loss: 0.0196\n",
      "Training batch 35, last loss: 0.0180\n",
      "Training batch 36, last loss: 0.0228\n",
      "Training batch 37, last loss: 0.0197\n",
      "Training batch 38, last loss: 0.0205\n",
      "Training batch 39, last loss: 0.0206\n",
      "Training batch 40, last loss: 0.0222\n",
      "Training batch 41, last loss: 0.0215\n",
      "Training batch 42, last loss: 0.0250\n",
      "Training batch 43, last loss: 0.0173\n",
      "Training batch 44, last loss: 0.0216\n",
      "Training batch 45, last loss: 0.0225\n",
      "Training batch 46, last loss: 0.0215\n",
      "Training batch 47, last loss: 0.0246\n",
      "Training batch 48, last loss: 0.0170\n",
      "Training batch 49, last loss: 0.0219\n",
      "Training batch 50, last loss: 0.0181\n",
      "Training batch 51, last loss: 0.0237\n",
      "Training batch 52, last loss: 0.0211\n",
      "Training batch 53, last loss: 0.0203\n",
      "Training batch 54, last loss: 0.0239\n",
      "Training batch 55, last loss: 0.0201\n",
      "Training batch 56, last loss: 0.0218\n",
      "Training batch 57, last loss: 0.0223\n",
      "Training batch 58, last loss: 0.0250\n",
      "Training batch 59, last loss: 0.0184\n",
      "Training batch 60, last loss: 0.0232\n",
      "Training batch 61, last loss: 0.0223\n",
      "Training batch 62, last loss: 0.0237\n",
      "Training batch 63, last loss: 0.0211\n",
      "Training batch 64, last loss: 0.0202\n",
      "Training batch 65, last loss: 0.0191\n",
      "Training batch 66, last loss: 0.0181\n",
      "Training batch 67, last loss: 0.0244\n",
      "Training batch 68, last loss: 0.0230\n",
      "Training batch 69, last loss: 0.0218\n",
      "Training batch 70, last loss: 0.0214\n",
      "Training batch 71, last loss: 0.0217\n",
      "Training batch 72, last loss: 0.0261\n",
      "Training batch 73, last loss: 0.0210\n",
      "Training batch 74, last loss: 0.0195\n",
      "Training batch 75, last loss: 0.0192\n",
      "Training batch 76, last loss: 0.0272\n",
      "Training batch 77, last loss: 0.0193\n",
      "Training batch 78, last loss: 0.0233\n",
      "Training batch 79, last loss: 0.0216\n",
      "Training batch 80, last loss: 0.0201\n",
      "Training batch 81, last loss: 0.0214\n",
      "Training batch 82, last loss: 0.0200\n",
      "Training batch 83, last loss: 0.0198\n",
      "Training batch 84, last loss: 0.0225\n",
      "Training batch 85, last loss: 0.0191\n",
      "Training batch 86, last loss: 0.0206\n",
      "Training batch 87, last loss: 0.0231\n",
      "Training batch 88, last loss: 0.0187\n",
      "Training batch 89, last loss: 0.0201\n",
      "Training batch 90, last loss: 0.0205\n",
      "Training batch 91, last loss: 0.0216\n",
      "Training batch 92, last loss: 0.0262\n",
      "Training batch 93, last loss: 0.0199\n",
      "\n",
      "Training epoch 13 completed, last loss: 0.0199\n",
      "\n",
      "Validation batch 1, last loss: 0.0224\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 2, last loss: 0.0231\n",
      "Validation accuracy: 0.4375\n",
      "Validation batch 3, last loss: 0.0195\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 4, last loss: 0.0235\n",
      "Validation accuracy: 0.4562\n",
      "Validation batch 5, last loss: 0.0184\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 6, last loss: 0.0208\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 7, last loss: 0.0227\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 8, last loss: 0.0235\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 9, last loss: 0.0195\n",
      "Validation accuracy: 0.4778\n",
      "Validation batch 10, last loss: 0.0198\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 11, last loss: 0.0201\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 12, last loss: 0.0201\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 13, last loss: 0.0220\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 14, last loss: 0.0210\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 15, last loss: 0.0176\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 16, last loss: 0.0208\n",
      "Validation accuracy: 0.4953\n",
      "Validation batch 17, last loss: 0.0182\n",
      "Validation accuracy: 0.5015\n",
      "Validation batch 18, last loss: 0.0187\n",
      "Validation accuracy: 0.5056\n",
      "Validation batch 19, last loss: 0.0196\n",
      "Validation accuracy: 0.5079\n",
      "Validation batch 20, last loss: 0.0222\n",
      "Validation accuracy: 0.5050\n",
      "Validation batch 21, last loss: 0.0187\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 22, last loss: 0.0206\n",
      "Validation accuracy: 0.5080\n",
      "Validation batch 23, last loss: 0.0182\n",
      "Validation accuracy: 0.5120\n",
      "Validation batch 24, last loss: 0.0233\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 25, last loss: 0.0189\n",
      "Validation accuracy: 0.5110\n",
      "Validation batch 26, last loss: 0.0220\n",
      "Validation accuracy: 0.5087\n",
      "Validation batch 27, last loss: 0.0210\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 28, last loss: 0.0194\n",
      "Validation accuracy: 0.5098\n",
      "Validation batch 29, last loss: 0.0199\n",
      "Validation accuracy: 0.5103\n",
      "Validation batch 30, last loss: 0.0244\n",
      "Validation accuracy: 0.5050\n",
      "Validation batch 31, last loss: 0.0226\n",
      "Validation accuracy: 0.5032\n",
      "Validation batch 32, last loss: 0.0231\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 33, last loss: 0.0262\n",
      "Validation accuracy: 0.4932\n",
      "Validation batch 34, last loss: 0.0217\n",
      "Validation accuracy: 0.4919\n",
      "Validation batch 35, last loss: 0.0216\n",
      "Validation accuracy: 0.4914\n",
      "Validation batch 36, last loss: 0.0241\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 37, last loss: 0.0213\n",
      "Validation accuracy: 0.4878\n",
      "Validation batch 38, last loss: 0.0215\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 39, last loss: 0.0220\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0206\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 13 completed, last loss: 0.0206, accuracy: 0.4871\n",
      "\n",
      "Epoch 14/30\n",
      "Training batch 1, last loss: 0.0209\n",
      "Training batch 2, last loss: 0.0219\n",
      "Training batch 3, last loss: 0.0191\n",
      "Training batch 4, last loss: 0.0222\n",
      "Training batch 5, last loss: 0.0254\n",
      "Training batch 6, last loss: 0.0231\n",
      "Training batch 7, last loss: 0.0233\n",
      "Training batch 8, last loss: 0.0201\n",
      "Training batch 9, last loss: 0.0212\n",
      "Training batch 10, last loss: 0.0211\n",
      "Training batch 11, last loss: 0.0245\n",
      "Training batch 12, last loss: 0.0215\n",
      "Training batch 13, last loss: 0.0236\n",
      "Training batch 14, last loss: 0.0210\n",
      "Training batch 15, last loss: 0.0224\n",
      "Training batch 16, last loss: 0.0196\n",
      "Training batch 17, last loss: 0.0216\n",
      "Training batch 18, last loss: 0.0225\n",
      "Training batch 19, last loss: 0.0228\n",
      "Training batch 20, last loss: 0.0172\n",
      "Training batch 21, last loss: 0.0214\n",
      "Training batch 22, last loss: 0.0199\n",
      "Training batch 23, last loss: 0.0248\n",
      "Training batch 24, last loss: 0.0227\n",
      "Training batch 25, last loss: 0.0188\n",
      "Training batch 26, last loss: 0.0179\n",
      "Training batch 27, last loss: 0.0181\n",
      "Training batch 28, last loss: 0.0228\n",
      "Training batch 29, last loss: 0.0199\n",
      "Training batch 30, last loss: 0.0202\n",
      "Training batch 31, last loss: 0.0243\n",
      "Training batch 32, last loss: 0.0222\n",
      "Training batch 33, last loss: 0.0217\n",
      "Training batch 34, last loss: 0.0212\n",
      "Training batch 35, last loss: 0.0240\n",
      "Training batch 36, last loss: 0.0185\n",
      "Training batch 37, last loss: 0.0203\n",
      "Training batch 38, last loss: 0.0198\n",
      "Training batch 39, last loss: 0.0220\n",
      "Training batch 40, last loss: 0.0189\n",
      "Training batch 41, last loss: 0.0190\n",
      "Training batch 42, last loss: 0.0221\n",
      "Training batch 43, last loss: 0.0241\n",
      "Training batch 44, last loss: 0.0171\n",
      "Training batch 45, last loss: 0.0193\n",
      "Training batch 46, last loss: 0.0232\n",
      "Training batch 47, last loss: 0.0177\n",
      "Training batch 48, last loss: 0.0211\n",
      "Training batch 49, last loss: 0.0258\n",
      "Training batch 50, last loss: 0.0219\n",
      "Training batch 51, last loss: 0.0179\n",
      "Training batch 52, last loss: 0.0218\n",
      "Training batch 53, last loss: 0.0211\n",
      "Training batch 54, last loss: 0.0229\n",
      "Training batch 55, last loss: 0.0211\n",
      "Training batch 56, last loss: 0.0234\n",
      "Training batch 57, last loss: 0.0218\n",
      "Training batch 58, last loss: 0.0209\n",
      "Training batch 59, last loss: 0.0224\n",
      "Training batch 60, last loss: 0.0231\n",
      "Training batch 61, last loss: 0.0172\n",
      "Training batch 62, last loss: 0.0240\n",
      "Training batch 63, last loss: 0.0224\n",
      "Training batch 64, last loss: 0.0224\n",
      "Training batch 65, last loss: 0.0200\n",
      "Training batch 66, last loss: 0.0200\n",
      "Training batch 67, last loss: 0.0227\n",
      "Training batch 68, last loss: 0.0222\n",
      "Training batch 69, last loss: 0.0220\n",
      "Training batch 70, last loss: 0.0215\n",
      "Training batch 71, last loss: 0.0215\n",
      "Training batch 72, last loss: 0.0213\n",
      "Training batch 73, last loss: 0.0210\n",
      "Training batch 74, last loss: 0.0214\n",
      "Training batch 75, last loss: 0.0209\n",
      "Training batch 76, last loss: 0.0179\n",
      "Training batch 77, last loss: 0.0178\n",
      "Training batch 78, last loss: 0.0220\n",
      "Training batch 79, last loss: 0.0239\n",
      "Training batch 80, last loss: 0.0239\n",
      "Training batch 81, last loss: 0.0187\n",
      "Training batch 82, last loss: 0.0218\n",
      "Training batch 83, last loss: 0.0178\n",
      "Training batch 84, last loss: 0.0248\n",
      "Training batch 85, last loss: 0.0248\n",
      "Training batch 86, last loss: 0.0221\n",
      "Training batch 87, last loss: 0.0235\n",
      "Training batch 88, last loss: 0.0191\n",
      "Training batch 89, last loss: 0.0206\n",
      "Training batch 90, last loss: 0.0189\n",
      "Training batch 91, last loss: 0.0217\n",
      "Training batch 92, last loss: 0.0206\n",
      "Training batch 93, last loss: 0.0236\n",
      "\n",
      "Training epoch 14 completed, last loss: 0.0236\n",
      "\n",
      "Validation batch 1, last loss: 0.0213\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 2, last loss: 0.0213\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 3, last loss: 0.0250\n",
      "Validation accuracy: 0.4333\n",
      "Validation batch 4, last loss: 0.0211\n",
      "Validation accuracy: 0.4437\n",
      "Validation batch 5, last loss: 0.0225\n",
      "Validation accuracy: 0.4400\n",
      "Validation batch 6, last loss: 0.0169\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 7, last loss: 0.0225\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 8, last loss: 0.0234\n",
      "Validation accuracy: 0.4594\n",
      "Validation batch 9, last loss: 0.0202\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 10, last loss: 0.0205\n",
      "Validation accuracy: 0.4700\n",
      "Validation batch 11, last loss: 0.0217\n",
      "Validation accuracy: 0.4705\n",
      "Validation batch 12, last loss: 0.0192\n",
      "Validation accuracy: 0.4771\n",
      "Validation batch 13, last loss: 0.0236\n",
      "Validation accuracy: 0.4712\n",
      "Validation batch 14, last loss: 0.0248\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 15, last loss: 0.0215\n",
      "Validation accuracy: 0.4633\n",
      "Validation batch 16, last loss: 0.0165\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 17, last loss: 0.0167\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 18, last loss: 0.0208\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 19, last loss: 0.0245\n",
      "Validation accuracy: 0.4789\n",
      "Validation batch 20, last loss: 0.0239\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 21, last loss: 0.0210\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 22, last loss: 0.0219\n",
      "Validation accuracy: 0.4739\n",
      "Validation batch 23, last loss: 0.0227\n",
      "Validation accuracy: 0.4728\n",
      "Validation batch 24, last loss: 0.0183\n",
      "Validation accuracy: 0.4781\n",
      "Validation batch 25, last loss: 0.0236\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 26, last loss: 0.0175\n",
      "Validation accuracy: 0.4808\n",
      "Validation batch 27, last loss: 0.0202\n",
      "Validation accuracy: 0.4815\n",
      "Validation batch 28, last loss: 0.0209\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 29, last loss: 0.0207\n",
      "Validation accuracy: 0.4836\n",
      "Validation batch 30, last loss: 0.0190\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 31, last loss: 0.0218\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 32, last loss: 0.0216\n",
      "Validation accuracy: 0.4852\n",
      "Validation batch 33, last loss: 0.0222\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 34, last loss: 0.0199\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 35, last loss: 0.0191\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 36, last loss: 0.0158\n",
      "Validation accuracy: 0.4931\n",
      "Validation batch 37, last loss: 0.0244\n",
      "Validation accuracy: 0.4899\n",
      "Validation batch 38, last loss: 0.0243\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 39, last loss: 0.0206\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0214\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 14 completed, last loss: 0.0214, accuracy: 0.4871\n",
      "\n",
      "Epoch 15/30\n",
      "Training batch 1, last loss: 0.0220\n",
      "Training batch 2, last loss: 0.0186\n",
      "Training batch 3, last loss: 0.0193\n",
      "Training batch 4, last loss: 0.0223\n",
      "Training batch 5, last loss: 0.0185\n",
      "Training batch 6, last loss: 0.0259\n",
      "Training batch 7, last loss: 0.0215\n",
      "Training batch 8, last loss: 0.0195\n",
      "Training batch 9, last loss: 0.0183\n",
      "Training batch 10, last loss: 0.0218\n",
      "Training batch 11, last loss: 0.0216\n",
      "Training batch 12, last loss: 0.0220\n",
      "Training batch 13, last loss: 0.0172\n",
      "Training batch 14, last loss: 0.0275\n",
      "Training batch 15, last loss: 0.0190\n",
      "Training batch 16, last loss: 0.0208\n",
      "Training batch 17, last loss: 0.0246\n",
      "Training batch 18, last loss: 0.0215\n",
      "Training batch 19, last loss: 0.0204\n",
      "Training batch 20, last loss: 0.0224\n",
      "Training batch 21, last loss: 0.0222\n",
      "Training batch 22, last loss: 0.0226\n",
      "Training batch 23, last loss: 0.0215\n",
      "Training batch 24, last loss: 0.0229\n",
      "Training batch 25, last loss: 0.0245\n",
      "Training batch 26, last loss: 0.0221\n",
      "Training batch 27, last loss: 0.0213\n",
      "Training batch 28, last loss: 0.0246\n",
      "Training batch 29, last loss: 0.0204\n",
      "Training batch 30, last loss: 0.0223\n",
      "Training batch 31, last loss: 0.0226\n",
      "Training batch 32, last loss: 0.0227\n",
      "Training batch 33, last loss: 0.0222\n",
      "Training batch 34, last loss: 0.0192\n",
      "Training batch 35, last loss: 0.0207\n",
      "Training batch 36, last loss: 0.0208\n",
      "Training batch 37, last loss: 0.0172\n",
      "Training batch 38, last loss: 0.0208\n",
      "Training batch 39, last loss: 0.0213\n",
      "Training batch 40, last loss: 0.0184\n",
      "Training batch 41, last loss: 0.0222\n",
      "Training batch 42, last loss: 0.0221\n",
      "Training batch 43, last loss: 0.0196\n",
      "Training batch 44, last loss: 0.0198\n",
      "Training batch 45, last loss: 0.0212\n",
      "Training batch 46, last loss: 0.0169\n",
      "Training batch 47, last loss: 0.0224\n",
      "Training batch 48, last loss: 0.0260\n",
      "Training batch 49, last loss: 0.0219\n",
      "Training batch 50, last loss: 0.0205\n",
      "Training batch 51, last loss: 0.0235\n",
      "Training batch 52, last loss: 0.0236\n",
      "Training batch 53, last loss: 0.0236\n",
      "Training batch 54, last loss: 0.0202\n",
      "Training batch 55, last loss: 0.0216\n",
      "Training batch 56, last loss: 0.0202\n",
      "Training batch 57, last loss: 0.0195\n",
      "Training batch 58, last loss: 0.0191\n",
      "Training batch 59, last loss: 0.0192\n",
      "Training batch 60, last loss: 0.0223\n",
      "Training batch 61, last loss: 0.0186\n",
      "Training batch 62, last loss: 0.0187\n",
      "Training batch 63, last loss: 0.0244\n",
      "Training batch 64, last loss: 0.0205\n",
      "Training batch 65, last loss: 0.0200\n",
      "Training batch 66, last loss: 0.0223\n",
      "Training batch 67, last loss: 0.0199\n",
      "Training batch 68, last loss: 0.0230\n",
      "Training batch 69, last loss: 0.0226\n",
      "Training batch 70, last loss: 0.0212\n",
      "Training batch 71, last loss: 0.0193\n",
      "Training batch 72, last loss: 0.0224\n",
      "Training batch 73, last loss: 0.0224\n",
      "Training batch 74, last loss: 0.0193\n",
      "Training batch 75, last loss: 0.0250\n",
      "Training batch 76, last loss: 0.0211\n",
      "Training batch 77, last loss: 0.0235\n",
      "Training batch 78, last loss: 0.0222\n",
      "Training batch 79, last loss: 0.0209\n",
      "Training batch 80, last loss: 0.0236\n",
      "Training batch 81, last loss: 0.0201\n",
      "Training batch 82, last loss: 0.0239\n",
      "Training batch 83, last loss: 0.0241\n",
      "Training batch 84, last loss: 0.0213\n",
      "Training batch 85, last loss: 0.0188\n",
      "Training batch 86, last loss: 0.0184\n",
      "Training batch 87, last loss: 0.0191\n",
      "Training batch 88, last loss: 0.0164\n",
      "Training batch 89, last loss: 0.0234\n",
      "Training batch 90, last loss: 0.0213\n",
      "Training batch 91, last loss: 0.0212\n",
      "Training batch 92, last loss: 0.0200\n",
      "Training batch 93, last loss: 0.0173\n",
      "\n",
      "Training epoch 15 completed, last loss: 0.0173\n",
      "\n",
      "Validation batch 1, last loss: 0.0243\n",
      "Validation accuracy: 0.3750\n",
      "Validation batch 2, last loss: 0.0222\n",
      "Validation accuracy: 0.4125\n",
      "Validation batch 3, last loss: 0.0212\n",
      "Validation accuracy: 0.4333\n",
      "Validation batch 4, last loss: 0.0174\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 5, last loss: 0.0238\n",
      "Validation accuracy: 0.4600\n",
      "Validation batch 6, last loss: 0.0229\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 7, last loss: 0.0179\n",
      "Validation accuracy: 0.4714\n",
      "Validation batch 8, last loss: 0.0200\n",
      "Validation accuracy: 0.4781\n",
      "Validation batch 9, last loss: 0.0245\n",
      "Validation accuracy: 0.4639\n",
      "Validation batch 10, last loss: 0.0218\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 11, last loss: 0.0227\n",
      "Validation accuracy: 0.4591\n",
      "Validation batch 12, last loss: 0.0212\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 13, last loss: 0.0175\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 14, last loss: 0.0206\n",
      "Validation accuracy: 0.4768\n",
      "Validation batch 15, last loss: 0.0200\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 16, last loss: 0.0203\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 17, last loss: 0.0230\n",
      "Validation accuracy: 0.4794\n",
      "Validation batch 18, last loss: 0.0236\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 19, last loss: 0.0236\n",
      "Validation accuracy: 0.4711\n",
      "Validation batch 20, last loss: 0.0202\n",
      "Validation accuracy: 0.4738\n",
      "Validation batch 21, last loss: 0.0189\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 22, last loss: 0.0208\n",
      "Validation accuracy: 0.4795\n",
      "Validation batch 23, last loss: 0.0224\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 24, last loss: 0.0205\n",
      "Validation accuracy: 0.4802\n",
      "Validation batch 25, last loss: 0.0223\n",
      "Validation accuracy: 0.4780\n",
      "Validation batch 26, last loss: 0.0214\n",
      "Validation accuracy: 0.4779\n",
      "Validation batch 27, last loss: 0.0242\n",
      "Validation accuracy: 0.4741\n",
      "Validation batch 28, last loss: 0.0214\n",
      "Validation accuracy: 0.4741\n",
      "Validation batch 29, last loss: 0.0229\n",
      "Validation accuracy: 0.4724\n",
      "Validation batch 30, last loss: 0.0165\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 31, last loss: 0.0263\n",
      "Validation accuracy: 0.4734\n",
      "Validation batch 32, last loss: 0.0197\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 33, last loss: 0.0231\n",
      "Validation accuracy: 0.4735\n",
      "Validation batch 34, last loss: 0.0201\n",
      "Validation accuracy: 0.4757\n",
      "Validation batch 35, last loss: 0.0232\n",
      "Validation accuracy: 0.4736\n",
      "Validation batch 36, last loss: 0.0208\n",
      "Validation accuracy: 0.4743\n",
      "Validation batch 37, last loss: 0.0173\n",
      "Validation accuracy: 0.4784\n",
      "Validation batch 38, last loss: 0.0214\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 39, last loss: 0.0167\n",
      "Validation accuracy: 0.4827\n",
      "Validation batch 40, last loss: 0.0143\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 15 completed, last loss: 0.0143, accuracy: 0.4871\n",
      "\n",
      "Epoch 16/30\n",
      "Training batch 1, last loss: 0.0203\n",
      "Training batch 2, last loss: 0.0241\n",
      "Training batch 3, last loss: 0.0205\n",
      "Training batch 4, last loss: 0.0234\n",
      "Training batch 5, last loss: 0.0196\n",
      "Training batch 6, last loss: 0.0219\n",
      "Training batch 7, last loss: 0.0182\n",
      "Training batch 8, last loss: 0.0231\n",
      "Training batch 9, last loss: 0.0224\n",
      "Training batch 10, last loss: 0.0237\n",
      "Training batch 11, last loss: 0.0213\n",
      "Training batch 12, last loss: 0.0214\n",
      "Training batch 13, last loss: 0.0181\n",
      "Training batch 14, last loss: 0.0202\n",
      "Training batch 15, last loss: 0.0198\n",
      "Training batch 16, last loss: 0.0226\n",
      "Training batch 17, last loss: 0.0193\n",
      "Training batch 18, last loss: 0.0189\n",
      "Training batch 19, last loss: 0.0188\n",
      "Training batch 20, last loss: 0.0199\n",
      "Training batch 21, last loss: 0.0231\n",
      "Training batch 22, last loss: 0.0186\n",
      "Training batch 23, last loss: 0.0196\n",
      "Training batch 24, last loss: 0.0223\n",
      "Training batch 25, last loss: 0.0174\n",
      "Training batch 26, last loss: 0.0198\n",
      "Training batch 27, last loss: 0.0221\n",
      "Training batch 28, last loss: 0.0215\n",
      "Training batch 29, last loss: 0.0224\n",
      "Training batch 30, last loss: 0.0236\n",
      "Training batch 31, last loss: 0.0209\n",
      "Training batch 32, last loss: 0.0230\n",
      "Training batch 33, last loss: 0.0223\n",
      "Training batch 34, last loss: 0.0214\n",
      "Training batch 35, last loss: 0.0186\n",
      "Training batch 36, last loss: 0.0198\n",
      "Training batch 37, last loss: 0.0237\n",
      "Training batch 38, last loss: 0.0187\n",
      "Training batch 39, last loss: 0.0206\n",
      "Training batch 40, last loss: 0.0174\n",
      "Training batch 41, last loss: 0.0210\n",
      "Training batch 42, last loss: 0.0249\n",
      "Training batch 43, last loss: 0.0210\n",
      "Training batch 44, last loss: 0.0215\n",
      "Training batch 45, last loss: 0.0192\n",
      "Training batch 46, last loss: 0.0196\n",
      "Training batch 47, last loss: 0.0226\n",
      "Training batch 48, last loss: 0.0232\n",
      "Training batch 49, last loss: 0.0195\n",
      "Training batch 50, last loss: 0.0231\n",
      "Training batch 51, last loss: 0.0233\n",
      "Training batch 52, last loss: 0.0255\n",
      "Training batch 53, last loss: 0.0213\n",
      "Training batch 54, last loss: 0.0213\n",
      "Training batch 55, last loss: 0.0209\n",
      "Training batch 56, last loss: 0.0199\n",
      "Training batch 57, last loss: 0.0217\n",
      "Training batch 58, last loss: 0.0225\n",
      "Training batch 59, last loss: 0.0228\n",
      "Training batch 60, last loss: 0.0208\n",
      "Training batch 61, last loss: 0.0228\n",
      "Training batch 62, last loss: 0.0195\n",
      "Training batch 63, last loss: 0.0232\n",
      "Training batch 64, last loss: 0.0204\n",
      "Training batch 65, last loss: 0.0207\n",
      "Training batch 66, last loss: 0.0208\n",
      "Training batch 67, last loss: 0.0209\n",
      "Training batch 68, last loss: 0.0237\n",
      "Training batch 69, last loss: 0.0205\n",
      "Training batch 70, last loss: 0.0173\n",
      "Training batch 71, last loss: 0.0222\n",
      "Training batch 72, last loss: 0.0218\n",
      "Training batch 73, last loss: 0.0213\n",
      "Training batch 74, last loss: 0.0227\n",
      "Training batch 75, last loss: 0.0236\n",
      "Training batch 76, last loss: 0.0210\n",
      "Training batch 77, last loss: 0.0230\n",
      "Training batch 78, last loss: 0.0211\n",
      "Training batch 79, last loss: 0.0231\n",
      "Training batch 80, last loss: 0.0206\n",
      "Training batch 81, last loss: 0.0235\n",
      "Training batch 82, last loss: 0.0243\n",
      "Training batch 83, last loss: 0.0223\n",
      "Training batch 84, last loss: 0.0246\n",
      "Training batch 85, last loss: 0.0195\n",
      "Training batch 86, last loss: 0.0201\n",
      "Training batch 87, last loss: 0.0192\n",
      "Training batch 88, last loss: 0.0207\n",
      "Training batch 89, last loss: 0.0209\n",
      "Training batch 90, last loss: 0.0227\n",
      "Training batch 91, last loss: 0.0240\n",
      "Training batch 92, last loss: 0.0259\n",
      "Training batch 93, last loss: 0.0207\n",
      "\n",
      "Training epoch 16 completed, last loss: 0.0207\n",
      "\n",
      "Validation batch 1, last loss: 0.0193\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0216\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 3, last loss: 0.0223\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 4, last loss: 0.0188\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 5, last loss: 0.0207\n",
      "Validation accuracy: 0.5050\n",
      "Validation batch 6, last loss: 0.0205\n",
      "Validation accuracy: 0.5042\n",
      "Validation batch 7, last loss: 0.0196\n",
      "Validation accuracy: 0.5071\n",
      "Validation batch 8, last loss: 0.0222\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 9, last loss: 0.0218\n",
      "Validation accuracy: 0.4972\n",
      "Validation batch 10, last loss: 0.0206\n",
      "Validation accuracy: 0.4975\n",
      "Validation batch 11, last loss: 0.0199\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 12, last loss: 0.0167\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 13, last loss: 0.0237\n",
      "Validation accuracy: 0.5038\n",
      "Validation batch 14, last loss: 0.0229\n",
      "Validation accuracy: 0.4982\n",
      "Validation batch 15, last loss: 0.0227\n",
      "Validation accuracy: 0.4933\n",
      "Validation batch 16, last loss: 0.0173\n",
      "Validation accuracy: 0.5016\n",
      "Validation batch 17, last loss: 0.0219\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 18, last loss: 0.0206\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 19, last loss: 0.0261\n",
      "Validation accuracy: 0.4908\n",
      "Validation batch 20, last loss: 0.0233\n",
      "Validation accuracy: 0.4863\n",
      "Validation batch 21, last loss: 0.0246\n",
      "Validation accuracy: 0.4810\n",
      "Validation batch 22, last loss: 0.0244\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 23, last loss: 0.0201\n",
      "Validation accuracy: 0.4772\n",
      "Validation batch 24, last loss: 0.0196\n",
      "Validation accuracy: 0.4802\n",
      "Validation batch 25, last loss: 0.0217\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 26, last loss: 0.0203\n",
      "Validation accuracy: 0.4817\n",
      "Validation batch 27, last loss: 0.0196\n",
      "Validation accuracy: 0.4843\n",
      "Validation batch 28, last loss: 0.0230\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 29, last loss: 0.0198\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 30, last loss: 0.0218\n",
      "Validation accuracy: 0.4825\n",
      "Validation batch 31, last loss: 0.0194\n",
      "Validation accuracy: 0.4847\n",
      "Validation batch 32, last loss: 0.0228\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 33, last loss: 0.0223\n",
      "Validation accuracy: 0.4818\n",
      "Validation batch 34, last loss: 0.0195\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 35, last loss: 0.0205\n",
      "Validation accuracy: 0.4843\n",
      "Validation batch 36, last loss: 0.0234\n",
      "Validation accuracy: 0.4819\n",
      "Validation batch 37, last loss: 0.0185\n",
      "Validation accuracy: 0.4845\n",
      "Validation batch 38, last loss: 0.0210\n",
      "Validation accuracy: 0.4849\n",
      "Validation batch 39, last loss: 0.0221\n",
      "Validation accuracy: 0.4840\n",
      "Validation batch 40, last loss: 0.0160\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 16 completed, last loss: 0.0160, accuracy: 0.4871\n",
      "\n",
      "Epoch 17/30\n",
      "Training batch 1, last loss: 0.0199\n",
      "Training batch 2, last loss: 0.0196\n",
      "Training batch 3, last loss: 0.0212\n",
      "Training batch 4, last loss: 0.0231\n",
      "Training batch 5, last loss: 0.0224\n",
      "Training batch 6, last loss: 0.0185\n",
      "Training batch 7, last loss: 0.0231\n",
      "Training batch 8, last loss: 0.0205\n",
      "Training batch 9, last loss: 0.0194\n",
      "Training batch 10, last loss: 0.0208\n",
      "Training batch 11, last loss: 0.0234\n",
      "Training batch 12, last loss: 0.0196\n",
      "Training batch 13, last loss: 0.0218\n",
      "Training batch 14, last loss: 0.0197\n",
      "Training batch 15, last loss: 0.0220\n",
      "Training batch 16, last loss: 0.0230\n",
      "Training batch 17, last loss: 0.0220\n",
      "Training batch 18, last loss: 0.0185\n",
      "Training batch 19, last loss: 0.0238\n",
      "Training batch 20, last loss: 0.0232\n",
      "Training batch 21, last loss: 0.0193\n",
      "Training batch 22, last loss: 0.0229\n",
      "Training batch 23, last loss: 0.0202\n",
      "Training batch 24, last loss: 0.0216\n",
      "Training batch 25, last loss: 0.0187\n",
      "Training batch 26, last loss: 0.0184\n",
      "Training batch 27, last loss: 0.0194\n",
      "Training batch 28, last loss: 0.0210\n",
      "Training batch 29, last loss: 0.0241\n",
      "Training batch 30, last loss: 0.0217\n",
      "Training batch 31, last loss: 0.0262\n",
      "Training batch 32, last loss: 0.0227\n",
      "Training batch 33, last loss: 0.0206\n",
      "Training batch 34, last loss: 0.0206\n",
      "Training batch 35, last loss: 0.0181\n",
      "Training batch 36, last loss: 0.0158\n",
      "Training batch 37, last loss: 0.0232\n",
      "Training batch 38, last loss: 0.0209\n",
      "Training batch 39, last loss: 0.0212\n",
      "Training batch 40, last loss: 0.0237\n",
      "Training batch 41, last loss: 0.0184\n",
      "Training batch 42, last loss: 0.0195\n",
      "Training batch 43, last loss: 0.0241\n",
      "Training batch 44, last loss: 0.0199\n",
      "Training batch 45, last loss: 0.0240\n",
      "Training batch 46, last loss: 0.0220\n",
      "Training batch 47, last loss: 0.0248\n",
      "Training batch 48, last loss: 0.0227\n",
      "Training batch 49, last loss: 0.0227\n",
      "Training batch 50, last loss: 0.0195\n",
      "Training batch 51, last loss: 0.0181\n",
      "Training batch 52, last loss: 0.0218\n",
      "Training batch 53, last loss: 0.0253\n",
      "Training batch 54, last loss: 0.0217\n",
      "Training batch 55, last loss: 0.0235\n",
      "Training batch 56, last loss: 0.0212\n",
      "Training batch 57, last loss: 0.0167\n",
      "Training batch 58, last loss: 0.0243\n",
      "Training batch 59, last loss: 0.0195\n",
      "Training batch 60, last loss: 0.0266\n",
      "Training batch 61, last loss: 0.0168\n",
      "Training batch 62, last loss: 0.0289\n",
      "Training batch 63, last loss: 0.0214\n",
      "Training batch 64, last loss: 0.0209\n",
      "Training batch 65, last loss: 0.0218\n",
      "Training batch 66, last loss: 0.0223\n",
      "Training batch 67, last loss: 0.0205\n",
      "Training batch 68, last loss: 0.0200\n",
      "Training batch 69, last loss: 0.0212\n",
      "Training batch 70, last loss: 0.0204\n",
      "Training batch 71, last loss: 0.0220\n",
      "Training batch 72, last loss: 0.0238\n",
      "Training batch 73, last loss: 0.0221\n",
      "Training batch 74, last loss: 0.0227\n",
      "Training batch 75, last loss: 0.0210\n",
      "Training batch 76, last loss: 0.0206\n",
      "Training batch 77, last loss: 0.0204\n",
      "Training batch 78, last loss: 0.0211\n",
      "Training batch 79, last loss: 0.0221\n",
      "Training batch 80, last loss: 0.0202\n",
      "Training batch 81, last loss: 0.0195\n",
      "Training batch 82, last loss: 0.0223\n",
      "Training batch 83, last loss: 0.0195\n",
      "Training batch 84, last loss: 0.0240\n",
      "Training batch 85, last loss: 0.0205\n",
      "Training batch 86, last loss: 0.0238\n",
      "Training batch 87, last loss: 0.0195\n",
      "Training batch 88, last loss: 0.0207\n",
      "Training batch 89, last loss: 0.0218\n",
      "Training batch 90, last loss: 0.0215\n",
      "Training batch 91, last loss: 0.0224\n",
      "Training batch 92, last loss: 0.0182\n",
      "Training batch 93, last loss: 0.0262\n",
      "\n",
      "Training epoch 17 completed, last loss: 0.0262\n",
      "\n",
      "Validation batch 1, last loss: 0.0221\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 2, last loss: 0.0233\n",
      "Validation accuracy: 0.4375\n",
      "Validation batch 3, last loss: 0.0219\n",
      "Validation accuracy: 0.4417\n",
      "Validation batch 4, last loss: 0.0199\n",
      "Validation accuracy: 0.4625\n",
      "Validation batch 5, last loss: 0.0233\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 6, last loss: 0.0224\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 7, last loss: 0.0172\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 8, last loss: 0.0234\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 9, last loss: 0.0219\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 10, last loss: 0.0214\n",
      "Validation accuracy: 0.4675\n",
      "Validation batch 11, last loss: 0.0183\n",
      "Validation accuracy: 0.4773\n",
      "Validation batch 12, last loss: 0.0214\n",
      "Validation accuracy: 0.4771\n",
      "Validation batch 13, last loss: 0.0232\n",
      "Validation accuracy: 0.4712\n",
      "Validation batch 14, last loss: 0.0229\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 15, last loss: 0.0220\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 16, last loss: 0.0203\n",
      "Validation accuracy: 0.4703\n",
      "Validation batch 17, last loss: 0.0216\n",
      "Validation accuracy: 0.4706\n",
      "Validation batch 18, last loss: 0.0186\n",
      "Validation accuracy: 0.4764\n",
      "Validation batch 19, last loss: 0.0194\n",
      "Validation accuracy: 0.4803\n",
      "Validation batch 20, last loss: 0.0231\n",
      "Validation accuracy: 0.4775\n",
      "Validation batch 21, last loss: 0.0209\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 22, last loss: 0.0184\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 23, last loss: 0.0171\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 24, last loss: 0.0234\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 25, last loss: 0.0202\n",
      "Validation accuracy: 0.4880\n",
      "Validation batch 26, last loss: 0.0188\n",
      "Validation accuracy: 0.4904\n",
      "Validation batch 27, last loss: 0.0228\n",
      "Validation accuracy: 0.4880\n",
      "Validation batch 28, last loss: 0.0195\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 29, last loss: 0.0212\n",
      "Validation accuracy: 0.4897\n",
      "Validation batch 30, last loss: 0.0216\n",
      "Validation accuracy: 0.4892\n",
      "Validation batch 31, last loss: 0.0214\n",
      "Validation accuracy: 0.4887\n",
      "Validation batch 32, last loss: 0.0219\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 33, last loss: 0.0190\n",
      "Validation accuracy: 0.4909\n",
      "Validation batch 34, last loss: 0.0200\n",
      "Validation accuracy: 0.4919\n",
      "Validation batch 35, last loss: 0.0193\n",
      "Validation accuracy: 0.4936\n",
      "Validation batch 36, last loss: 0.0198\n",
      "Validation accuracy: 0.4951\n",
      "Validation batch 37, last loss: 0.0200\n",
      "Validation accuracy: 0.4959\n",
      "Validation batch 38, last loss: 0.0247\n",
      "Validation accuracy: 0.4921\n",
      "Validation batch 39, last loss: 0.0247\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 40, last loss: 0.0233\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 17 completed, last loss: 0.0233, accuracy: 0.4871\n",
      "\n",
      "Epoch 18/30\n",
      "Training batch 1, last loss: 0.0200\n",
      "Training batch 2, last loss: 0.0236\n",
      "Training batch 3, last loss: 0.0251\n",
      "Training batch 4, last loss: 0.0246\n",
      "Training batch 5, last loss: 0.0217\n",
      "Training batch 6, last loss: 0.0227\n",
      "Training batch 7, last loss: 0.0222\n",
      "Training batch 8, last loss: 0.0175\n",
      "Training batch 9, last loss: 0.0215\n",
      "Training batch 10, last loss: 0.0203\n",
      "Training batch 11, last loss: 0.0214\n",
      "Training batch 12, last loss: 0.0223\n",
      "Training batch 13, last loss: 0.0232\n",
      "Training batch 14, last loss: 0.0224\n",
      "Training batch 15, last loss: 0.0244\n",
      "Training batch 16, last loss: 0.0191\n",
      "Training batch 17, last loss: 0.0221\n",
      "Training batch 18, last loss: 0.0222\n",
      "Training batch 19, last loss: 0.0174\n",
      "Training batch 20, last loss: 0.0217\n",
      "Training batch 21, last loss: 0.0203\n",
      "Training batch 22, last loss: 0.0206\n",
      "Training batch 23, last loss: 0.0182\n",
      "Training batch 24, last loss: 0.0195\n",
      "Training batch 25, last loss: 0.0178\n",
      "Training batch 26, last loss: 0.0244\n",
      "Training batch 27, last loss: 0.0198\n",
      "Training batch 28, last loss: 0.0219\n",
      "Training batch 29, last loss: 0.0225\n",
      "Training batch 30, last loss: 0.0185\n",
      "Training batch 31, last loss: 0.0187\n",
      "Training batch 32, last loss: 0.0208\n",
      "Training batch 33, last loss: 0.0218\n",
      "Training batch 34, last loss: 0.0232\n",
      "Training batch 35, last loss: 0.0178\n",
      "Training batch 36, last loss: 0.0224\n",
      "Training batch 37, last loss: 0.0208\n",
      "Training batch 38, last loss: 0.0251\n",
      "Training batch 39, last loss: 0.0227\n",
      "Training batch 40, last loss: 0.0212\n",
      "Training batch 41, last loss: 0.0224\n",
      "Training batch 42, last loss: 0.0219\n",
      "Training batch 43, last loss: 0.0187\n",
      "Training batch 44, last loss: 0.0187\n",
      "Training batch 45, last loss: 0.0219\n",
      "Training batch 46, last loss: 0.0219\n",
      "Training batch 47, last loss: 0.0227\n",
      "Training batch 48, last loss: 0.0191\n",
      "Training batch 49, last loss: 0.0238\n",
      "Training batch 50, last loss: 0.0239\n",
      "Training batch 51, last loss: 0.0208\n",
      "Training batch 52, last loss: 0.0234\n",
      "Training batch 53, last loss: 0.0209\n",
      "Training batch 54, last loss: 0.0236\n",
      "Training batch 55, last loss: 0.0188\n",
      "Training batch 56, last loss: 0.0229\n",
      "Training batch 57, last loss: 0.0204\n",
      "Training batch 58, last loss: 0.0212\n",
      "Training batch 59, last loss: 0.0214\n",
      "Training batch 60, last loss: 0.0230\n",
      "Training batch 61, last loss: 0.0197\n",
      "Training batch 62, last loss: 0.0229\n",
      "Training batch 63, last loss: 0.0186\n",
      "Training batch 64, last loss: 0.0216\n",
      "Training batch 65, last loss: 0.0225\n",
      "Training batch 66, last loss: 0.0186\n",
      "Training batch 67, last loss: 0.0227\n",
      "Training batch 68, last loss: 0.0190\n",
      "Training batch 69, last loss: 0.0173\n",
      "Training batch 70, last loss: 0.0199\n",
      "Training batch 71, last loss: 0.0224\n",
      "Training batch 72, last loss: 0.0239\n",
      "Training batch 73, last loss: 0.0240\n",
      "Training batch 74, last loss: 0.0239\n",
      "Training batch 75, last loss: 0.0191\n",
      "Training batch 76, last loss: 0.0228\n",
      "Training batch 77, last loss: 0.0241\n",
      "Training batch 78, last loss: 0.0217\n",
      "Training batch 79, last loss: 0.0175\n",
      "Training batch 80, last loss: 0.0212\n",
      "Training batch 81, last loss: 0.0226\n",
      "Training batch 82, last loss: 0.0199\n",
      "Training batch 83, last loss: 0.0241\n",
      "Training batch 84, last loss: 0.0217\n",
      "Training batch 85, last loss: 0.0173\n",
      "Training batch 86, last loss: 0.0261\n",
      "Training batch 87, last loss: 0.0210\n",
      "Training batch 88, last loss: 0.0216\n",
      "Training batch 89, last loss: 0.0187\n",
      "Training batch 90, last loss: 0.0196\n",
      "Training batch 91, last loss: 0.0211\n",
      "Training batch 92, last loss: 0.0191\n",
      "Training batch 93, last loss: 0.0151\n",
      "\n",
      "Training epoch 18 completed, last loss: 0.0151\n",
      "\n",
      "Validation batch 1, last loss: 0.0193\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0261\n",
      "Validation accuracy: 0.4250\n",
      "Validation batch 3, last loss: 0.0221\n",
      "Validation accuracy: 0.4333\n",
      "Validation batch 4, last loss: 0.0209\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 5, last loss: 0.0209\n",
      "Validation accuracy: 0.4550\n",
      "Validation batch 6, last loss: 0.0165\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 7, last loss: 0.0220\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 8, last loss: 0.0194\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 9, last loss: 0.0223\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 10, last loss: 0.0209\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 11, last loss: 0.0216\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 12, last loss: 0.0227\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 13, last loss: 0.0215\n",
      "Validation accuracy: 0.4788\n",
      "Validation batch 14, last loss: 0.0187\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 15, last loss: 0.0238\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 16, last loss: 0.0196\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 17, last loss: 0.0237\n",
      "Validation accuracy: 0.4779\n",
      "Validation batch 18, last loss: 0.0206\n",
      "Validation accuracy: 0.4792\n",
      "Validation batch 19, last loss: 0.0192\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 20, last loss: 0.0245\n",
      "Validation accuracy: 0.4775\n",
      "Validation batch 21, last loss: 0.0214\n",
      "Validation accuracy: 0.4774\n",
      "Validation batch 22, last loss: 0.0211\n",
      "Validation accuracy: 0.4784\n",
      "Validation batch 23, last loss: 0.0212\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 24, last loss: 0.0215\n",
      "Validation accuracy: 0.4781\n",
      "Validation batch 25, last loss: 0.0235\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 26, last loss: 0.0244\n",
      "Validation accuracy: 0.4712\n",
      "Validation batch 27, last loss: 0.0197\n",
      "Validation accuracy: 0.4741\n",
      "Validation batch 28, last loss: 0.0208\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 29, last loss: 0.0207\n",
      "Validation accuracy: 0.4767\n",
      "Validation batch 30, last loss: 0.0198\n",
      "Validation accuracy: 0.4783\n",
      "Validation batch 31, last loss: 0.0208\n",
      "Validation accuracy: 0.4790\n",
      "Validation batch 32, last loss: 0.0200\n",
      "Validation accuracy: 0.4805\n",
      "Validation batch 33, last loss: 0.0187\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 34, last loss: 0.0188\n",
      "Validation accuracy: 0.4860\n",
      "Validation batch 35, last loss: 0.0168\n",
      "Validation accuracy: 0.4907\n",
      "Validation batch 36, last loss: 0.0230\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 37, last loss: 0.0251\n",
      "Validation accuracy: 0.4851\n",
      "Validation batch 38, last loss: 0.0210\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 39, last loss: 0.0207\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 40, last loss: 0.0188\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 18 completed, last loss: 0.0188, accuracy: 0.4871\n",
      "\n",
      "Epoch 19/30\n",
      "Training batch 1, last loss: 0.0233\n",
      "Training batch 2, last loss: 0.0216\n",
      "Training batch 3, last loss: 0.0194\n",
      "Training batch 4, last loss: 0.0186\n",
      "Training batch 5, last loss: 0.0199\n",
      "Training batch 6, last loss: 0.0204\n",
      "Training batch 7, last loss: 0.0201\n",
      "Training batch 8, last loss: 0.0196\n",
      "Training batch 9, last loss: 0.0204\n",
      "Training batch 10, last loss: 0.0203\n",
      "Training batch 11, last loss: 0.0216\n",
      "Training batch 12, last loss: 0.0174\n",
      "Training batch 13, last loss: 0.0200\n",
      "Training batch 14, last loss: 0.0205\n",
      "Training batch 15, last loss: 0.0231\n",
      "Training batch 16, last loss: 0.0205\n",
      "Training batch 17, last loss: 0.0238\n",
      "Training batch 18, last loss: 0.0188\n",
      "Training batch 19, last loss: 0.0252\n",
      "Training batch 20, last loss: 0.0157\n",
      "Training batch 21, last loss: 0.0221\n",
      "Training batch 22, last loss: 0.0216\n",
      "Training batch 23, last loss: 0.0191\n",
      "Training batch 24, last loss: 0.0191\n",
      "Training batch 25, last loss: 0.0188\n",
      "Training batch 26, last loss: 0.0215\n",
      "Training batch 27, last loss: 0.0192\n",
      "Training batch 28, last loss: 0.0218\n",
      "Training batch 29, last loss: 0.0202\n",
      "Training batch 30, last loss: 0.0252\n",
      "Training batch 31, last loss: 0.0216\n",
      "Training batch 32, last loss: 0.0215\n",
      "Training batch 33, last loss: 0.0203\n",
      "Training batch 34, last loss: 0.0220\n",
      "Training batch 35, last loss: 0.0204\n",
      "Training batch 36, last loss: 0.0229\n",
      "Training batch 37, last loss: 0.0205\n",
      "Training batch 38, last loss: 0.0191\n",
      "Training batch 39, last loss: 0.0187\n",
      "Training batch 40, last loss: 0.0232\n",
      "Training batch 41, last loss: 0.0228\n",
      "Training batch 42, last loss: 0.0252\n",
      "Training batch 43, last loss: 0.0217\n",
      "Training batch 44, last loss: 0.0204\n",
      "Training batch 45, last loss: 0.0247\n",
      "Training batch 46, last loss: 0.0241\n",
      "Training batch 47, last loss: 0.0199\n",
      "Training batch 48, last loss: 0.0224\n",
      "Training batch 49, last loss: 0.0181\n",
      "Training batch 50, last loss: 0.0220\n",
      "Training batch 51, last loss: 0.0239\n",
      "Training batch 52, last loss: 0.0239\n",
      "Training batch 53, last loss: 0.0179\n",
      "Training batch 54, last loss: 0.0243\n",
      "Training batch 55, last loss: 0.0241\n",
      "Training batch 56, last loss: 0.0221\n",
      "Training batch 57, last loss: 0.0240\n",
      "Training batch 58, last loss: 0.0221\n",
      "Training batch 59, last loss: 0.0209\n",
      "Training batch 60, last loss: 0.0239\n",
      "Training batch 61, last loss: 0.0231\n",
      "Training batch 62, last loss: 0.0205\n",
      "Training batch 63, last loss: 0.0162\n",
      "Training batch 64, last loss: 0.0196\n",
      "Training batch 65, last loss: 0.0217\n",
      "Training batch 66, last loss: 0.0181\n",
      "Training batch 67, last loss: 0.0206\n",
      "Training batch 68, last loss: 0.0208\n",
      "Training batch 69, last loss: 0.0208\n",
      "Training batch 70, last loss: 0.0222\n",
      "Training batch 71, last loss: 0.0246\n",
      "Training batch 72, last loss: 0.0247\n",
      "Training batch 73, last loss: 0.0214\n",
      "Training batch 74, last loss: 0.0207\n",
      "Training batch 75, last loss: 0.0176\n",
      "Training batch 76, last loss: 0.0214\n",
      "Training batch 77, last loss: 0.0197\n",
      "Training batch 78, last loss: 0.0202\n",
      "Training batch 79, last loss: 0.0206\n",
      "Training batch 80, last loss: 0.0252\n",
      "Training batch 81, last loss: 0.0197\n",
      "Training batch 82, last loss: 0.0205\n",
      "Training batch 83, last loss: 0.0194\n",
      "Training batch 84, last loss: 0.0187\n",
      "Training batch 85, last loss: 0.0264\n",
      "Training batch 86, last loss: 0.0229\n",
      "Training batch 87, last loss: 0.0252\n",
      "Training batch 88, last loss: 0.0244\n",
      "Training batch 89, last loss: 0.0225\n",
      "Training batch 90, last loss: 0.0190\n",
      "Training batch 91, last loss: 0.0244\n",
      "Training batch 92, last loss: 0.0259\n",
      "Training batch 93, last loss: 0.0242\n",
      "\n",
      "Training epoch 19 completed, last loss: 0.0242\n",
      "\n",
      "Validation batch 1, last loss: 0.0191\n",
      "Validation accuracy: 0.5750\n",
      "Validation batch 2, last loss: 0.0226\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 3, last loss: 0.0218\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 4, last loss: 0.0255\n",
      "Validation accuracy: 0.4437\n",
      "Validation batch 5, last loss: 0.0197\n",
      "Validation accuracy: 0.4600\n",
      "Validation batch 6, last loss: 0.0244\n",
      "Validation accuracy: 0.4417\n",
      "Validation batch 7, last loss: 0.0237\n",
      "Validation accuracy: 0.4357\n",
      "Validation batch 8, last loss: 0.0226\n",
      "Validation accuracy: 0.4344\n",
      "Validation batch 9, last loss: 0.0207\n",
      "Validation accuracy: 0.4417\n",
      "Validation batch 10, last loss: 0.0227\n",
      "Validation accuracy: 0.4400\n",
      "Validation batch 11, last loss: 0.0244\n",
      "Validation accuracy: 0.4341\n",
      "Validation batch 12, last loss: 0.0223\n",
      "Validation accuracy: 0.4333\n",
      "Validation batch 13, last loss: 0.0245\n",
      "Validation accuracy: 0.4288\n",
      "Validation batch 14, last loss: 0.0181\n",
      "Validation accuracy: 0.4411\n",
      "Validation batch 15, last loss: 0.0202\n",
      "Validation accuracy: 0.4467\n",
      "Validation batch 16, last loss: 0.0191\n",
      "Validation accuracy: 0.4547\n",
      "Validation batch 17, last loss: 0.0215\n",
      "Validation accuracy: 0.4559\n",
      "Validation batch 18, last loss: 0.0213\n",
      "Validation accuracy: 0.4569\n",
      "Validation batch 19, last loss: 0.0223\n",
      "Validation accuracy: 0.4566\n",
      "Validation batch 20, last loss: 0.0174\n",
      "Validation accuracy: 0.4650\n",
      "Validation batch 21, last loss: 0.0183\n",
      "Validation accuracy: 0.4714\n",
      "Validation batch 22, last loss: 0.0213\n",
      "Validation accuracy: 0.4716\n",
      "Validation batch 23, last loss: 0.0230\n",
      "Validation accuracy: 0.4696\n",
      "Validation batch 24, last loss: 0.0206\n",
      "Validation accuracy: 0.4708\n",
      "Validation batch 25, last loss: 0.0188\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 26, last loss: 0.0200\n",
      "Validation accuracy: 0.4769\n",
      "Validation batch 27, last loss: 0.0163\n",
      "Validation accuracy: 0.4843\n",
      "Validation batch 28, last loss: 0.0222\n",
      "Validation accuracy: 0.4830\n",
      "Validation batch 29, last loss: 0.0184\n",
      "Validation accuracy: 0.4862\n",
      "Validation batch 30, last loss: 0.0206\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 31, last loss: 0.0194\n",
      "Validation accuracy: 0.4887\n",
      "Validation batch 32, last loss: 0.0208\n",
      "Validation accuracy: 0.4891\n",
      "Validation batch 33, last loss: 0.0264\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 34, last loss: 0.0213\n",
      "Validation accuracy: 0.4831\n",
      "Validation batch 35, last loss: 0.0221\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 36, last loss: 0.0161\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 37, last loss: 0.0187\n",
      "Validation accuracy: 0.4899\n",
      "Validation batch 38, last loss: 0.0225\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 39, last loss: 0.0217\n",
      "Validation accuracy: 0.4885\n",
      "Validation batch 40, last loss: 0.0233\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 19 completed, last loss: 0.0233, accuracy: 0.4871\n",
      "\n",
      "Epoch 20/30\n",
      "Training batch 1, last loss: 0.0219\n",
      "Training batch 2, last loss: 0.0227\n",
      "Training batch 3, last loss: 0.0235\n",
      "Training batch 4, last loss: 0.0195\n",
      "Training batch 5, last loss: 0.0211\n",
      "Training batch 6, last loss: 0.0220\n",
      "Training batch 7, last loss: 0.0179\n",
      "Training batch 8, last loss: 0.0237\n",
      "Training batch 9, last loss: 0.0221\n",
      "Training batch 10, last loss: 0.0193\n",
      "Training batch 11, last loss: 0.0231\n",
      "Training batch 12, last loss: 0.0179\n",
      "Training batch 13, last loss: 0.0253\n",
      "Training batch 14, last loss: 0.0214\n",
      "Training batch 15, last loss: 0.0241\n",
      "Training batch 16, last loss: 0.0210\n",
      "Training batch 17, last loss: 0.0249\n",
      "Training batch 18, last loss: 0.0215\n",
      "Training batch 19, last loss: 0.0199\n",
      "Training batch 20, last loss: 0.0225\n",
      "Training batch 21, last loss: 0.0199\n",
      "Training batch 22, last loss: 0.0204\n",
      "Training batch 23, last loss: 0.0190\n",
      "Training batch 24, last loss: 0.0218\n",
      "Training batch 25, last loss: 0.0199\n",
      "Training batch 26, last loss: 0.0252\n",
      "Training batch 27, last loss: 0.0215\n",
      "Training batch 28, last loss: 0.0225\n",
      "Training batch 29, last loss: 0.0200\n",
      "Training batch 30, last loss: 0.0234\n",
      "Training batch 31, last loss: 0.0177\n",
      "Training batch 32, last loss: 0.0181\n",
      "Training batch 33, last loss: 0.0222\n",
      "Training batch 34, last loss: 0.0249\n",
      "Training batch 35, last loss: 0.0223\n",
      "Training batch 36, last loss: 0.0230\n",
      "Training batch 37, last loss: 0.0201\n",
      "Training batch 38, last loss: 0.0213\n",
      "Training batch 39, last loss: 0.0197\n",
      "Training batch 40, last loss: 0.0243\n",
      "Training batch 41, last loss: 0.0248\n",
      "Training batch 42, last loss: 0.0192\n",
      "Training batch 43, last loss: 0.0206\n",
      "Training batch 44, last loss: 0.0217\n",
      "Training batch 45, last loss: 0.0227\n",
      "Training batch 46, last loss: 0.0209\n",
      "Training batch 47, last loss: 0.0235\n",
      "Training batch 48, last loss: 0.0207\n",
      "Training batch 49, last loss: 0.0222\n",
      "Training batch 50, last loss: 0.0222\n",
      "Training batch 51, last loss: 0.0251\n",
      "Training batch 52, last loss: 0.0232\n",
      "Training batch 53, last loss: 0.0211\n",
      "Training batch 54, last loss: 0.0200\n",
      "Training batch 55, last loss: 0.0188\n",
      "Training batch 56, last loss: 0.0189\n",
      "Training batch 57, last loss: 0.0191\n",
      "Training batch 58, last loss: 0.0206\n",
      "Training batch 59, last loss: 0.0206\n",
      "Training batch 60, last loss: 0.0197\n",
      "Training batch 61, last loss: 0.0202\n",
      "Training batch 62, last loss: 0.0215\n",
      "Training batch 63, last loss: 0.0207\n",
      "Training batch 64, last loss: 0.0232\n",
      "Training batch 65, last loss: 0.0197\n",
      "Training batch 66, last loss: 0.0207\n",
      "Training batch 67, last loss: 0.0215\n",
      "Training batch 68, last loss: 0.0213\n",
      "Training batch 69, last loss: 0.0202\n",
      "Training batch 70, last loss: 0.0193\n",
      "Training batch 71, last loss: 0.0176\n",
      "Training batch 72, last loss: 0.0241\n",
      "Training batch 73, last loss: 0.0246\n",
      "Training batch 74, last loss: 0.0192\n",
      "Training batch 75, last loss: 0.0209\n",
      "Training batch 76, last loss: 0.0188\n",
      "Training batch 77, last loss: 0.0191\n",
      "Training batch 78, last loss: 0.0226\n",
      "Training batch 79, last loss: 0.0218\n",
      "Training batch 80, last loss: 0.0232\n",
      "Training batch 81, last loss: 0.0248\n",
      "Training batch 82, last loss: 0.0184\n",
      "Training batch 83, last loss: 0.0201\n",
      "Training batch 84, last loss: 0.0225\n",
      "Training batch 85, last loss: 0.0219\n",
      "Training batch 86, last loss: 0.0159\n",
      "Training batch 87, last loss: 0.0231\n",
      "Training batch 88, last loss: 0.0176\n",
      "Training batch 89, last loss: 0.0204\n",
      "Training batch 90, last loss: 0.0227\n",
      "Training batch 91, last loss: 0.0209\n",
      "Training batch 92, last loss: 0.0255\n",
      "Training batch 93, last loss: 0.0171\n",
      "\n",
      "Training epoch 20 completed, last loss: 0.0171\n",
      "\n",
      "Validation batch 1, last loss: 0.0247\n",
      "Validation accuracy: 0.3500\n",
      "Validation batch 2, last loss: 0.0225\n",
      "Validation accuracy: 0.3875\n",
      "Validation batch 3, last loss: 0.0223\n",
      "Validation accuracy: 0.4083\n",
      "Validation batch 4, last loss: 0.0199\n",
      "Validation accuracy: 0.4375\n",
      "Validation batch 5, last loss: 0.0226\n",
      "Validation accuracy: 0.4350\n",
      "Validation batch 6, last loss: 0.0229\n",
      "Validation accuracy: 0.4333\n",
      "Validation batch 7, last loss: 0.0240\n",
      "Validation accuracy: 0.4250\n",
      "Validation batch 8, last loss: 0.0224\n",
      "Validation accuracy: 0.4281\n",
      "Validation batch 9, last loss: 0.0203\n",
      "Validation accuracy: 0.4389\n",
      "Validation batch 10, last loss: 0.0186\n",
      "Validation accuracy: 0.4525\n",
      "Validation batch 11, last loss: 0.0218\n",
      "Validation accuracy: 0.4545\n",
      "Validation batch 12, last loss: 0.0218\n",
      "Validation accuracy: 0.4562\n",
      "Validation batch 13, last loss: 0.0163\n",
      "Validation accuracy: 0.4731\n",
      "Validation batch 14, last loss: 0.0232\n",
      "Validation accuracy: 0.4679\n",
      "Validation batch 15, last loss: 0.0194\n",
      "Validation accuracy: 0.4733\n",
      "Validation batch 16, last loss: 0.0201\n",
      "Validation accuracy: 0.4766\n",
      "Validation batch 17, last loss: 0.0215\n",
      "Validation accuracy: 0.4765\n",
      "Validation batch 18, last loss: 0.0214\n",
      "Validation accuracy: 0.4764\n",
      "Validation batch 19, last loss: 0.0172\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 20, last loss: 0.0170\n",
      "Validation accuracy: 0.4913\n",
      "Validation batch 21, last loss: 0.0228\n",
      "Validation accuracy: 0.4881\n",
      "Validation batch 22, last loss: 0.0195\n",
      "Validation accuracy: 0.4909\n",
      "Validation batch 23, last loss: 0.0255\n",
      "Validation accuracy: 0.4848\n",
      "Validation batch 24, last loss: 0.0212\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 25, last loss: 0.0230\n",
      "Validation accuracy: 0.4820\n",
      "Validation batch 26, last loss: 0.0205\n",
      "Validation accuracy: 0.4827\n",
      "Validation batch 27, last loss: 0.0170\n",
      "Validation accuracy: 0.4880\n",
      "Validation batch 28, last loss: 0.0207\n",
      "Validation accuracy: 0.4884\n",
      "Validation batch 29, last loss: 0.0222\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 30, last loss: 0.0222\n",
      "Validation accuracy: 0.4858\n",
      "Validation batch 31, last loss: 0.0215\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 32, last loss: 0.0199\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 33, last loss: 0.0230\n",
      "Validation accuracy: 0.4856\n",
      "Validation batch 34, last loss: 0.0198\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 35, last loss: 0.0187\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 36, last loss: 0.0239\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 37, last loss: 0.0211\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 38, last loss: 0.0191\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 39, last loss: 0.0219\n",
      "Validation accuracy: 0.4878\n",
      "Validation batch 40, last loss: 0.0216\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 20 completed, last loss: 0.0216, accuracy: 0.4871\n",
      "\n",
      "Epoch 21/30\n",
      "Training batch 1, last loss: 0.0202\n",
      "Training batch 2, last loss: 0.0213\n",
      "Training batch 3, last loss: 0.0179\n",
      "Training batch 4, last loss: 0.0228\n",
      "Training batch 5, last loss: 0.0232\n",
      "Training batch 6, last loss: 0.0205\n",
      "Training batch 7, last loss: 0.0215\n",
      "Training batch 8, last loss: 0.0205\n",
      "Training batch 9, last loss: 0.0205\n",
      "Training batch 10, last loss: 0.0198\n",
      "Training batch 11, last loss: 0.0223\n",
      "Training batch 12, last loss: 0.0200\n",
      "Training batch 13, last loss: 0.0235\n",
      "Training batch 14, last loss: 0.0283\n",
      "Training batch 15, last loss: 0.0186\n",
      "Training batch 16, last loss: 0.0214\n",
      "Training batch 17, last loss: 0.0221\n",
      "Training batch 18, last loss: 0.0190\n",
      "Training batch 19, last loss: 0.0217\n",
      "Training batch 20, last loss: 0.0177\n",
      "Training batch 21, last loss: 0.0236\n",
      "Training batch 22, last loss: 0.0261\n",
      "Training batch 23, last loss: 0.0230\n",
      "Training batch 24, last loss: 0.0201\n",
      "Training batch 25, last loss: 0.0285\n",
      "Training batch 26, last loss: 0.0202\n",
      "Training batch 27, last loss: 0.0197\n",
      "Training batch 28, last loss: 0.0213\n",
      "Training batch 29, last loss: 0.0210\n",
      "Training batch 30, last loss: 0.0193\n",
      "Training batch 31, last loss: 0.0212\n",
      "Training batch 32, last loss: 0.0234\n",
      "Training batch 33, last loss: 0.0197\n",
      "Training batch 34, last loss: 0.0244\n",
      "Training batch 35, last loss: 0.0213\n",
      "Training batch 36, last loss: 0.0192\n",
      "Training batch 37, last loss: 0.0262\n",
      "Training batch 38, last loss: 0.0227\n",
      "Training batch 39, last loss: 0.0246\n",
      "Training batch 40, last loss: 0.0200\n",
      "Training batch 41, last loss: 0.0269\n",
      "Training batch 42, last loss: 0.0217\n",
      "Training batch 43, last loss: 0.0195\n",
      "Training batch 44, last loss: 0.0232\n",
      "Training batch 45, last loss: 0.0219\n",
      "Training batch 46, last loss: 0.0213\n",
      "Training batch 47, last loss: 0.0201\n",
      "Training batch 48, last loss: 0.0196\n",
      "Training batch 49, last loss: 0.0155\n",
      "Training batch 50, last loss: 0.0225\n",
      "Training batch 51, last loss: 0.0205\n",
      "Training batch 52, last loss: 0.0186\n",
      "Training batch 53, last loss: 0.0209\n",
      "Training batch 54, last loss: 0.0230\n",
      "Training batch 55, last loss: 0.0177\n",
      "Training batch 56, last loss: 0.0222\n",
      "Training batch 57, last loss: 0.0201\n",
      "Training batch 58, last loss: 0.0222\n",
      "Training batch 59, last loss: 0.0199\n",
      "Training batch 60, last loss: 0.0232\n",
      "Training batch 61, last loss: 0.0200\n",
      "Training batch 62, last loss: 0.0209\n",
      "Training batch 63, last loss: 0.0236\n",
      "Training batch 64, last loss: 0.0189\n",
      "Training batch 65, last loss: 0.0196\n",
      "Training batch 66, last loss: 0.0212\n",
      "Training batch 67, last loss: 0.0191\n",
      "Training batch 68, last loss: 0.0222\n",
      "Training batch 69, last loss: 0.0201\n",
      "Training batch 70, last loss: 0.0228\n",
      "Training batch 71, last loss: 0.0231\n",
      "Training batch 72, last loss: 0.0230\n",
      "Training batch 73, last loss: 0.0208\n",
      "Training batch 74, last loss: 0.0203\n",
      "Training batch 75, last loss: 0.0227\n",
      "Training batch 76, last loss: 0.0228\n",
      "Training batch 77, last loss: 0.0177\n",
      "Training batch 78, last loss: 0.0219\n",
      "Training batch 79, last loss: 0.0182\n",
      "Training batch 80, last loss: 0.0205\n",
      "Training batch 81, last loss: 0.0192\n",
      "Training batch 82, last loss: 0.0207\n",
      "Training batch 83, last loss: 0.0195\n",
      "Training batch 84, last loss: 0.0216\n",
      "Training batch 85, last loss: 0.0205\n",
      "Training batch 86, last loss: 0.0213\n",
      "Training batch 87, last loss: 0.0265\n",
      "Training batch 88, last loss: 0.0240\n",
      "Training batch 89, last loss: 0.0190\n",
      "Training batch 90, last loss: 0.0209\n",
      "Training batch 91, last loss: 0.0206\n",
      "Training batch 92, last loss: 0.0223\n",
      "Training batch 93, last loss: 0.0239\n",
      "\n",
      "Training epoch 21 completed, last loss: 0.0239\n",
      "\n",
      "Validation batch 1, last loss: 0.0228\n",
      "Validation accuracy: 0.4250\n",
      "Validation batch 2, last loss: 0.0194\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 3, last loss: 0.0193\n",
      "Validation accuracy: 0.5083\n",
      "Validation batch 4, last loss: 0.0217\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 5, last loss: 0.0180\n",
      "Validation accuracy: 0.5150\n",
      "Validation batch 6, last loss: 0.0205\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 7, last loss: 0.0209\n",
      "Validation accuracy: 0.5107\n",
      "Validation batch 8, last loss: 0.0192\n",
      "Validation accuracy: 0.5156\n",
      "Validation batch 9, last loss: 0.0199\n",
      "Validation accuracy: 0.5167\n",
      "Validation batch 10, last loss: 0.0185\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 11, last loss: 0.0226\n",
      "Validation accuracy: 0.5182\n",
      "Validation batch 12, last loss: 0.0221\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 13, last loss: 0.0233\n",
      "Validation accuracy: 0.5038\n",
      "Validation batch 14, last loss: 0.0169\n",
      "Validation accuracy: 0.5143\n",
      "Validation batch 15, last loss: 0.0195\n",
      "Validation accuracy: 0.5167\n",
      "Validation batch 16, last loss: 0.0190\n",
      "Validation accuracy: 0.5203\n",
      "Validation batch 17, last loss: 0.0200\n",
      "Validation accuracy: 0.5206\n",
      "Validation batch 18, last loss: 0.0197\n",
      "Validation accuracy: 0.5222\n",
      "Validation batch 19, last loss: 0.0201\n",
      "Validation accuracy: 0.5224\n",
      "Validation batch 20, last loss: 0.0199\n",
      "Validation accuracy: 0.5238\n",
      "Validation batch 21, last loss: 0.0249\n",
      "Validation accuracy: 0.5155\n",
      "Validation batch 22, last loss: 0.0196\n",
      "Validation accuracy: 0.5170\n",
      "Validation batch 23, last loss: 0.0248\n",
      "Validation accuracy: 0.5109\n",
      "Validation batch 24, last loss: 0.0200\n",
      "Validation accuracy: 0.5115\n",
      "Validation batch 25, last loss: 0.0219\n",
      "Validation accuracy: 0.5090\n",
      "Validation batch 26, last loss: 0.0251\n",
      "Validation accuracy: 0.5029\n",
      "Validation batch 27, last loss: 0.0216\n",
      "Validation accuracy: 0.5019\n",
      "Validation batch 28, last loss: 0.0218\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 29, last loss: 0.0191\n",
      "Validation accuracy: 0.5017\n",
      "Validation batch 30, last loss: 0.0199\n",
      "Validation accuracy: 0.5025\n",
      "Validation batch 31, last loss: 0.0214\n",
      "Validation accuracy: 0.5016\n",
      "Validation batch 32, last loss: 0.0251\n",
      "Validation accuracy: 0.4961\n",
      "Validation batch 33, last loss: 0.0228\n",
      "Validation accuracy: 0.4939\n",
      "Validation batch 34, last loss: 0.0201\n",
      "Validation accuracy: 0.4949\n",
      "Validation batch 35, last loss: 0.0217\n",
      "Validation accuracy: 0.4936\n",
      "Validation batch 36, last loss: 0.0222\n",
      "Validation accuracy: 0.4924\n",
      "Validation batch 37, last loss: 0.0234\n",
      "Validation accuracy: 0.4899\n",
      "Validation batch 38, last loss: 0.0190\n",
      "Validation accuracy: 0.4914\n",
      "Validation batch 39, last loss: 0.0257\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0216\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 21 completed, last loss: 0.0216, accuracy: 0.4871\n",
      "\n",
      "Epoch 22/30\n",
      "Training batch 1, last loss: 0.0211\n",
      "Training batch 2, last loss: 0.0184\n",
      "Training batch 3, last loss: 0.0201\n",
      "Training batch 4, last loss: 0.0208\n",
      "Training batch 5, last loss: 0.0182\n",
      "Training batch 6, last loss: 0.0269\n",
      "Training batch 7, last loss: 0.0220\n",
      "Training batch 8, last loss: 0.0199\n",
      "Training batch 9, last loss: 0.0217\n",
      "Training batch 10, last loss: 0.0216\n",
      "Training batch 11, last loss: 0.0169\n",
      "Training batch 12, last loss: 0.0216\n",
      "Training batch 13, last loss: 0.0155\n",
      "Training batch 14, last loss: 0.0250\n",
      "Training batch 15, last loss: 0.0255\n",
      "Training batch 16, last loss: 0.0247\n",
      "Training batch 17, last loss: 0.0231\n",
      "Training batch 18, last loss: 0.0196\n",
      "Training batch 19, last loss: 0.0213\n",
      "Training batch 20, last loss: 0.0265\n",
      "Training batch 21, last loss: 0.0211\n",
      "Training batch 22, last loss: 0.0250\n",
      "Training batch 23, last loss: 0.0228\n",
      "Training batch 24, last loss: 0.0234\n",
      "Training batch 25, last loss: 0.0241\n",
      "Training batch 26, last loss: 0.0243\n",
      "Training batch 27, last loss: 0.0192\n",
      "Training batch 28, last loss: 0.0198\n",
      "Training batch 29, last loss: 0.0203\n",
      "Training batch 30, last loss: 0.0206\n",
      "Training batch 31, last loss: 0.0212\n",
      "Training batch 32, last loss: 0.0200\n",
      "Training batch 33, last loss: 0.0211\n",
      "Training batch 34, last loss: 0.0210\n",
      "Training batch 35, last loss: 0.0240\n",
      "Training batch 36, last loss: 0.0239\n",
      "Training batch 37, last loss: 0.0244\n",
      "Training batch 38, last loss: 0.0249\n",
      "Training batch 39, last loss: 0.0192\n",
      "Training batch 40, last loss: 0.0208\n",
      "Training batch 41, last loss: 0.0205\n",
      "Training batch 42, last loss: 0.0211\n",
      "Training batch 43, last loss: 0.0195\n",
      "Training batch 44, last loss: 0.0188\n",
      "Training batch 45, last loss: 0.0258\n",
      "Training batch 46, last loss: 0.0175\n",
      "Training batch 47, last loss: 0.0194\n",
      "Training batch 48, last loss: 0.0188\n",
      "Training batch 49, last loss: 0.0235\n",
      "Training batch 50, last loss: 0.0231\n",
      "Training batch 51, last loss: 0.0227\n",
      "Training batch 52, last loss: 0.0222\n",
      "Training batch 53, last loss: 0.0239\n",
      "Training batch 54, last loss: 0.0225\n",
      "Training batch 55, last loss: 0.0225\n",
      "Training batch 56, last loss: 0.0169\n",
      "Training batch 57, last loss: 0.0227\n",
      "Training batch 58, last loss: 0.0200\n",
      "Training batch 59, last loss: 0.0190\n",
      "Training batch 60, last loss: 0.0207\n",
      "Training batch 61, last loss: 0.0197\n",
      "Training batch 62, last loss: 0.0242\n",
      "Training batch 63, last loss: 0.0229\n",
      "Training batch 64, last loss: 0.0190\n",
      "Training batch 65, last loss: 0.0185\n",
      "Training batch 66, last loss: 0.0213\n",
      "Training batch 67, last loss: 0.0194\n",
      "Training batch 68, last loss: 0.0167\n",
      "Training batch 69, last loss: 0.0214\n",
      "Training batch 70, last loss: 0.0248\n",
      "Training batch 71, last loss: 0.0242\n",
      "Training batch 72, last loss: 0.0167\n",
      "Training batch 73, last loss: 0.0214\n",
      "Training batch 74, last loss: 0.0209\n",
      "Training batch 75, last loss: 0.0200\n",
      "Training batch 76, last loss: 0.0198\n",
      "Training batch 77, last loss: 0.0227\n",
      "Training batch 78, last loss: 0.0182\n",
      "Training batch 79, last loss: 0.0198\n",
      "Training batch 80, last loss: 0.0218\n",
      "Training batch 81, last loss: 0.0232\n",
      "Training batch 82, last loss: 0.0216\n",
      "Training batch 83, last loss: 0.0229\n",
      "Training batch 84, last loss: 0.0236\n",
      "Training batch 85, last loss: 0.0221\n",
      "Training batch 86, last loss: 0.0202\n",
      "Training batch 87, last loss: 0.0220\n",
      "Training batch 88, last loss: 0.0198\n",
      "Training batch 89, last loss: 0.0184\n",
      "Training batch 90, last loss: 0.0227\n",
      "Training batch 91, last loss: 0.0182\n",
      "Training batch 92, last loss: 0.0211\n",
      "Training batch 93, last loss: 0.0224\n",
      "\n",
      "Training epoch 22 completed, last loss: 0.0224\n",
      "\n",
      "Validation batch 1, last loss: 0.0194\n",
      "Validation accuracy: 0.5500\n",
      "Validation batch 2, last loss: 0.0161\n",
      "Validation accuracy: 0.6125\n",
      "Validation batch 3, last loss: 0.0193\n",
      "Validation accuracy: 0.6000\n",
      "Validation batch 4, last loss: 0.0246\n",
      "Validation accuracy: 0.5375\n",
      "Validation batch 5, last loss: 0.0227\n",
      "Validation accuracy: 0.5200\n",
      "Validation batch 6, last loss: 0.0250\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 7, last loss: 0.0223\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 8, last loss: 0.0213\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0174\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 10, last loss: 0.0167\n",
      "Validation accuracy: 0.5150\n",
      "Validation batch 11, last loss: 0.0240\n",
      "Validation accuracy: 0.5023\n",
      "Validation batch 12, last loss: 0.0193\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 13, last loss: 0.0202\n",
      "Validation accuracy: 0.5058\n",
      "Validation batch 14, last loss: 0.0240\n",
      "Validation accuracy: 0.4964\n",
      "Validation batch 15, last loss: 0.0216\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 16, last loss: 0.0228\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 17, last loss: 0.0170\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 18, last loss: 0.0205\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 19, last loss: 0.0208\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 20, last loss: 0.0236\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 21, last loss: 0.0207\n",
      "Validation accuracy: 0.4952\n",
      "Validation batch 22, last loss: 0.0204\n",
      "Validation accuracy: 0.4966\n",
      "Validation batch 23, last loss: 0.0209\n",
      "Validation accuracy: 0.4957\n",
      "Validation batch 24, last loss: 0.0198\n",
      "Validation accuracy: 0.4969\n",
      "Validation batch 25, last loss: 0.0200\n",
      "Validation accuracy: 0.4980\n",
      "Validation batch 26, last loss: 0.0175\n",
      "Validation accuracy: 0.5019\n",
      "Validation batch 27, last loss: 0.0198\n",
      "Validation accuracy: 0.5037\n",
      "Validation batch 28, last loss: 0.0230\n",
      "Validation accuracy: 0.5009\n",
      "Validation batch 29, last loss: 0.0205\n",
      "Validation accuracy: 0.5009\n",
      "Validation batch 30, last loss: 0.0241\n",
      "Validation accuracy: 0.4967\n",
      "Validation batch 31, last loss: 0.0239\n",
      "Validation accuracy: 0.4935\n",
      "Validation batch 32, last loss: 0.0226\n",
      "Validation accuracy: 0.4914\n",
      "Validation batch 33, last loss: 0.0230\n",
      "Validation accuracy: 0.4894\n",
      "Validation batch 34, last loss: 0.0214\n",
      "Validation accuracy: 0.4890\n",
      "Validation batch 35, last loss: 0.0213\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 36, last loss: 0.0189\n",
      "Validation accuracy: 0.4910\n",
      "Validation batch 37, last loss: 0.0248\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 38, last loss: 0.0199\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 39, last loss: 0.0227\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 40, last loss: 0.0212\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 22 completed, last loss: 0.0212, accuracy: 0.4871\n",
      "\n",
      "Epoch 23/30\n",
      "Training batch 1, last loss: 0.0241\n",
      "Training batch 2, last loss: 0.0221\n",
      "Training batch 3, last loss: 0.0241\n",
      "Training batch 4, last loss: 0.0240\n",
      "Training batch 5, last loss: 0.0208\n",
      "Training batch 6, last loss: 0.0193\n",
      "Training batch 7, last loss: 0.0173\n",
      "Training batch 8, last loss: 0.0239\n",
      "Training batch 9, last loss: 0.0247\n",
      "Training batch 10, last loss: 0.0196\n",
      "Training batch 11, last loss: 0.0184\n",
      "Training batch 12, last loss: 0.0250\n",
      "Training batch 13, last loss: 0.0213\n",
      "Training batch 14, last loss: 0.0218\n",
      "Training batch 15, last loss: 0.0216\n",
      "Training batch 16, last loss: 0.0203\n",
      "Training batch 17, last loss: 0.0219\n",
      "Training batch 18, last loss: 0.0260\n",
      "Training batch 19, last loss: 0.0207\n",
      "Training batch 20, last loss: 0.0215\n",
      "Training batch 21, last loss: 0.0225\n",
      "Training batch 22, last loss: 0.0219\n",
      "Training batch 23, last loss: 0.0171\n",
      "Training batch 24, last loss: 0.0181\n",
      "Training batch 25, last loss: 0.0191\n",
      "Training batch 26, last loss: 0.0217\n",
      "Training batch 27, last loss: 0.0231\n",
      "Training batch 28, last loss: 0.0206\n",
      "Training batch 29, last loss: 0.0208\n",
      "Training batch 30, last loss: 0.0209\n",
      "Training batch 31, last loss: 0.0236\n",
      "Training batch 32, last loss: 0.0199\n",
      "Training batch 33, last loss: 0.0201\n",
      "Training batch 34, last loss: 0.0207\n",
      "Training batch 35, last loss: 0.0233\n",
      "Training batch 36, last loss: 0.0229\n",
      "Training batch 37, last loss: 0.0211\n",
      "Training batch 38, last loss: 0.0236\n",
      "Training batch 39, last loss: 0.0203\n",
      "Training batch 40, last loss: 0.0218\n",
      "Training batch 41, last loss: 0.0253\n",
      "Training batch 42, last loss: 0.0204\n",
      "Training batch 43, last loss: 0.0230\n",
      "Training batch 44, last loss: 0.0209\n",
      "Training batch 45, last loss: 0.0178\n",
      "Training batch 46, last loss: 0.0219\n",
      "Training batch 47, last loss: 0.0252\n",
      "Training batch 48, last loss: 0.0225\n",
      "Training batch 49, last loss: 0.0188\n",
      "Training batch 50, last loss: 0.0217\n",
      "Training batch 51, last loss: 0.0162\n",
      "Training batch 52, last loss: 0.0191\n",
      "Training batch 53, last loss: 0.0202\n",
      "Training batch 54, last loss: 0.0239\n",
      "Training batch 55, last loss: 0.0237\n",
      "Training batch 56, last loss: 0.0207\n",
      "Training batch 57, last loss: 0.0201\n",
      "Training batch 58, last loss: 0.0193\n",
      "Training batch 59, last loss: 0.0208\n",
      "Training batch 60, last loss: 0.0194\n",
      "Training batch 61, last loss: 0.0205\n",
      "Training batch 62, last loss: 0.0209\n",
      "Training batch 63, last loss: 0.0222\n",
      "Training batch 64, last loss: 0.0213\n",
      "Training batch 65, last loss: 0.0237\n",
      "Training batch 66, last loss: 0.0232\n",
      "Training batch 67, last loss: 0.0205\n",
      "Training batch 68, last loss: 0.0226\n",
      "Training batch 69, last loss: 0.0205\n",
      "Training batch 70, last loss: 0.0228\n",
      "Training batch 71, last loss: 0.0187\n",
      "Training batch 72, last loss: 0.0181\n",
      "Training batch 73, last loss: 0.0206\n",
      "Training batch 74, last loss: 0.0246\n",
      "Training batch 75, last loss: 0.0240\n",
      "Training batch 76, last loss: 0.0171\n",
      "Training batch 77, last loss: 0.0191\n",
      "Training batch 78, last loss: 0.0202\n",
      "Training batch 79, last loss: 0.0234\n",
      "Training batch 80, last loss: 0.0189\n",
      "Training batch 81, last loss: 0.0210\n",
      "Training batch 82, last loss: 0.0173\n",
      "Training batch 83, last loss: 0.0270\n",
      "Training batch 84, last loss: 0.0219\n",
      "Training batch 85, last loss: 0.0203\n",
      "Training batch 86, last loss: 0.0197\n",
      "Training batch 87, last loss: 0.0180\n",
      "Training batch 88, last loss: 0.0252\n",
      "Training batch 89, last loss: 0.0201\n",
      "Training batch 90, last loss: 0.0208\n",
      "Training batch 91, last loss: 0.0196\n",
      "Training batch 92, last loss: 0.0254\n",
      "Training batch 93, last loss: 0.0237\n",
      "\n",
      "Training epoch 23 completed, last loss: 0.0237\n",
      "\n",
      "Validation batch 1, last loss: 0.0182\n",
      "Validation accuracy: 0.6000\n",
      "Validation batch 2, last loss: 0.0229\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 3, last loss: 0.0226\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 4, last loss: 0.0220\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 5, last loss: 0.0252\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 6, last loss: 0.0221\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 7, last loss: 0.0207\n",
      "Validation accuracy: 0.4571\n",
      "Validation batch 8, last loss: 0.0184\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 9, last loss: 0.0226\n",
      "Validation accuracy: 0.4694\n",
      "Validation batch 10, last loss: 0.0202\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 11, last loss: 0.0215\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 12, last loss: 0.0194\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 13, last loss: 0.0216\n",
      "Validation accuracy: 0.4808\n",
      "Validation batch 14, last loss: 0.0214\n",
      "Validation accuracy: 0.4804\n",
      "Validation batch 15, last loss: 0.0199\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 16, last loss: 0.0231\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 17, last loss: 0.0203\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 18, last loss: 0.0187\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 19, last loss: 0.0205\n",
      "Validation accuracy: 0.4895\n",
      "Validation batch 20, last loss: 0.0194\n",
      "Validation accuracy: 0.4925\n",
      "Validation batch 21, last loss: 0.0218\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 22, last loss: 0.0189\n",
      "Validation accuracy: 0.4943\n",
      "Validation batch 23, last loss: 0.0211\n",
      "Validation accuracy: 0.4935\n",
      "Validation batch 24, last loss: 0.0205\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 25, last loss: 0.0215\n",
      "Validation accuracy: 0.4930\n",
      "Validation batch 26, last loss: 0.0236\n",
      "Validation accuracy: 0.4894\n",
      "Validation batch 27, last loss: 0.0227\n",
      "Validation accuracy: 0.4870\n",
      "Validation batch 28, last loss: 0.0199\n",
      "Validation accuracy: 0.4884\n",
      "Validation batch 29, last loss: 0.0204\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 30, last loss: 0.0229\n",
      "Validation accuracy: 0.4858\n",
      "Validation batch 31, last loss: 0.0202\n",
      "Validation accuracy: 0.4871\n",
      "Validation batch 32, last loss: 0.0213\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 33, last loss: 0.0186\n",
      "Validation accuracy: 0.4894\n",
      "Validation batch 34, last loss: 0.0182\n",
      "Validation accuracy: 0.4926\n",
      "Validation batch 35, last loss: 0.0220\n",
      "Validation accuracy: 0.4921\n",
      "Validation batch 36, last loss: 0.0223\n",
      "Validation accuracy: 0.4910\n",
      "Validation batch 37, last loss: 0.0239\n",
      "Validation accuracy: 0.4878\n",
      "Validation batch 38, last loss: 0.0241\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 39, last loss: 0.0211\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 40, last loss: 0.0182\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 23 completed, last loss: 0.0182, accuracy: 0.4871\n",
      "\n",
      "Epoch 24/30\n",
      "Training batch 1, last loss: 0.0218\n",
      "Training batch 2, last loss: 0.0197\n",
      "Training batch 3, last loss: 0.0213\n",
      "Training batch 4, last loss: 0.0199\n",
      "Training batch 5, last loss: 0.0244\n",
      "Training batch 6, last loss: 0.0206\n",
      "Training batch 7, last loss: 0.0185\n",
      "Training batch 8, last loss: 0.0183\n",
      "Training batch 9, last loss: 0.0216\n",
      "Training batch 10, last loss: 0.0196\n",
      "Training batch 11, last loss: 0.0218\n",
      "Training batch 12, last loss: 0.0210\n",
      "Training batch 13, last loss: 0.0214\n",
      "Training batch 14, last loss: 0.0208\n",
      "Training batch 15, last loss: 0.0209\n",
      "Training batch 16, last loss: 0.0230\n",
      "Training batch 17, last loss: 0.0243\n",
      "Training batch 18, last loss: 0.0246\n",
      "Training batch 19, last loss: 0.0191\n",
      "Training batch 20, last loss: 0.0187\n",
      "Training batch 21, last loss: 0.0236\n",
      "Training batch 22, last loss: 0.0214\n",
      "Training batch 23, last loss: 0.0242\n",
      "Training batch 24, last loss: 0.0209\n",
      "Training batch 25, last loss: 0.0201\n",
      "Training batch 26, last loss: 0.0196\n",
      "Training batch 27, last loss: 0.0219\n",
      "Training batch 28, last loss: 0.0200\n",
      "Training batch 29, last loss: 0.0243\n",
      "Training batch 30, last loss: 0.0194\n",
      "Training batch 31, last loss: 0.0224\n",
      "Training batch 32, last loss: 0.0231\n",
      "Training batch 33, last loss: 0.0188\n",
      "Training batch 34, last loss: 0.0193\n",
      "Training batch 35, last loss: 0.0214\n",
      "Training batch 36, last loss: 0.0238\n",
      "Training batch 37, last loss: 0.0228\n",
      "Training batch 38, last loss: 0.0198\n",
      "Training batch 39, last loss: 0.0203\n",
      "Training batch 40, last loss: 0.0191\n",
      "Training batch 41, last loss: 0.0195\n",
      "Training batch 42, last loss: 0.0209\n",
      "Training batch 43, last loss: 0.0216\n",
      "Training batch 44, last loss: 0.0198\n",
      "Training batch 45, last loss: 0.0178\n",
      "Training batch 46, last loss: 0.0222\n",
      "Training batch 47, last loss: 0.0198\n",
      "Training batch 48, last loss: 0.0208\n",
      "Training batch 49, last loss: 0.0221\n",
      "Training batch 50, last loss: 0.0209\n",
      "Training batch 51, last loss: 0.0239\n",
      "Training batch 52, last loss: 0.0229\n",
      "Training batch 53, last loss: 0.0193\n",
      "Training batch 54, last loss: 0.0214\n",
      "Training batch 55, last loss: 0.0200\n",
      "Training batch 56, last loss: 0.0208\n",
      "Training batch 57, last loss: 0.0208\n",
      "Training batch 58, last loss: 0.0225\n",
      "Training batch 59, last loss: 0.0245\n",
      "Training batch 60, last loss: 0.0237\n",
      "Training batch 61, last loss: 0.0200\n",
      "Training batch 62, last loss: 0.0204\n",
      "Training batch 63, last loss: 0.0224\n",
      "Training batch 64, last loss: 0.0184\n",
      "Training batch 65, last loss: 0.0229\n",
      "Training batch 66, last loss: 0.0192\n",
      "Training batch 67, last loss: 0.0202\n",
      "Training batch 68, last loss: 0.0206\n",
      "Training batch 69, last loss: 0.0186\n",
      "Training batch 70, last loss: 0.0229\n",
      "Training batch 71, last loss: 0.0229\n",
      "Training batch 72, last loss: 0.0204\n",
      "Training batch 73, last loss: 0.0240\n",
      "Training batch 74, last loss: 0.0204\n",
      "Training batch 75, last loss: 0.0214\n",
      "Training batch 76, last loss: 0.0195\n",
      "Training batch 77, last loss: 0.0221\n",
      "Training batch 78, last loss: 0.0225\n",
      "Training batch 79, last loss: 0.0167\n",
      "Training batch 80, last loss: 0.0216\n",
      "Training batch 81, last loss: 0.0193\n",
      "Training batch 82, last loss: 0.0171\n",
      "Training batch 83, last loss: 0.0253\n",
      "Training batch 84, last loss: 0.0257\n",
      "Training batch 85, last loss: 0.0248\n",
      "Training batch 86, last loss: 0.0217\n",
      "Training batch 87, last loss: 0.0226\n",
      "Training batch 88, last loss: 0.0191\n",
      "Training batch 89, last loss: 0.0233\n",
      "Training batch 90, last loss: 0.0214\n",
      "Training batch 91, last loss: 0.0197\n",
      "Training batch 92, last loss: 0.0197\n",
      "Training batch 93, last loss: 0.0244\n",
      "\n",
      "Training epoch 24 completed, last loss: 0.0244\n",
      "\n",
      "Validation batch 1, last loss: 0.0168\n",
      "Validation accuracy: 0.6500\n",
      "Validation batch 2, last loss: 0.0228\n",
      "Validation accuracy: 0.5375\n",
      "Validation batch 3, last loss: 0.0209\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 4, last loss: 0.0192\n",
      "Validation accuracy: 0.5312\n",
      "Validation batch 5, last loss: 0.0240\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 6, last loss: 0.0216\n",
      "Validation accuracy: 0.4958\n",
      "Validation batch 7, last loss: 0.0220\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 8, last loss: 0.0225\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0191\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 10, last loss: 0.0207\n",
      "Validation accuracy: 0.4950\n",
      "Validation batch 11, last loss: 0.0209\n",
      "Validation accuracy: 0.4955\n",
      "Validation batch 12, last loss: 0.0194\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 13, last loss: 0.0232\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 14, last loss: 0.0186\n",
      "Validation accuracy: 0.4982\n",
      "Validation batch 15, last loss: 0.0196\n",
      "Validation accuracy: 0.5017\n",
      "Validation batch 16, last loss: 0.0243\n",
      "Validation accuracy: 0.4922\n",
      "Validation batch 17, last loss: 0.0238\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 18, last loss: 0.0202\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 19, last loss: 0.0209\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 20, last loss: 0.0203\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 21, last loss: 0.0211\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 22, last loss: 0.0174\n",
      "Validation accuracy: 0.4955\n",
      "Validation batch 23, last loss: 0.0205\n",
      "Validation accuracy: 0.4957\n",
      "Validation batch 24, last loss: 0.0244\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 25, last loss: 0.0202\n",
      "Validation accuracy: 0.4920\n",
      "Validation batch 26, last loss: 0.0245\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 27, last loss: 0.0181\n",
      "Validation accuracy: 0.4907\n",
      "Validation batch 28, last loss: 0.0215\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 29, last loss: 0.0196\n",
      "Validation accuracy: 0.4922\n",
      "Validation batch 30, last loss: 0.0233\n",
      "Validation accuracy: 0.4892\n",
      "Validation batch 31, last loss: 0.0241\n",
      "Validation accuracy: 0.4855\n",
      "Validation batch 32, last loss: 0.0208\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 33, last loss: 0.0179\n",
      "Validation accuracy: 0.4902\n",
      "Validation batch 34, last loss: 0.0235\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 35, last loss: 0.0226\n",
      "Validation accuracy: 0.4857\n",
      "Validation batch 36, last loss: 0.0194\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 37, last loss: 0.0217\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 38, last loss: 0.0245\n",
      "Validation accuracy: 0.4836\n",
      "Validation batch 39, last loss: 0.0208\n",
      "Validation accuracy: 0.4840\n",
      "Validation batch 40, last loss: 0.0164\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 24 completed, last loss: 0.0164, accuracy: 0.4871\n",
      "\n",
      "Epoch 25/30\n",
      "Training batch 1, last loss: 0.0202\n",
      "Training batch 2, last loss: 0.0246\n",
      "Training batch 3, last loss: 0.0212\n",
      "Training batch 4, last loss: 0.0232\n",
      "Training batch 5, last loss: 0.0204\n",
      "Training batch 6, last loss: 0.0218\n",
      "Training batch 7, last loss: 0.0209\n",
      "Training batch 8, last loss: 0.0192\n",
      "Training batch 9, last loss: 0.0205\n",
      "Training batch 10, last loss: 0.0227\n",
      "Training batch 11, last loss: 0.0243\n",
      "Training batch 12, last loss: 0.0244\n",
      "Training batch 13, last loss: 0.0222\n",
      "Training batch 14, last loss: 0.0218\n",
      "Training batch 15, last loss: 0.0236\n",
      "Training batch 16, last loss: 0.0234\n",
      "Training batch 17, last loss: 0.0225\n",
      "Training batch 18, last loss: 0.0202\n",
      "Training batch 19, last loss: 0.0209\n",
      "Training batch 20, last loss: 0.0158\n",
      "Training batch 21, last loss: 0.0178\n",
      "Training batch 22, last loss: 0.0222\n",
      "Training batch 23, last loss: 0.0218\n",
      "Training batch 24, last loss: 0.0184\n",
      "Training batch 25, last loss: 0.0199\n",
      "Training batch 26, last loss: 0.0160\n",
      "Training batch 27, last loss: 0.0228\n",
      "Training batch 28, last loss: 0.0183\n",
      "Training batch 29, last loss: 0.0225\n",
      "Training batch 30, last loss: 0.0243\n",
      "Training batch 31, last loss: 0.0216\n",
      "Training batch 32, last loss: 0.0198\n",
      "Training batch 33, last loss: 0.0198\n",
      "Training batch 34, last loss: 0.0196\n",
      "Training batch 35, last loss: 0.0191\n",
      "Training batch 36, last loss: 0.0217\n",
      "Training batch 37, last loss: 0.0197\n",
      "Training batch 38, last loss: 0.0244\n",
      "Training batch 39, last loss: 0.0246\n",
      "Training batch 40, last loss: 0.0209\n",
      "Training batch 41, last loss: 0.0168\n",
      "Training batch 42, last loss: 0.0218\n",
      "Training batch 43, last loss: 0.0227\n",
      "Training batch 44, last loss: 0.0202\n",
      "Training batch 45, last loss: 0.0203\n",
      "Training batch 46, last loss: 0.0214\n",
      "Training batch 47, last loss: 0.0186\n",
      "Training batch 48, last loss: 0.0159\n",
      "Training batch 49, last loss: 0.0243\n",
      "Training batch 50, last loss: 0.0178\n",
      "Training batch 51, last loss: 0.0198\n",
      "Training batch 52, last loss: 0.0210\n",
      "Training batch 53, last loss: 0.0233\n",
      "Training batch 54, last loss: 0.0227\n",
      "Training batch 55, last loss: 0.0205\n",
      "Training batch 56, last loss: 0.0230\n",
      "Training batch 57, last loss: 0.0228\n",
      "Training batch 58, last loss: 0.0228\n",
      "Training batch 59, last loss: 0.0257\n",
      "Training batch 60, last loss: 0.0201\n",
      "Training batch 61, last loss: 0.0186\n",
      "Training batch 62, last loss: 0.0253\n",
      "Training batch 63, last loss: 0.0191\n",
      "Training batch 64, last loss: 0.0205\n",
      "Training batch 65, last loss: 0.0232\n",
      "Training batch 66, last loss: 0.0204\n",
      "Training batch 67, last loss: 0.0200\n",
      "Training batch 68, last loss: 0.0210\n",
      "Training batch 69, last loss: 0.0233\n",
      "Training batch 70, last loss: 0.0250\n",
      "Training batch 71, last loss: 0.0229\n",
      "Training batch 72, last loss: 0.0199\n",
      "Training batch 73, last loss: 0.0227\n",
      "Training batch 74, last loss: 0.0219\n",
      "Training batch 75, last loss: 0.0206\n",
      "Training batch 76, last loss: 0.0220\n",
      "Training batch 77, last loss: 0.0177\n",
      "Training batch 78, last loss: 0.0203\n",
      "Training batch 79, last loss: 0.0197\n",
      "Training batch 80, last loss: 0.0213\n",
      "Training batch 81, last loss: 0.0210\n",
      "Training batch 82, last loss: 0.0197\n",
      "Training batch 83, last loss: 0.0196\n",
      "Training batch 84, last loss: 0.0245\n",
      "Training batch 85, last loss: 0.0240\n",
      "Training batch 86, last loss: 0.0235\n",
      "Training batch 87, last loss: 0.0209\n",
      "Training batch 88, last loss: 0.0233\n",
      "Training batch 89, last loss: 0.0224\n",
      "Training batch 90, last loss: 0.0231\n",
      "Training batch 91, last loss: 0.0228\n",
      "Training batch 92, last loss: 0.0200\n",
      "Training batch 93, last loss: 0.0128\n",
      "\n",
      "Training epoch 25 completed, last loss: 0.0128\n",
      "\n",
      "Validation batch 1, last loss: 0.0210\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 2, last loss: 0.0222\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 3, last loss: 0.0196\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 4, last loss: 0.0188\n",
      "Validation accuracy: 0.5188\n",
      "Validation batch 5, last loss: 0.0223\n",
      "Validation accuracy: 0.5050\n",
      "Validation batch 6, last loss: 0.0216\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 7, last loss: 0.0215\n",
      "Validation accuracy: 0.4964\n",
      "Validation batch 8, last loss: 0.0217\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 9, last loss: 0.0219\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 10, last loss: 0.0223\n",
      "Validation accuracy: 0.4825\n",
      "Validation batch 11, last loss: 0.0201\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 12, last loss: 0.0217\n",
      "Validation accuracy: 0.4854\n",
      "Validation batch 13, last loss: 0.0182\n",
      "Validation accuracy: 0.4942\n",
      "Validation batch 14, last loss: 0.0175\n",
      "Validation accuracy: 0.5036\n",
      "Validation batch 15, last loss: 0.0208\n",
      "Validation accuracy: 0.5033\n",
      "Validation batch 16, last loss: 0.0184\n",
      "Validation accuracy: 0.5078\n",
      "Validation batch 17, last loss: 0.0226\n",
      "Validation accuracy: 0.5029\n",
      "Validation batch 18, last loss: 0.0256\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 19, last loss: 0.0182\n",
      "Validation accuracy: 0.4987\n",
      "Validation batch 20, last loss: 0.0195\n",
      "Validation accuracy: 0.5012\n",
      "Validation batch 21, last loss: 0.0204\n",
      "Validation accuracy: 0.5024\n",
      "Validation batch 22, last loss: 0.0216\n",
      "Validation accuracy: 0.5011\n",
      "Validation batch 23, last loss: 0.0233\n",
      "Validation accuracy: 0.4967\n",
      "Validation batch 24, last loss: 0.0208\n",
      "Validation accuracy: 0.4969\n",
      "Validation batch 25, last loss: 0.0235\n",
      "Validation accuracy: 0.4930\n",
      "Validation batch 26, last loss: 0.0250\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 27, last loss: 0.0222\n",
      "Validation accuracy: 0.4861\n",
      "Validation batch 28, last loss: 0.0227\n",
      "Validation accuracy: 0.4839\n",
      "Validation batch 29, last loss: 0.0217\n",
      "Validation accuracy: 0.4828\n",
      "Validation batch 30, last loss: 0.0238\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 31, last loss: 0.0212\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 32, last loss: 0.0196\n",
      "Validation accuracy: 0.4820\n",
      "Validation batch 33, last loss: 0.0220\n",
      "Validation accuracy: 0.4811\n",
      "Validation batch 34, last loss: 0.0221\n",
      "Validation accuracy: 0.4801\n",
      "Validation batch 35, last loss: 0.0214\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 36, last loss: 0.0195\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 37, last loss: 0.0214\n",
      "Validation accuracy: 0.4811\n",
      "Validation batch 38, last loss: 0.0200\n",
      "Validation accuracy: 0.4822\n",
      "Validation batch 39, last loss: 0.0168\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0203\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 25 completed, last loss: 0.0203, accuracy: 0.4871\n",
      "\n",
      "Epoch 26/30\n",
      "Training batch 1, last loss: 0.0212\n",
      "Training batch 2, last loss: 0.0214\n",
      "Training batch 3, last loss: 0.0220\n",
      "Training batch 4, last loss: 0.0180\n",
      "Training batch 5, last loss: 0.0225\n",
      "Training batch 6, last loss: 0.0244\n",
      "Training batch 7, last loss: 0.0263\n",
      "Training batch 8, last loss: 0.0230\n",
      "Training batch 9, last loss: 0.0246\n",
      "Training batch 10, last loss: 0.0215\n",
      "Training batch 11, last loss: 0.0195\n",
      "Training batch 12, last loss: 0.0214\n",
      "Training batch 13, last loss: 0.0213\n",
      "Training batch 14, last loss: 0.0219\n",
      "Training batch 15, last loss: 0.0200\n",
      "Training batch 16, last loss: 0.0241\n",
      "Training batch 17, last loss: 0.0200\n",
      "Training batch 18, last loss: 0.0213\n",
      "Training batch 19, last loss: 0.0224\n",
      "Training batch 20, last loss: 0.0219\n",
      "Training batch 21, last loss: 0.0222\n",
      "Training batch 22, last loss: 0.0234\n",
      "Training batch 23, last loss: 0.0224\n",
      "Training batch 24, last loss: 0.0228\n",
      "Training batch 25, last loss: 0.0198\n",
      "Training batch 26, last loss: 0.0174\n",
      "Training batch 27, last loss: 0.0209\n",
      "Training batch 28, last loss: 0.0208\n",
      "Training batch 29, last loss: 0.0181\n",
      "Training batch 30, last loss: 0.0191\n",
      "Training batch 31, last loss: 0.0209\n",
      "Training batch 32, last loss: 0.0174\n",
      "Training batch 33, last loss: 0.0207\n",
      "Training batch 34, last loss: 0.0236\n",
      "Training batch 35, last loss: 0.0258\n",
      "Training batch 36, last loss: 0.0199\n",
      "Training batch 37, last loss: 0.0226\n",
      "Training batch 38, last loss: 0.0208\n",
      "Training batch 39, last loss: 0.0221\n",
      "Training batch 40, last loss: 0.0221\n",
      "Training batch 41, last loss: 0.0208\n",
      "Training batch 42, last loss: 0.0212\n",
      "Training batch 43, last loss: 0.0189\n",
      "Training batch 44, last loss: 0.0226\n",
      "Training batch 45, last loss: 0.0259\n",
      "Training batch 46, last loss: 0.0192\n",
      "Training batch 47, last loss: 0.0175\n",
      "Training batch 48, last loss: 0.0192\n",
      "Training batch 49, last loss: 0.0234\n",
      "Training batch 50, last loss: 0.0211\n",
      "Training batch 51, last loss: 0.0202\n",
      "Training batch 52, last loss: 0.0220\n",
      "Training batch 53, last loss: 0.0198\n",
      "Training batch 54, last loss: 0.0180\n",
      "Training batch 55, last loss: 0.0183\n",
      "Training batch 56, last loss: 0.0276\n",
      "Training batch 57, last loss: 0.0182\n",
      "Training batch 58, last loss: 0.0189\n",
      "Training batch 59, last loss: 0.0227\n",
      "Training batch 60, last loss: 0.0200\n",
      "Training batch 61, last loss: 0.0194\n",
      "Training batch 62, last loss: 0.0229\n",
      "Training batch 63, last loss: 0.0212\n",
      "Training batch 64, last loss: 0.0200\n",
      "Training batch 65, last loss: 0.0236\n",
      "Training batch 66, last loss: 0.0217\n",
      "Training batch 67, last loss: 0.0235\n",
      "Training batch 68, last loss: 0.0235\n",
      "Training batch 69, last loss: 0.0239\n",
      "Training batch 70, last loss: 0.0206\n",
      "Training batch 71, last loss: 0.0206\n",
      "Training batch 72, last loss: 0.0204\n",
      "Training batch 73, last loss: 0.0230\n",
      "Training batch 74, last loss: 0.0226\n",
      "Training batch 75, last loss: 0.0213\n",
      "Training batch 76, last loss: 0.0256\n",
      "Training batch 77, last loss: 0.0226\n",
      "Training batch 78, last loss: 0.0186\n",
      "Training batch 79, last loss: 0.0237\n",
      "Training batch 80, last loss: 0.0193\n",
      "Training batch 81, last loss: 0.0233\n",
      "Training batch 82, last loss: 0.0194\n",
      "Training batch 83, last loss: 0.0208\n",
      "Training batch 84, last loss: 0.0204\n",
      "Training batch 85, last loss: 0.0233\n",
      "Training batch 86, last loss: 0.0192\n",
      "Training batch 87, last loss: 0.0181\n",
      "Training batch 88, last loss: 0.0233\n",
      "Training batch 89, last loss: 0.0223\n",
      "Training batch 90, last loss: 0.0218\n",
      "Training batch 91, last loss: 0.0227\n",
      "Training batch 92, last loss: 0.0197\n",
      "Training batch 93, last loss: 0.0213\n",
      "\n",
      "Training epoch 26 completed, last loss: 0.0213\n",
      "\n",
      "Validation batch 1, last loss: 0.0186\n",
      "Validation accuracy: 0.5750\n",
      "Validation batch 2, last loss: 0.0213\n",
      "Validation accuracy: 0.5375\n",
      "Validation batch 3, last loss: 0.0194\n",
      "Validation accuracy: 0.5417\n",
      "Validation batch 4, last loss: 0.0229\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 5, last loss: 0.0220\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 6, last loss: 0.0214\n",
      "Validation accuracy: 0.4958\n",
      "Validation batch 7, last loss: 0.0222\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 8, last loss: 0.0222\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 9, last loss: 0.0216\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 10, last loss: 0.0210\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 11, last loss: 0.0205\n",
      "Validation accuracy: 0.4841\n",
      "Validation batch 12, last loss: 0.0213\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 13, last loss: 0.0224\n",
      "Validation accuracy: 0.4808\n",
      "Validation batch 14, last loss: 0.0205\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 15, last loss: 0.0259\n",
      "Validation accuracy: 0.4700\n",
      "Validation batch 16, last loss: 0.0214\n",
      "Validation accuracy: 0.4703\n",
      "Validation batch 17, last loss: 0.0224\n",
      "Validation accuracy: 0.4676\n",
      "Validation batch 18, last loss: 0.0152\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 19, last loss: 0.0232\n",
      "Validation accuracy: 0.4776\n",
      "Validation batch 20, last loss: 0.0235\n",
      "Validation accuracy: 0.4738\n",
      "Validation batch 21, last loss: 0.0188\n",
      "Validation accuracy: 0.4786\n",
      "Validation batch 22, last loss: 0.0221\n",
      "Validation accuracy: 0.4773\n",
      "Validation batch 23, last loss: 0.0199\n",
      "Validation accuracy: 0.4793\n",
      "Validation batch 24, last loss: 0.0203\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 25, last loss: 0.0225\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 26, last loss: 0.0192\n",
      "Validation accuracy: 0.4827\n",
      "Validation batch 27, last loss: 0.0219\n",
      "Validation accuracy: 0.4815\n",
      "Validation batch 28, last loss: 0.0217\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 29, last loss: 0.0219\n",
      "Validation accuracy: 0.4802\n",
      "Validation batch 30, last loss: 0.0233\n",
      "Validation accuracy: 0.4775\n",
      "Validation batch 31, last loss: 0.0204\n",
      "Validation accuracy: 0.4790\n",
      "Validation batch 32, last loss: 0.0187\n",
      "Validation accuracy: 0.4820\n",
      "Validation batch 33, last loss: 0.0209\n",
      "Validation accuracy: 0.4826\n",
      "Validation batch 34, last loss: 0.0205\n",
      "Validation accuracy: 0.4831\n",
      "Validation batch 35, last loss: 0.0198\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 36, last loss: 0.0217\n",
      "Validation accuracy: 0.4847\n",
      "Validation batch 37, last loss: 0.0211\n",
      "Validation accuracy: 0.4845\n",
      "Validation batch 38, last loss: 0.0244\n",
      "Validation accuracy: 0.4816\n",
      "Validation batch 39, last loss: 0.0211\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 40, last loss: 0.0135\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 26 completed, last loss: 0.0135, accuracy: 0.4871\n",
      "\n",
      "Epoch 27/30\n",
      "Training batch 1, last loss: 0.0228\n",
      "Training batch 2, last loss: 0.0252\n",
      "Training batch 3, last loss: 0.0198\n",
      "Training batch 4, last loss: 0.0208\n",
      "Training batch 5, last loss: 0.0187\n",
      "Training batch 6, last loss: 0.0204\n",
      "Training batch 7, last loss: 0.0205\n",
      "Training batch 8, last loss: 0.0234\n",
      "Training batch 9, last loss: 0.0215\n",
      "Training batch 10, last loss: 0.0206\n",
      "Training batch 11, last loss: 0.0228\n",
      "Training batch 12, last loss: 0.0254\n",
      "Training batch 13, last loss: 0.0215\n",
      "Training batch 14, last loss: 0.0202\n",
      "Training batch 15, last loss: 0.0230\n",
      "Training batch 16, last loss: 0.0250\n",
      "Training batch 17, last loss: 0.0187\n",
      "Training batch 18, last loss: 0.0192\n",
      "Training batch 19, last loss: 0.0220\n",
      "Training batch 20, last loss: 0.0190\n",
      "Training batch 21, last loss: 0.0220\n",
      "Training batch 22, last loss: 0.0263\n",
      "Training batch 23, last loss: 0.0220\n",
      "Training batch 24, last loss: 0.0218\n",
      "Training batch 25, last loss: 0.0212\n",
      "Training batch 26, last loss: 0.0224\n",
      "Training batch 27, last loss: 0.0221\n",
      "Training batch 28, last loss: 0.0230\n",
      "Training batch 29, last loss: 0.0242\n",
      "Training batch 30, last loss: 0.0196\n",
      "Training batch 31, last loss: 0.0194\n",
      "Training batch 32, last loss: 0.0212\n",
      "Training batch 33, last loss: 0.0207\n",
      "Training batch 34, last loss: 0.0244\n",
      "Training batch 35, last loss: 0.0228\n",
      "Training batch 36, last loss: 0.0242\n",
      "Training batch 37, last loss: 0.0201\n",
      "Training batch 38, last loss: 0.0209\n",
      "Training batch 39, last loss: 0.0216\n",
      "Training batch 40, last loss: 0.0203\n",
      "Training batch 41, last loss: 0.0213\n",
      "Training batch 42, last loss: 0.0197\n",
      "Training batch 43, last loss: 0.0241\n",
      "Training batch 44, last loss: 0.0195\n",
      "Training batch 45, last loss: 0.0216\n",
      "Training batch 46, last loss: 0.0211\n",
      "Training batch 47, last loss: 0.0163\n",
      "Training batch 48, last loss: 0.0203\n",
      "Training batch 49, last loss: 0.0217\n",
      "Training batch 50, last loss: 0.0204\n",
      "Training batch 51, last loss: 0.0243\n",
      "Training batch 52, last loss: 0.0252\n",
      "Training batch 53, last loss: 0.0204\n",
      "Training batch 54, last loss: 0.0212\n",
      "Training batch 55, last loss: 0.0192\n",
      "Training batch 56, last loss: 0.0197\n",
      "Training batch 57, last loss: 0.0215\n",
      "Training batch 58, last loss: 0.0212\n",
      "Training batch 59, last loss: 0.0208\n",
      "Training batch 60, last loss: 0.0209\n",
      "Training batch 61, last loss: 0.0213\n",
      "Training batch 62, last loss: 0.0180\n",
      "Training batch 63, last loss: 0.0217\n",
      "Training batch 64, last loss: 0.0180\n",
      "Training batch 65, last loss: 0.0208\n",
      "Training batch 66, last loss: 0.0187\n",
      "Training batch 67, last loss: 0.0189\n",
      "Training batch 68, last loss: 0.0199\n",
      "Training batch 69, last loss: 0.0183\n",
      "Training batch 70, last loss: 0.0208\n",
      "Training batch 71, last loss: 0.0194\n",
      "Training batch 72, last loss: 0.0211\n",
      "Training batch 73, last loss: 0.0201\n",
      "Training batch 74, last loss: 0.0202\n",
      "Training batch 75, last loss: 0.0240\n",
      "Training batch 76, last loss: 0.0224\n",
      "Training batch 77, last loss: 0.0200\n",
      "Training batch 78, last loss: 0.0224\n",
      "Training batch 79, last loss: 0.0222\n",
      "Training batch 80, last loss: 0.0206\n",
      "Training batch 81, last loss: 0.0233\n",
      "Training batch 82, last loss: 0.0270\n",
      "Training batch 83, last loss: 0.0193\n",
      "Training batch 84, last loss: 0.0208\n",
      "Training batch 85, last loss: 0.0242\n",
      "Training batch 86, last loss: 0.0206\n",
      "Training batch 87, last loss: 0.0187\n",
      "Training batch 88, last loss: 0.0217\n",
      "Training batch 89, last loss: 0.0236\n",
      "Training batch 90, last loss: 0.0239\n",
      "Training batch 91, last loss: 0.0201\n",
      "Training batch 92, last loss: 0.0194\n",
      "Training batch 93, last loss: 0.0208\n",
      "\n",
      "Training epoch 27 completed, last loss: 0.0208\n",
      "\n",
      "Validation batch 1, last loss: 0.0202\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 2, last loss: 0.0194\n",
      "Validation accuracy: 0.5375\n",
      "Validation batch 3, last loss: 0.0200\n",
      "Validation accuracy: 0.5333\n",
      "Validation batch 4, last loss: 0.0216\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 5, last loss: 0.0216\n",
      "Validation accuracy: 0.5150\n",
      "Validation batch 6, last loss: 0.0201\n",
      "Validation accuracy: 0.5167\n",
      "Validation batch 7, last loss: 0.0220\n",
      "Validation accuracy: 0.5071\n",
      "Validation batch 8, last loss: 0.0249\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 9, last loss: 0.0200\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 10, last loss: 0.0185\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 11, last loss: 0.0243\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 12, last loss: 0.0214\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 13, last loss: 0.0212\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 14, last loss: 0.0238\n",
      "Validation accuracy: 0.4804\n",
      "Validation batch 15, last loss: 0.0228\n",
      "Validation accuracy: 0.4767\n",
      "Validation batch 16, last loss: 0.0180\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 17, last loss: 0.0196\n",
      "Validation accuracy: 0.4868\n",
      "Validation batch 18, last loss: 0.0194\n",
      "Validation accuracy: 0.4903\n",
      "Validation batch 19, last loss: 0.0209\n",
      "Validation accuracy: 0.4908\n",
      "Validation batch 20, last loss: 0.0235\n",
      "Validation accuracy: 0.4863\n",
      "Validation batch 21, last loss: 0.0201\n",
      "Validation accuracy: 0.4881\n",
      "Validation batch 22, last loss: 0.0188\n",
      "Validation accuracy: 0.4920\n",
      "Validation batch 23, last loss: 0.0216\n",
      "Validation accuracy: 0.4913\n",
      "Validation batch 24, last loss: 0.0193\n",
      "Validation accuracy: 0.4938\n",
      "Validation batch 25, last loss: 0.0212\n",
      "Validation accuracy: 0.4940\n",
      "Validation batch 26, last loss: 0.0220\n",
      "Validation accuracy: 0.4923\n",
      "Validation batch 27, last loss: 0.0190\n",
      "Validation accuracy: 0.4944\n",
      "Validation batch 28, last loss: 0.0203\n",
      "Validation accuracy: 0.4946\n",
      "Validation batch 29, last loss: 0.0181\n",
      "Validation accuracy: 0.4983\n",
      "Validation batch 30, last loss: 0.0211\n",
      "Validation accuracy: 0.4975\n",
      "Validation batch 31, last loss: 0.0213\n",
      "Validation accuracy: 0.4968\n",
      "Validation batch 32, last loss: 0.0220\n",
      "Validation accuracy: 0.4953\n",
      "Validation batch 33, last loss: 0.0209\n",
      "Validation accuracy: 0.4955\n",
      "Validation batch 34, last loss: 0.0211\n",
      "Validation accuracy: 0.4956\n",
      "Validation batch 35, last loss: 0.0249\n",
      "Validation accuracy: 0.4914\n",
      "Validation batch 36, last loss: 0.0204\n",
      "Validation accuracy: 0.4924\n",
      "Validation batch 37, last loss: 0.0257\n",
      "Validation accuracy: 0.4872\n",
      "Validation batch 38, last loss: 0.0237\n",
      "Validation accuracy: 0.4849\n",
      "Validation batch 39, last loss: 0.0201\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 40, last loss: 0.0196\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 27 completed, last loss: 0.0196, accuracy: 0.4871\n",
      "\n",
      "Epoch 28/30\n",
      "Training batch 1, last loss: 0.0231\n",
      "Training batch 2, last loss: 0.0220\n",
      "Training batch 3, last loss: 0.0240\n",
      "Training batch 4, last loss: 0.0172\n",
      "Training batch 5, last loss: 0.0150\n",
      "Training batch 6, last loss: 0.0206\n",
      "Training batch 7, last loss: 0.0245\n",
      "Training batch 8, last loss: 0.0218\n",
      "Training batch 9, last loss: 0.0237\n",
      "Training batch 10, last loss: 0.0212\n",
      "Training batch 11, last loss: 0.0234\n",
      "Training batch 12, last loss: 0.0218\n",
      "Training batch 13, last loss: 0.0207\n",
      "Training batch 14, last loss: 0.0237\n",
      "Training batch 15, last loss: 0.0207\n",
      "Training batch 16, last loss: 0.0215\n",
      "Training batch 17, last loss: 0.0214\n",
      "Training batch 18, last loss: 0.0195\n",
      "Training batch 19, last loss: 0.0177\n",
      "Training batch 20, last loss: 0.0230\n",
      "Training batch 21, last loss: 0.0240\n",
      "Training batch 22, last loss: 0.0186\n",
      "Training batch 23, last loss: 0.0212\n",
      "Training batch 24, last loss: 0.0211\n",
      "Training batch 25, last loss: 0.0216\n",
      "Training batch 26, last loss: 0.0237\n",
      "Training batch 27, last loss: 0.0192\n",
      "Training batch 28, last loss: 0.0212\n",
      "Training batch 29, last loss: 0.0217\n",
      "Training batch 30, last loss: 0.0253\n",
      "Training batch 31, last loss: 0.0228\n",
      "Training batch 32, last loss: 0.0274\n",
      "Training batch 33, last loss: 0.0190\n",
      "Training batch 34, last loss: 0.0235\n",
      "Training batch 35, last loss: 0.0197\n",
      "Training batch 36, last loss: 0.0239\n",
      "Training batch 37, last loss: 0.0174\n",
      "Training batch 38, last loss: 0.0274\n",
      "Training batch 39, last loss: 0.0174\n",
      "Training batch 40, last loss: 0.0200\n",
      "Training batch 41, last loss: 0.0223\n",
      "Training batch 42, last loss: 0.0204\n",
      "Training batch 43, last loss: 0.0191\n",
      "Training batch 44, last loss: 0.0172\n",
      "Training batch 45, last loss: 0.0179\n",
      "Training batch 46, last loss: 0.0232\n",
      "Training batch 47, last loss: 0.0230\n",
      "Training batch 48, last loss: 0.0237\n",
      "Training batch 49, last loss: 0.0220\n",
      "Training batch 50, last loss: 0.0189\n",
      "Training batch 51, last loss: 0.0220\n",
      "Training batch 52, last loss: 0.0212\n",
      "Training batch 53, last loss: 0.0213\n",
      "Training batch 54, last loss: 0.0203\n",
      "Training batch 55, last loss: 0.0244\n",
      "Training batch 56, last loss: 0.0222\n",
      "Training batch 57, last loss: 0.0213\n",
      "Training batch 58, last loss: 0.0234\n",
      "Training batch 59, last loss: 0.0197\n",
      "Training batch 60, last loss: 0.0228\n",
      "Training batch 61, last loss: 0.0154\n",
      "Training batch 62, last loss: 0.0214\n",
      "Training batch 63, last loss: 0.0194\n",
      "Training batch 64, last loss: 0.0258\n",
      "Training batch 65, last loss: 0.0229\n",
      "Training batch 66, last loss: 0.0206\n",
      "Training batch 67, last loss: 0.0216\n",
      "Training batch 68, last loss: 0.0188\n",
      "Training batch 69, last loss: 0.0166\n",
      "Training batch 70, last loss: 0.0211\n",
      "Training batch 71, last loss: 0.0225\n",
      "Training batch 72, last loss: 0.0235\n",
      "Training batch 73, last loss: 0.0229\n",
      "Training batch 74, last loss: 0.0213\n",
      "Training batch 75, last loss: 0.0189\n",
      "Training batch 76, last loss: 0.0210\n",
      "Training batch 77, last loss: 0.0177\n",
      "Training batch 78, last loss: 0.0203\n",
      "Training batch 79, last loss: 0.0175\n",
      "Training batch 80, last loss: 0.0249\n",
      "Training batch 81, last loss: 0.0219\n",
      "Training batch 82, last loss: 0.0185\n",
      "Training batch 83, last loss: 0.0228\n",
      "Training batch 84, last loss: 0.0216\n",
      "Training batch 85, last loss: 0.0210\n",
      "Training batch 86, last loss: 0.0215\n",
      "Training batch 87, last loss: 0.0186\n",
      "Training batch 88, last loss: 0.0191\n",
      "Training batch 89, last loss: 0.0230\n",
      "Training batch 90, last loss: 0.0234\n",
      "Training batch 91, last loss: 0.0203\n",
      "Training batch 92, last loss: 0.0238\n",
      "Training batch 93, last loss: 0.0214\n",
      "\n",
      "Training epoch 28 completed, last loss: 0.0214\n",
      "\n",
      "Validation batch 1, last loss: 0.0210\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 2, last loss: 0.0209\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 3, last loss: 0.0256\n",
      "Validation accuracy: 0.4417\n",
      "Validation batch 4, last loss: 0.0222\n",
      "Validation accuracy: 0.4437\n",
      "Validation batch 5, last loss: 0.0227\n",
      "Validation accuracy: 0.4400\n",
      "Validation batch 6, last loss: 0.0212\n",
      "Validation accuracy: 0.4458\n",
      "Validation batch 7, last loss: 0.0216\n",
      "Validation accuracy: 0.4464\n",
      "Validation batch 8, last loss: 0.0179\n",
      "Validation accuracy: 0.4656\n",
      "Validation batch 9, last loss: 0.0228\n",
      "Validation accuracy: 0.4611\n",
      "Validation batch 10, last loss: 0.0188\n",
      "Validation accuracy: 0.4725\n",
      "Validation batch 11, last loss: 0.0212\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 12, last loss: 0.0218\n",
      "Validation accuracy: 0.4729\n",
      "Validation batch 13, last loss: 0.0195\n",
      "Validation accuracy: 0.4788\n",
      "Validation batch 14, last loss: 0.0210\n",
      "Validation accuracy: 0.4804\n",
      "Validation batch 15, last loss: 0.0191\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 16, last loss: 0.0215\n",
      "Validation accuracy: 0.4844\n",
      "Validation batch 17, last loss: 0.0228\n",
      "Validation accuracy: 0.4809\n",
      "Validation batch 18, last loss: 0.0205\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 19, last loss: 0.0187\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 20, last loss: 0.0185\n",
      "Validation accuracy: 0.4925\n",
      "Validation batch 21, last loss: 0.0217\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 22, last loss: 0.0212\n",
      "Validation accuracy: 0.4909\n",
      "Validation batch 23, last loss: 0.0273\n",
      "Validation accuracy: 0.4815\n",
      "Validation batch 24, last loss: 0.0217\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 25, last loss: 0.0216\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 26, last loss: 0.0183\n",
      "Validation accuracy: 0.4837\n",
      "Validation batch 27, last loss: 0.0236\n",
      "Validation accuracy: 0.4806\n",
      "Validation batch 28, last loss: 0.0190\n",
      "Validation accuracy: 0.4839\n",
      "Validation batch 29, last loss: 0.0200\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 30, last loss: 0.0223\n",
      "Validation accuracy: 0.4842\n",
      "Validation batch 31, last loss: 0.0167\n",
      "Validation accuracy: 0.4895\n",
      "Validation batch 32, last loss: 0.0223\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 33, last loss: 0.0199\n",
      "Validation accuracy: 0.4894\n",
      "Validation batch 34, last loss: 0.0207\n",
      "Validation accuracy: 0.4904\n",
      "Validation batch 35, last loss: 0.0232\n",
      "Validation accuracy: 0.4886\n",
      "Validation batch 36, last loss: 0.0212\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 37, last loss: 0.0213\n",
      "Validation accuracy: 0.4878\n",
      "Validation batch 38, last loss: 0.0209\n",
      "Validation accuracy: 0.4882\n",
      "Validation batch 39, last loss: 0.0231\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 40, last loss: 0.0192\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 28 completed, last loss: 0.0192, accuracy: 0.4871\n",
      "\n",
      "Epoch 29/30\n",
      "Training batch 1, last loss: 0.0197\n",
      "Training batch 2, last loss: 0.0214\n",
      "Training batch 3, last loss: 0.0223\n",
      "Training batch 4, last loss: 0.0171\n",
      "Training batch 5, last loss: 0.0212\n",
      "Training batch 6, last loss: 0.0245\n",
      "Training batch 7, last loss: 0.0182\n",
      "Training batch 8, last loss: 0.0212\n",
      "Training batch 9, last loss: 0.0190\n",
      "Training batch 10, last loss: 0.0215\n",
      "Training batch 11, last loss: 0.0235\n",
      "Training batch 12, last loss: 0.0232\n",
      "Training batch 13, last loss: 0.0232\n",
      "Training batch 14, last loss: 0.0246\n",
      "Training batch 15, last loss: 0.0251\n",
      "Training batch 16, last loss: 0.0203\n",
      "Training batch 17, last loss: 0.0209\n",
      "Training batch 18, last loss: 0.0205\n",
      "Training batch 19, last loss: 0.0201\n",
      "Training batch 20, last loss: 0.0178\n",
      "Training batch 21, last loss: 0.0191\n",
      "Training batch 22, last loss: 0.0229\n",
      "Training batch 23, last loss: 0.0218\n",
      "Training batch 24, last loss: 0.0232\n",
      "Training batch 25, last loss: 0.0214\n",
      "Training batch 26, last loss: 0.0206\n",
      "Training batch 27, last loss: 0.0206\n",
      "Training batch 28, last loss: 0.0207\n",
      "Training batch 29, last loss: 0.0210\n",
      "Training batch 30, last loss: 0.0204\n",
      "Training batch 31, last loss: 0.0227\n",
      "Training batch 32, last loss: 0.0215\n",
      "Training batch 33, last loss: 0.0239\n",
      "Training batch 34, last loss: 0.0200\n",
      "Training batch 35, last loss: 0.0246\n",
      "Training batch 36, last loss: 0.0191\n",
      "Training batch 37, last loss: 0.0208\n",
      "Training batch 38, last loss: 0.0183\n",
      "Training batch 39, last loss: 0.0191\n",
      "Training batch 40, last loss: 0.0178\n",
      "Training batch 41, last loss: 0.0220\n",
      "Training batch 42, last loss: 0.0222\n",
      "Training batch 43, last loss: 0.0168\n",
      "Training batch 44, last loss: 0.0235\n",
      "Training batch 45, last loss: 0.0240\n",
      "Training batch 46, last loss: 0.0248\n",
      "Training batch 47, last loss: 0.0229\n",
      "Training batch 48, last loss: 0.0229\n",
      "Training batch 49, last loss: 0.0286\n",
      "Training batch 50, last loss: 0.0248\n",
      "Training batch 51, last loss: 0.0221\n",
      "Training batch 52, last loss: 0.0197\n",
      "Training batch 53, last loss: 0.0201\n",
      "Training batch 54, last loss: 0.0190\n",
      "Training batch 55, last loss: 0.0212\n",
      "Training batch 56, last loss: 0.0240\n",
      "Training batch 57, last loss: 0.0227\n",
      "Training batch 58, last loss: 0.0227\n",
      "Training batch 59, last loss: 0.0257\n",
      "Training batch 60, last loss: 0.0193\n",
      "Training batch 61, last loss: 0.0221\n",
      "Training batch 62, last loss: 0.0201\n",
      "Training batch 63, last loss: 0.0196\n",
      "Training batch 64, last loss: 0.0226\n",
      "Training batch 65, last loss: 0.0225\n",
      "Training batch 66, last loss: 0.0219\n",
      "Training batch 67, last loss: 0.0214\n",
      "Training batch 68, last loss: 0.0186\n",
      "Training batch 69, last loss: 0.0205\n",
      "Training batch 70, last loss: 0.0266\n",
      "Training batch 71, last loss: 0.0221\n",
      "Training batch 72, last loss: 0.0214\n",
      "Training batch 73, last loss: 0.0197\n",
      "Training batch 74, last loss: 0.0264\n",
      "Training batch 75, last loss: 0.0216\n",
      "Training batch 76, last loss: 0.0214\n",
      "Training batch 77, last loss: 0.0230\n",
      "Training batch 78, last loss: 0.0176\n",
      "Training batch 79, last loss: 0.0163\n",
      "Training batch 80, last loss: 0.0192\n",
      "Training batch 81, last loss: 0.0198\n",
      "Training batch 82, last loss: 0.0225\n",
      "Training batch 83, last loss: 0.0220\n",
      "Training batch 84, last loss: 0.0204\n",
      "Training batch 85, last loss: 0.0254\n",
      "Training batch 86, last loss: 0.0208\n",
      "Training batch 87, last loss: 0.0179\n",
      "Training batch 88, last loss: 0.0175\n",
      "Training batch 89, last loss: 0.0199\n",
      "Training batch 90, last loss: 0.0211\n",
      "Training batch 91, last loss: 0.0232\n",
      "Training batch 92, last loss: 0.0203\n",
      "Training batch 93, last loss: 0.0199\n",
      "\n",
      "Training epoch 29 completed, last loss: 0.0199\n",
      "\n",
      "Validation batch 1, last loss: 0.0250\n",
      "Validation accuracy: 0.3500\n",
      "Validation batch 2, last loss: 0.0199\n",
      "Validation accuracy: 0.4500\n",
      "Validation batch 3, last loss: 0.0207\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 4, last loss: 0.0214\n",
      "Validation accuracy: 0.4688\n",
      "Validation batch 5, last loss: 0.0204\n",
      "Validation accuracy: 0.4750\n",
      "Validation batch 6, last loss: 0.0228\n",
      "Validation accuracy: 0.4667\n",
      "Validation batch 7, last loss: 0.0175\n",
      "Validation accuracy: 0.4893\n",
      "Validation batch 8, last loss: 0.0214\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 9, last loss: 0.0209\n",
      "Validation accuracy: 0.4889\n",
      "Validation batch 10, last loss: 0.0214\n",
      "Validation accuracy: 0.4875\n",
      "Validation batch 11, last loss: 0.0244\n",
      "Validation accuracy: 0.4773\n",
      "Validation batch 12, last loss: 0.0193\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 13, last loss: 0.0224\n",
      "Validation accuracy: 0.4808\n",
      "Validation batch 14, last loss: 0.0200\n",
      "Validation accuracy: 0.4839\n",
      "Validation batch 15, last loss: 0.0225\n",
      "Validation accuracy: 0.4800\n",
      "Validation batch 16, last loss: 0.0210\n",
      "Validation accuracy: 0.4813\n",
      "Validation batch 17, last loss: 0.0216\n",
      "Validation accuracy: 0.4809\n",
      "Validation batch 18, last loss: 0.0212\n",
      "Validation accuracy: 0.4819\n",
      "Validation batch 19, last loss: 0.0225\n",
      "Validation accuracy: 0.4789\n",
      "Validation batch 20, last loss: 0.0216\n",
      "Validation accuracy: 0.4788\n",
      "Validation batch 21, last loss: 0.0198\n",
      "Validation accuracy: 0.4821\n",
      "Validation batch 22, last loss: 0.0227\n",
      "Validation accuracy: 0.4795\n",
      "Validation batch 23, last loss: 0.0166\n",
      "Validation accuracy: 0.4870\n",
      "Validation batch 24, last loss: 0.0184\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 25, last loss: 0.0260\n",
      "Validation accuracy: 0.4840\n",
      "Validation batch 26, last loss: 0.0208\n",
      "Validation accuracy: 0.4846\n",
      "Validation batch 27, last loss: 0.0222\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 28, last loss: 0.0199\n",
      "Validation accuracy: 0.4848\n",
      "Validation batch 29, last loss: 0.0215\n",
      "Validation accuracy: 0.4845\n",
      "Validation batch 30, last loss: 0.0223\n",
      "Validation accuracy: 0.4825\n",
      "Validation batch 31, last loss: 0.0241\n",
      "Validation accuracy: 0.4790\n",
      "Validation batch 32, last loss: 0.0202\n",
      "Validation accuracy: 0.4797\n",
      "Validation batch 33, last loss: 0.0189\n",
      "Validation accuracy: 0.4826\n",
      "Validation batch 34, last loss: 0.0188\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 35, last loss: 0.0218\n",
      "Validation accuracy: 0.4843\n",
      "Validation batch 36, last loss: 0.0215\n",
      "Validation accuracy: 0.4840\n",
      "Validation batch 37, last loss: 0.0210\n",
      "Validation accuracy: 0.4845\n",
      "Validation batch 38, last loss: 0.0220\n",
      "Validation accuracy: 0.4836\n",
      "Validation batch 39, last loss: 0.0198\n",
      "Validation accuracy: 0.4853\n",
      "Validation batch 40, last loss: 0.0181\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 29 completed, last loss: 0.0181, accuracy: 0.4871\n",
      "\n",
      "Epoch 30/30\n",
      "Training batch 1, last loss: 0.0221\n",
      "Training batch 2, last loss: 0.0201\n",
      "Training batch 3, last loss: 0.0189\n",
      "Training batch 4, last loss: 0.0203\n",
      "Training batch 5, last loss: 0.0223\n",
      "Training batch 6, last loss: 0.0217\n",
      "Training batch 7, last loss: 0.0238\n",
      "Training batch 8, last loss: 0.0203\n",
      "Training batch 9, last loss: 0.0205\n",
      "Training batch 10, last loss: 0.0211\n",
      "Training batch 11, last loss: 0.0208\n",
      "Training batch 12, last loss: 0.0259\n",
      "Training batch 13, last loss: 0.0259\n",
      "Training batch 14, last loss: 0.0245\n",
      "Training batch 15, last loss: 0.0222\n",
      "Training batch 16, last loss: 0.0201\n",
      "Training batch 17, last loss: 0.0203\n",
      "Training batch 18, last loss: 0.0185\n",
      "Training batch 19, last loss: 0.0203\n",
      "Training batch 20, last loss: 0.0195\n",
      "Training batch 21, last loss: 0.0193\n",
      "Training batch 22, last loss: 0.0244\n",
      "Training batch 23, last loss: 0.0216\n",
      "Training batch 24, last loss: 0.0210\n",
      "Training batch 25, last loss: 0.0203\n",
      "Training batch 26, last loss: 0.0198\n",
      "Training batch 27, last loss: 0.0212\n",
      "Training batch 28, last loss: 0.0217\n",
      "Training batch 29, last loss: 0.0193\n",
      "Training batch 30, last loss: 0.0232\n",
      "Training batch 31, last loss: 0.0212\n",
      "Training batch 32, last loss: 0.0209\n",
      "Training batch 33, last loss: 0.0248\n",
      "Training batch 34, last loss: 0.0208\n",
      "Training batch 35, last loss: 0.0195\n",
      "Training batch 36, last loss: 0.0211\n",
      "Training batch 37, last loss: 0.0246\n",
      "Training batch 38, last loss: 0.0227\n",
      "Training batch 39, last loss: 0.0204\n",
      "Training batch 40, last loss: 0.0195\n",
      "Training batch 41, last loss: 0.0212\n",
      "Training batch 42, last loss: 0.0205\n",
      "Training batch 43, last loss: 0.0219\n",
      "Training batch 44, last loss: 0.0233\n",
      "Training batch 45, last loss: 0.0189\n",
      "Training batch 46, last loss: 0.0194\n",
      "Training batch 47, last loss: 0.0203\n",
      "Training batch 48, last loss: 0.0210\n",
      "Training batch 49, last loss: 0.0205\n",
      "Training batch 50, last loss: 0.0206\n",
      "Training batch 51, last loss: 0.0184\n",
      "Training batch 52, last loss: 0.0234\n",
      "Training batch 53, last loss: 0.0231\n",
      "Training batch 54, last loss: 0.0197\n",
      "Training batch 55, last loss: 0.0206\n",
      "Training batch 56, last loss: 0.0238\n",
      "Training batch 57, last loss: 0.0190\n",
      "Training batch 58, last loss: 0.0224\n",
      "Training batch 59, last loss: 0.0222\n",
      "Training batch 60, last loss: 0.0198\n",
      "Training batch 61, last loss: 0.0259\n",
      "Training batch 62, last loss: 0.0206\n",
      "Training batch 63, last loss: 0.0220\n",
      "Training batch 64, last loss: 0.0223\n",
      "Training batch 65, last loss: 0.0214\n",
      "Training batch 66, last loss: 0.0184\n",
      "Training batch 67, last loss: 0.0222\n",
      "Training batch 68, last loss: 0.0236\n",
      "Training batch 69, last loss: 0.0217\n",
      "Training batch 70, last loss: 0.0189\n",
      "Training batch 71, last loss: 0.0242\n",
      "Training batch 72, last loss: 0.0180\n",
      "Training batch 73, last loss: 0.0211\n",
      "Training batch 74, last loss: 0.0225\n",
      "Training batch 75, last loss: 0.0220\n",
      "Training batch 76, last loss: 0.0236\n",
      "Training batch 77, last loss: 0.0235\n",
      "Training batch 78, last loss: 0.0151\n",
      "Training batch 79, last loss: 0.0239\n",
      "Training batch 80, last loss: 0.0191\n",
      "Training batch 81, last loss: 0.0169\n",
      "Training batch 82, last loss: 0.0208\n",
      "Training batch 83, last loss: 0.0189\n",
      "Training batch 84, last loss: 0.0212\n",
      "Training batch 85, last loss: 0.0214\n",
      "Training batch 86, last loss: 0.0212\n",
      "Training batch 87, last loss: 0.0257\n",
      "Training batch 88, last loss: 0.0184\n",
      "Training batch 89, last loss: 0.0240\n",
      "Training batch 90, last loss: 0.0202\n",
      "Training batch 91, last loss: 0.0217\n",
      "Training batch 92, last loss: 0.0228\n",
      "Training batch 93, last loss: 0.0270\n",
      "\n",
      "Training epoch 30 completed, last loss: 0.0270\n",
      "\n",
      "Validation batch 1, last loss: 0.0204\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 2, last loss: 0.0192\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 3, last loss: 0.0204\n",
      "Validation accuracy: 0.5250\n",
      "Validation batch 4, last loss: 0.0219\n",
      "Validation accuracy: 0.5062\n",
      "Validation batch 5, last loss: 0.0212\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 6, last loss: 0.0202\n",
      "Validation accuracy: 0.5042\n",
      "Validation batch 7, last loss: 0.0214\n",
      "Validation accuracy: 0.5000\n",
      "Validation batch 8, last loss: 0.0226\n",
      "Validation accuracy: 0.4906\n",
      "Validation batch 9, last loss: 0.0175\n",
      "Validation accuracy: 0.5056\n",
      "Validation batch 10, last loss: 0.0186\n",
      "Validation accuracy: 0.5125\n",
      "Validation batch 11, last loss: 0.0227\n",
      "Validation accuracy: 0.5045\n",
      "Validation batch 12, last loss: 0.0216\n",
      "Validation accuracy: 0.5021\n",
      "Validation batch 13, last loss: 0.0181\n",
      "Validation accuracy: 0.5096\n",
      "Validation batch 14, last loss: 0.0234\n",
      "Validation accuracy: 0.5036\n",
      "Validation batch 15, last loss: 0.0254\n",
      "Validation accuracy: 0.4917\n",
      "Validation batch 16, last loss: 0.0235\n",
      "Validation accuracy: 0.4859\n",
      "Validation batch 17, last loss: 0.0232\n",
      "Validation accuracy: 0.4809\n",
      "Validation batch 18, last loss: 0.0192\n",
      "Validation accuracy: 0.4847\n",
      "Validation batch 19, last loss: 0.0182\n",
      "Validation accuracy: 0.4895\n",
      "Validation batch 20, last loss: 0.0223\n",
      "Validation accuracy: 0.4863\n",
      "Validation batch 21, last loss: 0.0187\n",
      "Validation accuracy: 0.4905\n",
      "Validation batch 22, last loss: 0.0189\n",
      "Validation accuracy: 0.4943\n",
      "Validation batch 23, last loss: 0.0169\n",
      "Validation accuracy: 0.5011\n",
      "Validation batch 24, last loss: 0.0239\n",
      "Validation accuracy: 0.4969\n",
      "Validation batch 25, last loss: 0.0192\n",
      "Validation accuracy: 0.4990\n",
      "Validation batch 26, last loss: 0.0238\n",
      "Validation accuracy: 0.4952\n",
      "Validation batch 27, last loss: 0.0225\n",
      "Validation accuracy: 0.4935\n",
      "Validation batch 28, last loss: 0.0183\n",
      "Validation accuracy: 0.4964\n",
      "Validation batch 29, last loss: 0.0249\n",
      "Validation accuracy: 0.4914\n",
      "Validation batch 30, last loss: 0.0238\n",
      "Validation accuracy: 0.4883\n",
      "Validation batch 31, last loss: 0.0214\n",
      "Validation accuracy: 0.4879\n",
      "Validation batch 32, last loss: 0.0221\n",
      "Validation accuracy: 0.4867\n",
      "Validation batch 33, last loss: 0.0222\n",
      "Validation accuracy: 0.4856\n",
      "Validation batch 34, last loss: 0.0232\n",
      "Validation accuracy: 0.4838\n",
      "Validation batch 35, last loss: 0.0202\n",
      "Validation accuracy: 0.4850\n",
      "Validation batch 36, last loss: 0.0228\n",
      "Validation accuracy: 0.4833\n",
      "Validation batch 37, last loss: 0.0190\n",
      "Validation accuracy: 0.4858\n",
      "Validation batch 38, last loss: 0.0182\n",
      "Validation accuracy: 0.4888\n",
      "Validation batch 39, last loss: 0.0236\n",
      "Validation accuracy: 0.4865\n",
      "Validation batch 40, last loss: 0.0204\n",
      "Validation accuracy: 0.4831\n",
      "\n",
      "Validation epoch 30 completed, last loss: 0.0204, accuracy: 0.4871\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:24:23.064919Z",
     "start_time": "2025-06-18T04:21:50.490512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初版训练任务跑通\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 加载数据\n",
    "data_train = pd.read_csv('../data/COVID-19-Tweet/updated_train.csv')\n",
    "data_test = pd.read_csv('../data/COVID-19-Tweet/updated_test.csv')\n",
    "submission_example = pd.read_csv('../data/COVID-19-Tweet/updated_ss.csv')\n",
    "\n",
    "# 检查数据分布\n",
    "print(\"训练数据标签分布:\")\n",
    "print(data_train['target'].value_counts())\n",
    "print(f\"数据不平衡比例: {data_train['target'].value_counts()[0] / data_train['target'].value_counts()[1]:.2f}\")\n",
    "\n",
    "# 数据预处理 - 清理文本\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # 移除特殊字符但保留基本标点\n",
    "    import re\n",
    "    text = re.sub(r'[^\\w\\s@#.,!?-]', ' ', text)\n",
    "    # 移除多余空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "data_train['text'] = data_train['text'].apply(clean_text)\n",
    "\n",
    "# 分层采样确保训练集和验证集的标签分布一致\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    data_train['text'], data_train['target'],\n",
    "    test_size=0.2, shuffle=True, random_state=42, stratify=data_train['target']\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {len(X_train)}, 验证集大小: {len(X_valid)}\")\n",
    "print(f\"训练集标签分布: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"验证集标签分布: {y_valid.value_counts().to_dict()}\")\n",
    "\n",
    "# 使用更适合这个任务的模型\n",
    "MODEL_NAME = 'bert-base-uncased'  # 改用uncased版本，对社交媒体文本更友好\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 分析文本长度，优化截断长度\n",
    "train_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in X_train[:1000]]\n",
    "print(f\"平均token长度: {np.mean(train_lengths):.2f}\")\n",
    "print(f\"95%分位数token长度: {np.percentile(train_lengths, 95):.0f}\")\n",
    "\n",
    "MAX_LENGTH = min(128, int(np.percentile(train_lengths, 95)))  # 使用更合适的长度\n",
    "print(f\"使用最大长度: {MAX_LENGTH}\")\n",
    "\n",
    "# 改进的tokenization\n",
    "train_tokens = tokenizer(\n",
    "    list(X_train),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "valid_tokens = tokenizer(\n",
    "    list(X_valid),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, tokens, labels):\n",
    "        self.texts = texts\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.tokens['input_ids'][idx],\n",
    "            'attention_mask': self.tokens['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = TweetDataset(X_train, train_tokens, y_train)\n",
    "valid_dataset = TweetDataset(X_valid, valid_tokens, y_valid)\n",
    "\n",
    "# 调整批次大小\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 使用更好的模型配置\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 2\n",
    "config.hidden_dropout_prob = 0.3  # 增加dropout防止过拟合\n",
    "config.attention_probs_dropout_prob = 0.3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(DEVICE)\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "\n",
    "# 处理类别不平衡 - 计算类别权重\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (2 * class_counts)\n",
    "class_weights = torch.FloatTensor(class_weights.values).to(DEVICE)\n",
    "print(f\"类别权重: {class_weights}\")\n",
    "\n",
    "# 改进的优化器和损失函数\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 学习率调度器\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "\n",
    "# 早停机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "# 训练函数\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct_predictions += (predictions == batch['labels']).sum().item()\n",
    "        total_predictions += batch['labels'].size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 验证函数\n",
    "def validate_epoch(model, valid_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(valid_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n",
    "\n",
    "# 训练循环\n",
    "NUM_EPOCHS = 10\n",
    "best_val_accuracy = 0\n",
    "\n",
    "print(\"开始训练...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 训练\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "\n",
    "    # 验证\n",
    "    val_loss, val_acc, val_predictions, val_labels = validate_epoch(model, valid_loader, loss_fn, DEVICE)\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'训练损失: {train_loss:.4f}, 训练准确率: {train_acc:.4f}')\n",
    "    print(f'验证损失: {val_loss:.4f}, 验证准确率: {val_acc:.4f}')\n",
    "    print(f'当前学习率: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'保存最佳模型，验证准确率: {val_acc:.4f}')\n",
    "\n",
    "    # 早停检查\n",
    "    early_stopping(val_acc)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"早停触发!\")\n",
    "        break\n",
    "\n",
    "# 最终评估\n",
    "print(f'\\n最佳验证准确率: {best_val_accuracy:.4f}')\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(val_labels, val_predictions, target_names=['Non-COVID', 'COVID']))"
   ],
   "id": "ff975434d16ffa47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据标签分布:\n",
      "target\n",
      "0    2746\n",
      "1    2541\n",
      "Name: count, dtype: int64\n",
      "数据不平衡比例: 1.08\n",
      "训练集大小: 4229, 验证集大小: 1058\n",
      "训练集标签分布: {0: 2196, 1: 2033}\n",
      "验证集标签分布: {0: 550, 1: 508}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33627599921c44f1bc18f49a7dc155ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52cc8803d9ea427ab6ed000e20c427df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bb6be48960d47f3aef6310041e2dff3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77a1370d8c8a4d2f95cda5e2a0afd995"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均token长度: 24.88\n",
      "95%分位数token长度: 48\n",
      "使用最大长度: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfba9f7463534cd6bb263bbe3ba1a2f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "类别权重: tensor([0.9629, 1.0401], device='cuda:0')\n",
      "开始训练...\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:15<00:00, 17.66it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 79.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.4195, 训练准确率: 0.8078\n",
      "验证损失: 0.2813, 验证准确率: 0.8932\n",
      "当前学习率: 1.95e-05\n",
      "保存最佳模型，验证准确率: 0.8932\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:15<00:00, 17.64it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 73.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.2494, 训练准确率: 0.9092\n",
      "验证损失: 0.2607, 验证准确率: 0.9074\n",
      "当前学习率: 1.82e-05\n",
      "保存最佳模型，验证准确率: 0.9074\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:15<00:00, 17.23it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 76.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.1912, 训练准确率: 0.9371\n",
      "验证损失: 0.3052, 验证准确率: 0.9093\n",
      "当前学习率: 1.61e-05\n",
      "保存最佳模型，验证准确率: 0.9093\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:15<00:00, 17.53it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 74.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.1585, 训练准确率: 0.9492\n",
      "验证损失: 0.3214, 验证准确率: 0.9187\n",
      "当前学习率: 1.34e-05\n",
      "保存最佳模型，验证准确率: 0.9187\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:15<00:00, 17.53it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 77.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.1272, 训练准确率: 0.9648\n",
      "验证损失: 0.3650, 验证准确率: 0.9121\n",
      "当前学习率: 1.05e-05\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:14<00:00, 17.88it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 78.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.1140, 训练准确率: 0.9704\n",
      "验证损失: 0.3908, 验证准确率: 0.9083\n",
      "当前学习率: 7.56e-06\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 265/265 [00:14<00:00, 18.05it/s]\n",
      "Validation: 100%|██████████| 67/67 [00:00<00:00, 80.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练损失: 0.0751, 训练准确率: 0.9823\n",
      "验证损失: 0.4030, 验证准确率: 0.9159\n",
      "当前学习率: 4.92e-06\n",
      "早停触发!\n",
      "\n",
      "最佳验证准确率: 0.9187\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-COVID       0.91      0.93      0.92       550\n",
      "       COVID       0.92      0.90      0.91       508\n",
      "\n",
      "    accuracy                           0.92      1058\n",
      "   macro avg       0.92      0.92      0.92      1058\n",
      "weighted avg       0.92      0.92      0.92      1058\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cea3cfac3e3423fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
