{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T08:40:36.269213Z",
     "start_time": "2025-07-07T08:40:36.264273Z"
    }
   },
   "source": [
    "# https://github.com/rasbt/dora-from-scratch\n",
    "# https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch\n",
    "\n",
    "from utils import EarlyStopping, train_loop_with_resume, train_epoch, validate_epoch\n",
    "from models import LinearWithLoRA, LoRALayer\n",
    "import time, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.deterministic = True"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:36:57.281993Z",
     "start_time": "2025-07-07T07:36:57.168491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###########################\n",
    "#### Settings\n",
    "###########################\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "###########################\n",
    "#### mnist dataset\n",
    "###########################\n",
    "labelled_set = datasets.MNIST(root='../../data', train=True, transform=transforms.ToTensor(), download=False)\n",
    "test_set = datasets.MNIST(root='../../data', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(TRAIN_RATIO * len(labelled_set))\n",
    "valid_size = len(labelled_set) - train_size\n",
    "train_set, valid_set = random_split(labelled_set, [train_size, valid_size])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 检查数据集张量维度\n",
    "check_batch = next(iter(train_loader))\n",
    "print(f\"批次数据维度: {check_batch[0].shape}, 标签维度: {check_batch[1].shape}\")"
   ],
   "id": "cdb9adebac9fa70f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次数据维度: torch.Size([128, 1, 28, 28]), 标签维度: torch.Size([128])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:37:59.811578Z",
     "start_time": "2025-07-07T07:37:36.835852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#############################\n",
    "#### Hyperparameters\n",
    "#############################\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "LEARNING_RATE = 5e-3\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "#############################\n",
    "#### Architecture\n",
    "#############################\n",
    "n_features = 28 * 28  # MNIST images are 28x28 pixels\n",
    "n_classes = 10\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 64\n",
    "\n",
    "\n",
    "#############################\n",
    "#### Perceptron Model\n",
    "#############################\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_features, hidden1, hidden2, out_features, rank=4, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # 展平输入\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_pretrained = MultiLayerPerceptron(n_features, n_hidden1, n_hidden2, n_classes).to(DEVICE)\n",
    "optimizer_pretrained = optim.Adam(model_pretrained.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer_pretrained, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "early_stopping = EarlyStopping(patience=5, delta=1e-4, mode=\"max\")\n",
    "best_ckpt_file_path = \"best_pretrained_20250707.pth\"\n",
    "# 训练\n",
    "accuracy, discord = train_loop_with_resume(\n",
    "    model_pretrained,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer_pretrained,\n",
    "    scheduler,\n",
    "    early_stopping,\n",
    "    best_ckpt_file_path,\n",
    "    NUM_EPOCHS,\n",
    "    DEVICE,\n",
    ")\n",
    "print(f\"训练过程损失: {discord['train_loss']}\")\n",
    "print(f\"训练过程准确率: {discord['train_acc']}\")\n",
    "print(f\"验证过程损失: {discord['valid_loss']}\")\n",
    "print(f\"验证过程准确率: {discord['valid_acc']}\")"
   ],
   "id": "46fa15aed37fcae1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from best_pretrained_20250707.pth, resume from epoch 6\n",
      "加载训练点模型成功，当前准确率为0.9754，从第6个epoch开始训练...\n",
      "\n",
      "Epoch 7/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9867\n",
      "当前学习率: 4.76e-03\n",
      "更新最佳验证准确率: 0.9867\n",
      "Checkpoint saved to best_pretrained_20250707.pth\n",
      "\n",
      "Epoch 8/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9843\n",
      "当前学习率: 4.69e-03\n",
      "\n",
      "Epoch 9/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9804\n",
      "当前学习率: 4.61e-03\n",
      "\n",
      "Epoch 10/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9799\n",
      "当前学习率: 4.52e-03\n",
      "\n",
      "Epoch 11/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9799\n",
      "当前学习率: 4.43e-03\n",
      "\n",
      "Epoch 12/50 - --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9801\n",
      "早停触发!\n",
      "训练过程损失: [0.04945656222539643, 0.038173826846294105, 0.028851708532776684, 0.02816136488694853, 0.02662047257569308, 0.027006679687959452]\n",
      "训练过程准确率: [0.9852083333333334, 0.9877291666666667, 0.9906666666666667, 0.9913125, 0.9912083333333334, 0.9914791666666667]\n",
      "验证过程损失: [0.044369788820544875, 0.05006299970547358, 0.07011902468154828, 0.07342269016553958, 0.07831427867741635, 0.08914228246423106]\n",
      "验证过程准确率: [0.9866666666666667, 0.9843333333333333, 0.9804166666666667, 0.9799166666666667, 0.9799166666666667, 0.9800833333333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:00:38.538156Z",
     "start_time": "2025-07-07T08:00:38.531458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "print(\"改造前的模型结构:\")\n",
    "print(model_pretrained)\n",
    "lora_rank = 4\n",
    "lora_alpha = 1.5\n",
    "model_lora = copy.deepcopy(model_pretrained)\n",
    "\n",
    "\n",
    "def add_lora_to_linear(net: nn.Module, rank=4, alpha=1.0) -> None:\n",
    "    \"\"\"递归替换模型中的线性层为带LoRA的线性层\"\"\"\n",
    "    for name, module in net.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f\"替换层: {name} -> 带LoRA的线性层\")\n",
    "            lora_layer = LoRALayer(module.in_features, module.out_features, rank, alpha)\n",
    "            setattr(net, name, LinearWithLoRA(module, lora_layer))\n",
    "        elif len(list(module.children())) > 0:\n",
    "            # 如果模块还有子模块，递归替换\n",
    "            add_lora_to_linear(module, rank, alpha)\n",
    "\n",
    "\n",
    "add_lora_to_linear(model_lora, rank=lora_rank, alpha=lora_alpha)\n",
    "print(\"改造后的模型结构:\")\n",
    "print(model_lora)\n",
    "\n",
    "# --- 步骤 3：验证LoRA模型的可训练参数 ---\n",
    "print(\"\\n验证LoRA模型的可训练参数:\")\n",
    "for name, param in model_lora.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"可训练 (Trainable): {name}: 形状: {param.shape}\")\n",
    "    else:\n",
    "        print(f\"已冻结 (Frozen): {name}: 形状: {param.shape}\")"
   ],
   "id": "ccd32688cb07a13c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "改造前的模型结构:\n",
      "MultiLayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "替换层: fc1 -> 带LoRA的线性层\n",
      "替换层: fc2 -> 带LoRA的线性层\n",
      "替换层: fc3 -> 带LoRA的线性层\n",
      "改造后的模型结构:\n",
      "MultiLayerPerceptron(\n",
      "  (fc1): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (fc2): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (fc3): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n",
      "\n",
      "验证LoRA模型的可训练参数:\n",
      "已冻结 (Frozen): fc1.linear.weight: 形状: torch.Size([256, 784])\n",
      "已冻结 (Frozen): fc1.linear.bias: 形状: torch.Size([256])\n",
      "可训练 (Trainable): fc1.lora.A: 形状: torch.Size([784, 4])\n",
      "可训练 (Trainable): fc1.lora.B: 形状: torch.Size([4, 256])\n",
      "已冻结 (Frozen): fc2.linear.weight: 形状: torch.Size([64, 256])\n",
      "已冻结 (Frozen): fc2.linear.bias: 形状: torch.Size([64])\n",
      "可训练 (Trainable): fc2.lora.A: 形状: torch.Size([256, 4])\n",
      "可训练 (Trainable): fc2.lora.B: 形状: torch.Size([4, 64])\n",
      "已冻结 (Frozen): fc3.linear.weight: 形状: torch.Size([10, 64])\n",
      "已冻结 (Frozen): fc3.linear.bias: 形状: torch.Size([10])\n",
      "可训练 (Trainable): fc3.lora.A: 形状: torch.Size([64, 4])\n",
      "可训练 (Trainable): fc3.lora.B: 形状: torch.Size([4, 10])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:57:47.900200Z",
     "start_time": "2025-07-07T08:57:47.894031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 步骤 3：获取并保存LoRA模型参数 ---\n",
    "def get_lora_state_dict_explicit(net):\n",
    "    \"\"\"显式地遍历模块\"\"\"\n",
    "    lora_state_dict = {}\n",
    "    for name, module in net.named_modules():\n",
    "        # 我们要找的是 LoRALayer, 而不是 LinearWithLoRA\n",
    "        if isinstance(module, LoRALayer):\n",
    "            # 'name' 已经是正确的层级名称，例如 'fc1.lora'\n",
    "            lora_state_dict[name + \".A\"] = module.A\n",
    "            lora_state_dict[name + \".B\"] = module.B\n",
    "    return lora_state_dict\n",
    "\n",
    "\n",
    "def get_lora_state_dict(net):\n",
    "    return {k: v for k, v in net.named_parameters() if 'lora' in k and v.requires_grad}\n",
    "\n",
    "\n",
    "def save_lora_checkpoint(lora_model, optimizer, scheduler, epoch, best_acc, ckpt_path):\n",
    "    \"\"\"保存LoRA模型的checkpoint，注意，不保存完整的 model.state_dict()\"\"\"\n",
    "    lora_state_dict = get_lora_state_dict(lora_model)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'best_acc': best_acc,\n",
    "        'lora_state_dict': lora_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, ckpt_path)\n",
    "    print(f\"LoRA模型参数已保存到 {ckpt_path}\")\n",
    "\n",
    "\n",
    "def load_lora_checkpoint(ckpt_path, lora_model, optimizer, scheduler=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    加载LoRA检查点以恢复训练或进行推理\n",
    "\n",
    "    Args:\n",
    "        ckpt_path (str): 检查点文件路径。\n",
    "        lora_model (nn.Module): LoRA模型实例 (预训练模型 + LoRA结构)。\n",
    "        optimizer (torch.optim.Optimizer, optional): 优化器实例。\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler, optional): 调度器实例。\n",
    "        device (str, optional): 设备类型，默认为 \"cpu\"。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (start_epoch, best_acc) 重开的轮次和最佳准确率。\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    lora_model.load_state_dict(checkpoint['lora_state_dict'], strict=False)  # strict=False 是必须的，因为只加载模型参数的一个子集\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    print(f\"LoRA模型参数已从 {ckpt_path} 加载\")\n",
    "    return start_epoch, best_acc\n",
    "\n",
    "\n",
    "dict1 = get_lora_state_dict_explicit(model_lora)\n",
    "\n",
    "dict2 = get_lora_state_dict_by_grad(model_lora)\n",
    "\n",
    "print(dict2 == dict1)  # 应该为 True"
   ],
   "id": "3c874227c2b53364",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:59:12.535243Z",
     "start_time": "2025-07-07T08:58:11.121174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 步骤 4：训练LoRA模型 ---\n",
    "def merge_all_lora_weights(net):\n",
    "    \"\"\"递归合并所有LoRA层的权重\"\"\"\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.merge_weights()\n",
    "\n",
    "\n",
    "def unmerge_all_lora_weights(net):\n",
    "    \"\"\"递归取消合并所有LoRA层的权重\"\"\"\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.unmerge_weights()\n",
    "\n",
    "\n",
    "model_lora.to(DEVICE)\n",
    "lora_params_path = \"best_lora_parameters.pth\"\n",
    "optimizer_lora = optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer_pretrained, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "early_stopping = EarlyStopping(patience=7, delta=1e-4, mode=\"max\")\n",
    "if os.path.exists(lora_params_path):\n",
    "    start_epoch, best_acc = load_lora_checkpoint(lora_params_path, model_lora, optimizer_lora, scheduler, DEVICE)\n",
    "    print(f\"加载训练点LoRA参数成功，当前准确率为{best_acc:.4f}，从第{start_epoch}个epoch开始训练...\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_acc = 0.0\n",
    "    print(\"未找到LoRA参数文件，开始从头训练LoRA层...\")\n",
    "\n",
    "unmerge_all_lora_weights(model_lora)  # 确保在训练前权重是 *未合并* 的状态\n",
    "model_lora.train()  # 设置模型为训练模式\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    train_loss, train_acc = train_epoch(model_lora, train_loader, optimizer_lora, criterion, DEVICE)\n",
    "    valid_loss, valid_acc = validate_epoch(model_lora, valid_loader, criterion, DEVICE)\n",
    "    print(f\"当前验证准确率: {valid_acc:.4f}\")\n",
    "\n",
    "    early_stopping(valid_acc)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"验证准确率未提升，提前停止训练\")\n",
    "        break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"当前学习率: {scheduler.get_last_lr()[0]:.3e}\")\n",
    "\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        save_lora_checkpoint(model_lora, optimizer_lora, scheduler, epoch, best_acc, lora_params_path)\n",
    "        print(f\"保存新的最佳LoRA参数，当前最佳验证准确率: {best_acc:.4f}\")"
   ],
   "id": "6726f179ca59fb6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA模型参数已从 best_lora_parameters.pth 加载\n",
      "加载训练点LoRA参数成功，当前准确率为0.9882，从第2个epoch开始训练...\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9870\n",
      "当前学习率: 4.187e-03\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9872\n",
      "当前学习率: 4.149e-03\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9873\n",
      "当前学习率: 4.104e-03\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9864\n",
      "当前学习率: 4.051e-03\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9888\n",
      "当前学习率: 3.990e-03\n",
      "LoRA模型参数已保存到 best_lora_parameters.pth\n",
      "保存新的最佳LoRA参数，当前最佳验证准确率: 0.9888\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9884\n",
      "当前学习率: 3.922e-03\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9867\n",
      "当前学习率: 3.847e-03\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9892\n",
      "当前学习率: 3.766e-03\n",
      "LoRA模型参数已保存到 best_lora_parameters.pth\n",
      "保存新的最佳LoRA参数，当前最佳验证准确率: 0.9892\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9872\n",
      "当前学习率: 3.677e-03\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9879\n",
      "当前学习率: 3.583e-03\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9878\n",
      "当前学习率: 3.483e-03\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9860\n",
      "当前学习率: 3.377e-03\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9878\n",
      "当前学习率: 3.267e-03\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9883\n",
      "当前学习率: 3.152e-03\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前验证准确率: 0.9884\n",
      "验证准确率未提升，提前停止训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于输入向量矩阵 $X \\in \\mathbb{R}^{T \\times d}$（$T$ 为序列长度，$d$ 为维度）：\n",
    "1. 计算 Query、Key、Value：\n",
    "2. 计算注意力分数：\n",
    "结果仍然是 $(T, d)$，但每个 token 的输出向量都变成了融合其他 token 信息后的新向量。"
   ],
   "id": "ba2a4e5482c6f28a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T06:54:25.530013Z",
     "start_time": "2025-07-08T06:54:25.517914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 定义我们自己的、结构清晰的 CustomMHA\n",
    "# =============================================================================\n",
    "class CustomMHA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = attn_weights @ v\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 验证脚本主流程\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 参数设置 ---\n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # --- 实例化两个模块 ---\n",
    "    # 使用 batch_first=True 以匹配现代的常用做法\n",
    "    torch_mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "    custom_mha = CustomMHA(embed_dim, num_heads)\n",
    "\n",
    "    print(\"--- 步骤1: 实例化两个MHA模块 ---\")\n",
    "    print(\"PyTorch MHA 实例已创建。\")\n",
    "    print(\"Custom MHA 实例已创建。\")\n",
    "\n",
    "    # --- 关键：权重迁移 ---\n",
    "    # 将 PyTorch MHA 的权重复制到我们的 CustomMHA 中\n",
    "    with torch.no_grad():\n",
    "        # PyTorch 将 Q, K, V 的权重合并在一个 `in_proj_weight` 中，我们需要拆分它\n",
    "        # 形状: (3 * embed_dim, embed_dim) -> 分成三块 (embed_dim, embed_dim)\n",
    "        custom_mha.q_proj.weight.data = torch_mha.in_proj_weight.data[:embed_dim, :]\n",
    "        custom_mha.k_proj.weight.data = torch_mha.in_proj_weight.data[embed_dim:2*embed_dim, :]\n",
    "        custom_mha.v_proj.weight.data = torch_mha.in_proj_weight.data[2*embed_dim:, :]\n",
    "\n",
    "        # 同样地处理偏置项 (bias)\n",
    "        custom_mha.q_proj.bias.data = torch_mha.in_proj_bias.data[:embed_dim]\n",
    "        custom_mha.k_proj.bias.data = torch_mha.in_proj_bias.data[embed_dim:2*embed_dim]\n",
    "        custom_mha.v_proj.bias.data = torch_mha.in_proj_bias.data[2*embed_dim:]\n",
    "\n",
    "        # 输出投影层的权重是分开的，可以直接复制\n",
    "        custom_mha.out_proj.weight.data = torch_mha.out_proj.weight.data\n",
    "        custom_mha.out_proj.bias.data = torch_mha.out_proj.bias.data\n",
    "\n",
    "    print(\"\\n--- 步骤2: 权重迁移 ---\")\n",
    "    print(\"已将 torch_mha 的权重成功复制到 custom_mha。\")\n",
    "\n",
    "    # --- 准备输入数据 ---\n",
    "    input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # --- 执行并比较输出 ---\n",
    "    # 设置为评估模式以确保行为确定性 (例如，关闭dropout)\n",
    "    torch_mha.eval()\n",
    "    custom_mha.eval()\n",
    "\n",
    "    # 获取输出\n",
    "    # nn.MultiheadAttention 的 forward 需要 query, key, value\n",
    "    # 在自注意力中，它们是同一个张量\n",
    "    # 它返回一个元组 (attn_output, attn_output_weights)\n",
    "    output_torch, _ = torch_mha(input_tensor, input_tensor, input_tensor)\n",
    "    output_custom = custom_mha(input_tensor)\n",
    "\n",
    "    print(\"\\n--- 步骤3: 执行前向传播 ---\")\n",
    "    print(f\"输入张量形状: {input_tensor.shape}\")\n",
    "    print(f\"torch_mha 输出形状: {output_torch.shape}\")\n",
    "    print(f\"custom_mha 输出形状: {output_custom.shape}\")\n",
    "\n",
    "    # --- 最终验证 ---\n",
    "    # 使用 torch.allclose 来比较两个张量是否在数值上非常接近\n",
    "    are_outputs_equal = torch.allclose(output_torch, output_custom)\n",
    "\n",
    "    print(\"\\n--- 步骤4: 最终验证 ---\")\n",
    "    print(f\"两个模块的输出是否一致? -> {are_outputs_equal}\")\n",
    "\n",
    "    if are_outputs_equal:\n",
    "        print(\"\\n[结论] 验证成功！✅ CustomMHA 完美复刻了 nn.MultiheadAttention 的功能。\")\n",
    "    else:\n",
    "        print(\"\\n[结论] 验证失败！❌ 输出不一致，请检查权重迁移逻辑。\")\n",
    "        # 打印差值的绝对和，以供调试\n",
    "        print(f\"差值总和: {torch.sum(torch.abs(output_torch - output_custom))}\")\n"
   ],
   "id": "d651d80ca6fdaa5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤1: 实例化两个MHA模块 ---\n",
      "PyTorch MHA 实例已创建。\n",
      "Custom MHA 实例已创建。\n",
      "\n",
      "--- 步骤2: 权重迁移 ---\n",
      "已将 torch_mha 的权重成功复制到 custom_mha。\n",
      "\n",
      "--- 步骤3: 执行前向传播 ---\n",
      "输入张量形状: torch.Size([4, 10, 128])\n",
      "torch_mha 输出形状: torch.Size([4, 10, 128])\n",
      "custom_mha 输出形状: torch.Size([4, 10, 128])\n",
      "\n",
      "--- 步骤4: 最终验证 ---\n",
      "两个模块的输出是否一致? -> True\n",
      "\n",
      "[结论] 验证成功！✅ CustomMHA 完美复刻了 nn.MultiheadAttention 的功能。\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T08:14:54.988295Z",
     "start_time": "2025-07-08T08:14:54.975296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, seq_len, d_model = 300, 50, 128\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "h = 8  # 注意力头数\n",
    "d_k = d_model // h  # 每个头的维度\n",
    "d_v = d_model // h\n",
    "q_proj = nn.Linear(d_model, d_k)\n",
    "k_proj = nn.Linear(d_model, d_k)\n",
    "v_proj = nn.Linear(d_model, d_v)\n",
    "Q = q_proj(X)\n",
    "print(Q.shape)  # (batch_size, seq_len, d_k)"
   ],
   "id": "632f582245cff211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 50, 16])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        d_k = d_model // 1  # 1个注意力头\n",
    "        d_v = d_model // 1\n",
    "        self.q_proj = nn.Linear(d_model, d_k)\n",
    "        self.k_proj = nn.Linear(d_model, d_k)\n",
    "        self.v_proj = nn.Linear(d_model, d_v)\n",
    "        self.out_proj = nn.Linear(d_v, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q_proj(x) # (batch_size, seq_len, d_k)\n",
    "        K = self.k_proj(x) # (batch_size, seq_len, d_k)\n",
    "        V = self.v_proj(x) # (batch_size, seq_len, d_v)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)  # (batch_size, seq_len, seq_len)"
   ],
   "id": "4ed5b9a84e986748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T09:49:02.575884Z",
     "start_time": "2025-07-08T09:49:02.540818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(10, 20, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "model = TestModel()\n",
    "model._modules"
   ],
   "id": "dafba52b318b8bb4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': Sequential(\n",
       "   (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=20, out_features=10, bias=True)\n",
       " ),\n",
       " 'fc': Linear(in_features=10, out_features=1, bias=True)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T10:13:02.108880Z",
     "start_time": "2025-07-08T10:13:02.104883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"层名称: {name}, 模块: {module}\")"
   ],
   "id": "2ee36f2d33496e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "层名称: , 模块: TestModel(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "层名称: layer1, 模块: Sequential(\n",
      "  (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n",
      "层名称: layer1.0, 模块: Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "层名称: layer1.1, 模块: ReLU()\n",
      "层名称: layer1.2, 模块: Linear(in_features=20, out_features=10, bias=True)\n",
      "层名称: fc, 模块: Linear(in_features=10, out_features=1, bias=True)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e470997a1c20952"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
