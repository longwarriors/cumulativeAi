{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 特征工程怎么做\n",
    "# https://gemini.google.com/app/f091005ad8d4a8e7\n",
    "# https://zhuanlan.zhihu.com/p/1929223292528620363\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import KFold\n",
    "# 目标编码5折交叉验证防泄露\n",
    "enc = ce.TargetEncoder(cols=['city'])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for tr_idx, val_idx in kf.split(df):\n",
    "    enc.fit(df.loc[tr_idx, 'city'], df.loc[tr_idx, 'churn'])\n",
    "    df.loc[val_idx, 'city_te'] = enc.transform(df.loc[val_idx, 'city'])"
   ],
   "id": "d0e7dc50e5f25e53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T01:59:24.997345Z",
     "start_time": "2025-07-18T01:59:24.779983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "print(dir(gensim.models))\n",
    "# TF-IDF 向量化\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['comment'])\n",
    "\n",
    "# Word2Vec\n",
    "sentences = [comment.split() for comment in df['comment']]\n",
    "w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=3)\n",
    "df['vector'] = df['comment'].apply(lambda x: w2v.wv.get_vector(x.split()[0]) if x.split() else np.zeros(100))"
   ],
   "id": "98b233da97823b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AtireBM25Model', 'AuthorTopicModel', 'BackMappingTranslationMatrix', 'CoherenceModel', 'Doc2Vec', 'EnsembleLda', 'FAST_VERSION', 'FastText', 'HdpModel', 'KeyedVectors', 'LdaModel', 'LdaMulticore', 'LdaSeqModel', 'LogEntropyModel', 'LsiModel', 'LuceneBM25Model', 'Nmf', 'NormModel', 'OkapiBM25Model', 'Phrases', 'RpModel', 'TfidfModel', 'TranslationMatrix', 'VocabTransform', 'Word2Vec', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_fasttext_bin', 'atmodel', 'basemodel', 'bm25model', 'callbacks', 'coherencemodel', 'doc2vec', 'doc2vec_corpusfile', 'doc2vec_inner', 'ensemblelda', 'fasttext', 'fasttext_corpusfile', 'fasttext_inner', 'hdpmodel', 'interfaces', 'keyedvectors', 'ldamodel', 'ldamulticore', 'ldaseqmodel', 'logentropy_model', 'lsimodel', 'nmf', 'nmf_pgd', 'normmodel', 'phrases', 'rpmodel', 'tfidfmodel', 'translation_matrix', 'utils', 'word2vec', 'word2vec_corpusfile', 'word2vec_inner']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LassoCV\n",
    "# filter\n",
    "X_new = SelectKBest(mutual_info_classif, k=10).fit_transform(X, y)\n",
    "#Embedded\n",
    "model = LassoCV(cv=5)\n",
    "model.fit(X, y)\n",
    "keep = X.columns[model.coef_ != 0]"
   ],
   "id": "5c7c85f8790e3ef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 自动特征工程\n",
    "import featuretools as ft\n",
    "es = ft.EntitySet(id='customer_data')\n",
    "es.add_dataframe(\"customers\", df, index=\"customer_id\")\n",
    "feat_matrix, feat_defs = ft.dfs(entityset=es, target_dataframe_name=\"customers\", max_depth=2)\n"
   ],
   "id": "98733c1c825a0e28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:09:22.593110Z",
     "start_time": "2025-07-17T10:09:22.588532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "original_idx = np.arange(10)\n",
    "print(f\"原始数据索引: {original_idx}\\n\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(f\"使用 KFold 进行 5 折交叉验证:\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(original_idx)):\n",
    "    print(f\"--- 第 {fold + 1} 折 ---\")\n",
    "    print(f\"  训练集索引: {train_idx}\")\n",
    "    print(f\"  验证集索引: {val_idx}\")"
   ],
   "id": "8db7b2694a5277e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据索引: [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "使用 KFold 进行 5 折交叉验证:\n",
      "--- 第 1 折 ---\n",
      "  训练集索引: [0 2 3 4 5 6 7 9]\n",
      "  验证集索引: [1 8]\n",
      "--- 第 2 折 ---\n",
      "  训练集索引: [1 2 3 4 6 7 8 9]\n",
      "  验证集索引: [0 5]\n",
      "--- 第 3 折 ---\n",
      "  训练集索引: [0 1 3 4 5 6 8 9]\n",
      "  验证集索引: [2 7]\n",
      "--- 第 4 折 ---\n",
      "  训练集索引: [0 1 2 3 5 6 7 8]\n",
      "  验证集索引: [4 9]\n",
      "--- 第 5 折 ---\n",
      "  训练集索引: [0 1 2 4 5 7 8 9]\n",
      "  验证集索引: [3 6]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://gemini.google.com/app/3f22d0a18dda3fea\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"负责数据预处理的类，利用 df.pipe() 提升代码可读性和流程清晰度。\"\"\"\n",
    "\n",
    "    def __init__(self, id_cols=None, target_col=None):\n",
    "        \"\"\"\n",
    "        初始化预处理器。\n",
    "\n",
    "        参数:\n",
    "            id_cols (list): 需要从特征处理中排除的ID列名列表。\n",
    "            target_col (str): 目标列名，也需要从特征处理中排除。\n",
    "        \"\"\"\n",
    "        self.num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        self.cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\n",
    "        # Store encoders for categorical features that need fitting (OrdinalEncoder/OneHotEncoder)\n",
    "        self.categorical_transformers = {} # Stores {'col_name': fitted_encoder_pipeline}\n",
    "        self.standard_scaler = StandardScaler()\n",
    "\n",
    "        # Define columns to exclude from general feature processing\n",
    "        self.exclude_cols = set(id_cols if id_cols is not None else [\"SK_ID_CURR\", \"SK_ID_BUREAU\", \"SK_ID_PREV\"])\n",
    "        if target_col:\n",
    "            self.exclude_cols.add(target_col)\n",
    "        self.target_col = target_col\n",
    "\n",
    "\n",
    "    def _handle_infinities(self, df):\n",
    "        \"\"\"将DataFrame中的无穷大值转换为NaN，以便缺失值填充器处理。\"\"\"\n",
    "        logger.info(\"处理无穷大值...\")\n",
    "        return df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    def _impute_numerical(self, df, is_train):\n",
    "        \"\"\"处理数值特征的缺失值。\"\"\"\n",
    "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        # Filter out exclude_cols if they were not removed earlier\n",
    "        num_cols = [col for col in num_cols if col not in self.exclude_cols]\n",
    "\n",
    "        if not num_cols:\n",
    "            logger.info(\"没有数值特征需要填充。\")\n",
    "            return df\n",
    "\n",
    "        logger.info(f\"处理数值特征缺失值: {len(num_cols)}列...\")\n",
    "        if is_train:\n",
    "            df[num_cols] = self.num_imputer.fit_transform(df[num_cols])\n",
    "        else:\n",
    "            # Handle potential missing columns in test set that were in train set\n",
    "            existing_num_cols = [col for col in num_cols if col in df.columns]\n",
    "            if existing_num_cols:\n",
    "                df[existing_num_cols] = self.num_imputer.transform(df[existing_num_cols])\n",
    "            # If some num_cols from train are missing in test, we just skip them.\n",
    "            # Imputer already fitted on train, so it won't complain about new columns.\n",
    "        return df\n",
    "\n",
    "    def _impute_categorical(self, df, is_train):\n",
    "        \"\"\"处理分类特征的缺失值。\"\"\"\n",
    "        cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "        # Filter out exclude_cols if they were not removed earlier\n",
    "        cat_cols = [col for col in cat_cols if col not in self.exclude_cols]\n",
    "\n",
    "        if not cat_cols:\n",
    "            logger.info(\"没有分类特征需要填充。\")\n",
    "            return df\n",
    "\n",
    "        logger.info(f\"处理分类特征缺失值: {len(cat_cols)}列...\")\n",
    "        if is_train:\n",
    "            df[cat_cols] = self.cat_imputer.fit_transform(df[cat_cols])\n",
    "        else:\n",
    "            # Similar to numerical, handle potentially missing columns in test set\n",
    "            existing_cat_cols = [col for col in cat_cols if col in df.columns]\n",
    "            if existing_cat_cols:\n",
    "                df[existing_cat_cols] = self.cat_imputer.transform(df[existing_cat_cols])\n",
    "        return df\n",
    "\n",
    "    def _encode_categorical(self, df, is_train):\n",
    "        \"\"\"编码分类变量，使用 OneHotEncoder 或 OrdinalEncoder。\"\"\"\n",
    "        logger.info(\"编码分类变量...\")\n",
    "\n",
    "        df_encoded = df.copy() # Make a copy to avoid modifying original df during loop/concat\n",
    "        processed_cat_cols = [] # Keep track of cols that will be replaced\n",
    "\n",
    "        cat_cols = df_encoded.select_dtypes(include='object').columns.tolist()\n",
    "        cat_cols = [col for col in cat_cols if col not in self.exclude_cols] # Ensure we don't encode excluded cols\n",
    "\n",
    "        if not cat_cols:\n",
    "            logger.info(\"没有分类特征需要编码。\")\n",
    "            return df_encoded\n",
    "\n",
    "        for col in cat_cols:\n",
    "            # Handle columns that might not exist in test set if they were only in train\n",
    "            if col not in df_encoded.columns:\n",
    "                logger.warning(f\"列 '{col}' 在当前数据中不存在，跳过编码。\")\n",
    "                continue\n",
    "\n",
    "            if is_train:\n",
    "                if df_encoded[col].nunique() <= 2:\n",
    "                    # Binary/Low-cardinality: OrdinalEncoder\n",
    "                    encoder_pipeline = Pipeline(steps=[\n",
    "                        ('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "                    ])\n",
    "                    df_encoded[col] = encoder_pipeline.fit_transform(df_encoded[[col]])\n",
    "                else:\n",
    "                    # High-cardinality: OneHotEncoder\n",
    "                    encoder_pipeline = Pipeline(steps=[\n",
    "                        ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "                    ])\n",
    "                    # OneHotEncoder returns a numpy array, need to convert back to DataFrame\n",
    "                    encoded_array = encoder_pipeline.fit_transform(df_encoded[[col]])\n",
    "                    # Get feature names for new columns\n",
    "                    feature_names = encoder_pipeline.named_steps['onehot_encoder'].get_feature_names_out([col])\n",
    "                    temp_df = pd.DataFrame(encoded_array, columns=feature_names, index=df_encoded.index)\n",
    "                    df_encoded = pd.concat([df_encoded.drop(columns=[col]), temp_df], axis=1)\n",
    "                self.categorical_transformers[col] = encoder_pipeline # Store the fitted encoder\n",
    "\n",
    "            else: # is_test\n",
    "                if col in self.categorical_transformers:\n",
    "                    encoder_pipeline = self.categorical_transformers[col]\n",
    "                    if isinstance(encoder_pipeline.named_steps['ordinal_encoder'], OrdinalEncoder):\n",
    "                        # OrdinalEncoder\n",
    "                        try:\n",
    "                            df_encoded[col] = encoder_pipeline.transform(df_encoded[[col]])\n",
    "                        except ValueError as e:\n",
    "                            # Handle cases where `unknown_value` might be a class if not handled by handle_unknown='use_encoded_value'\n",
    "                            # For OrdinalEncoder, handle_unknown='use_encoded_value' should prevent errors for new categories\n",
    "                            logger.error(f\"OrdinalEncoder转换列 '{col}' 时出错: {e}. 确保handle_unknown正确设置.\")\n",
    "                            df_encoded[col] = -1 # Fallback to unknown value if error occurs\n",
    "                    else: # OneHotEncoder\n",
    "                        try:\n",
    "                            encoded_array = encoder_pipeline.transform(df_encoded[[col]])\n",
    "                            feature_names = encoder_pipeline.named_steps['onehot_encoder'].get_feature_names_out([col])\n",
    "                            temp_df = pd.DataFrame(encoded_array, columns=feature_names, index=df_encoded.index)\n",
    "                            df_encoded = pd.concat([df_encoded.drop(columns=[col]), temp_df], axis=1)\n",
    "                        except ValueError as e:\n",
    "                            logger.error(f\"OneHotEncoder转换列 '{col}' 时出错: {e}. 确保handle_unknown='ignore'正确设置.\")\n",
    "                            # If OneHotEncoder fails (e.g., due to unexpected issues),\n",
    "                            # we might end up with missing one-hot columns.\n",
    "                            # Best practice is to ensure handle_unknown='ignore'\n",
    "                            # If training columns are missing in test after one-hot, fill with 0.\n",
    "                            pass # handle_unknown='ignore' should prevent this error for new categories\n",
    "                else:\n",
    "                    logger.warning(f\"列 '{col}' 在训练集中未被拟合，测试集跳过编码。\")\n",
    "        return df_encoded\n",
    "\n",
    "\n",
    "    def _scale_numerical(self, df, is_train):\n",
    "        \"\"\"对数值特征进行标准化。\"\"\"\n",
    "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        num_cols = [col for col in num_cols if col not in self.exclude_cols] # Ensure we don't scale excluded cols\n",
    "\n",
    "        if not num_cols:\n",
    "            logger.info(\"没有数值特征需要标准化。\")\n",
    "            return df\n",
    "\n",
    "        logger.info(f\"标准化数值特征: {len(num_cols)}列...\")\n",
    "        if is_train:\n",
    "            df[num_cols] = self.standard_scaler.fit_transform(df[num_cols])\n",
    "        else:\n",
    "            existing_num_cols = [col for col in num_cols if col in df.columns]\n",
    "            if existing_num_cols:\n",
    "                df[existing_num_cols] = self.standard_scaler.transform(df[existing_num_cols])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def preprocess(self, df, is_train=True):\n",
    "        \"\"\"\n",
    "        预处理数据框。\n",
    "\n",
    "        参数:\n",
    "            df (DataFrame): 要预处理的数据框。\n",
    "            is_train (bool): 是否为训练数据。\n",
    "\n",
    "        返回:\n",
    "            DataFrame: 预处理后的数据框。\n",
    "        \"\"\"\n",
    "        logger.info(f\"开始预处理{'训练' if is_train else '测试'}数据\")\n",
    "\n",
    "        # 将ID列和TARGET列暂时保存，并在所有转换完成后重新合并\n",
    "        preserved_cols_data = df[list(self.exclude_cols.intersection(df.columns))].copy()\n",
    "        df_working = df.drop(columns=list(self.exclude_cols.intersection(df.columns)), errors='ignore').copy()\n",
    "\n",
    "        # 使用 pipe() 链式调用预处理步骤\n",
    "        processed_df = (df_working\n",
    "                        .pipe(self._handle_infinities)\n",
    "                        .pipe(self._impute_numerical, is_train=is_train)\n",
    "                        .pipe(self._impute_categorical, is_train=is_train)\n",
    "                        .pipe(self._encode_categorical, is_train=is_train) # Note: _encode_categorical needs to be careful with column changes\n",
    "                        .pipe(self._scale_numerical, is_train=is_train)\n",
    "                       )\n",
    "\n",
    "        # 重新合并保留的列\n",
    "        final_df = pd.concat([processed_df, preserved_cols_data], axis=1)\n",
    "\n",
    "        logger.info(\"预处理完成。\")\n",
    "        return final_df\n",
    "\n",
    "# --- 示例使用 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建一些示例数据\n",
    "    data_train = {\n",
    "        'SK_ID_CURR': [1, 2, 3, 4, 5, 6],\n",
    "        'Feature_Num1': [10.0, 20.0, np.nan, 40.0, 50.0, 60.0],\n",
    "        'Feature_Num2': [1.0, np.inf, 3.0, 4.0, -np.inf, 6.0],\n",
    "        'Feature_Cat1': ['A', 'B', 'A', 'C', 'B', 'A'], # High-cardinality categorical\n",
    "        'Feature_Cat2': ['Yes', 'No', 'Yes', 'No', np.nan, 'Yes'], # Binary categorical\n",
    "        'Feature_Cat3': ['X', 'Y', 'X', 'Z', 'Y', 'X'], # Another high-cardinality\n",
    "        'Some_Other_Col': ['val1', 'val2', 'val3', 'val4', 'val5', 'val6'], # Untouched feature type\n",
    "        'TARGET': [0, 1, 0, 1, 0, 1]\n",
    "    }\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "\n",
    "    data_test = {\n",
    "        'SK_ID_CURR': [7, 8, 9, 10],\n",
    "        'Feature_Num1': [70.0, np.nan, 90.0, 100.0],\n",
    "        'Feature_Num2': [7.0, 8.0, np.inf, 10.0],\n",
    "        'Feature_Cat1': ['B', 'D', 'C', 'A'], # 'D' is new category\n",
    "        'Feature_Cat2': ['No', 'Yes', 'No', 'Yes'],\n",
    "        'Feature_Cat3': ['Y', 'A', 'Z', 'Y'], # 'A' is new category\n",
    "        'Some_Other_Col': ['val7', 'val8', 'val9', 'val10'],\n",
    "        'TARGET': [0, 0, 1, 1]\n",
    "    }\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = Preprocessor(target_col='TARGET', id_cols=['SK_ID_CURR'])\n",
    "\n",
    "    # Train data preprocessing\n",
    "    print(\"\\n--- 训练数据预处理 ---\")\n",
    "    df_train_processed = preprocessor.preprocess(df_train, is_train=True)\n",
    "    print(df_train_processed.head())\n",
    "    print(f\"训练集处理后形状: {df_train_processed.shape}\")\n",
    "    print(df_train_processed.info(verbose=True, show_counts=True))\n",
    "\n",
    "\n",
    "    # Test data preprocessing\n",
    "    print(\"\\n--- 测试数据预处理 ---\")\n",
    "    df_test_processed = preprocessor.preprocess(df_test, is_train=False)\n",
    "    print(df_test_processed.head())\n",
    "    print(f\"测试集处理后形状: {df_test_processed.shape}\")\n",
    "    print(df_test_processed.info(verbose=True, show_counts=True))\n",
    "\n",
    "    # Verify column consistency (excluding id/target columns)\n",
    "    train_cols_for_check = [col for col in df_train_processed.columns if col not in preprocessor.exclude_cols]\n",
    "    test_cols_for_check = [col for col in df_test_processed.columns if col not in preprocessor.exclude_cols]\n",
    "    print(f\"\\n训练集和测试集处理后的**特征**列名是否一致: {set(train_cols_for_check) == set(test_cols_for_check)}\")\n",
    "    # Note: Column order might differ, so check set equality or sort for list equality"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dfb5950e924e3b2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fca33e90a3473338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# https://grok.com/chat/b1655899-c5e5-469e-9dcc-dcb4fb931ab0\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InfinityHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Replace infinite values with NaN.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        logger.info(\"Handling infinite values\")\n",
    "        X = X.copy()\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        return X\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical variables (label encoding for binary, one-hot for multi-class).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.one_hot_columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.label_encoders = {}\n",
    "        self.one_hot_columns = set()\n",
    "        for col in X.columns:\n",
    "            if X[col].nunique() <= 2:  # Label encoding for binary\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                X[col] = self.label_encoders[col].fit_transform(X[col].fillna('missing'))\n",
    "            else:  # One-hot encoding for multi-class\n",
    "                X = pd.get_dummies(X, columns=[col], prefix=col, dummy_na=False)\n",
    "                self.one_hot_columns.update([c for c in X.columns if c.startswith(f\"{col}_\")])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        logger.info(\"Encoding categorical variables\")\n",
    "        for col in X.columns:\n",
    "            if col in self.label_encoders:  # Label encoding\n",
    "                X[col] = X[col].map(lambda x: x if x in self.label_encoders[col].classes_ else 'missing')\n",
    "                if 'missing' not in self.label_encoders[col].classes_:\n",
    "                    classes = list(self.label_encoders[col].classes_)\n",
    "                    classes.append('missing')\n",
    "                    self.label_encoders[col].classes_ = np.array(classes)\n",
    "                X[col] = self.label_encoders[col].transform(X[col])\n",
    "            else:  # One-hot encoding\n",
    "                if X[col].nunique() > 2:\n",
    "                    X = pd.get_dummies(X, columns=[col], prefix=col, dummy_na=False)\n",
    "                    X = X.reindex(columns=X.columns.union(self.one_hot_columns), fill_value=0)\n",
    "        return X\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.exclude_cols = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']\n",
    "        self.pipeline = None\n",
    "        self.num_cols = None\n",
    "        self.cat_cols = None\n",
    "\n",
    "    def _get_column_types(self, df):\n",
    "        \"\"\"Identify numerical and categorical columns, excluding specified columns.\"\"\"\n",
    "        self.num_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                        if col not in self.exclude_cols]\n",
    "        self.cat_cols = [col for col in df.select_dtypes(include=['object']).columns\n",
    "                        if col not in self.exclude_cols]\n",
    "        return self.num_cols, self.cat_cols\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess data using a scikit-learn pipeline.\n",
    "\n",
    "        :param df: Input DataFrame\n",
    "        :param is_train: Whether this is the training set\n",
    "        :return: Preprocessed DataFrame\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting preprocessing for {'training' if is_train else 'test'} data, shape: {df.shape}\")\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # Identify column types\n",
    "        num_cols, cat_cols = self._get_column_types(df_copy)\n",
    "\n",
    "        # Define pipeline for numerical and categorical columns\n",
    "        num_pipeline = Pipeline([\n",
    "            ('inf_handler', InfinityHandler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        cat_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', CategoricalEncoder())\n",
    "        ])\n",
    "\n",
    "        # Combine pipelines with ColumnTransformer\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', num_pipeline, num_cols),\n",
    "            ('cat', cat_pipeline, cat_cols),\n",
    "            ('passthrough', 'passthrough', self.exclude_cols +\n",
    "             [col for col in df_copy.columns if col not in num_cols + cat_cols])\n",
    "        ])\n",
    "\n",
    "        if is_train:\n",
    "            self.pipeline = preprocessor\n",
    "            result = self.pipeline.fit_transform(df_copy)\n",
    "        else:\n",
    "            if self.pipeline is None:\n",
    "                raise ValueError(\"Pipeline not fitted. Run on training data first.\")\n",
    "            result = self.pipeline.transform(df_copy)\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        all_cols = num_cols + cat_cols + self.exclude_cols + \\\n",
    "                   [col for col in df_copy.columns if col not in num_cols + cat_cols]\n",
    "        if is_train:\n",
    "            # Update cat_cols to include one-hot encoded columns\n",
    "            cat_encoder = self.pipeline.named_transformers_['cat'].named_steps['encoder']\n",
    "            all_cols = (num_cols +\n",
    "                       list(cat_encoder.one_hot_columns) +\n",
    "                       [col for col in cat_cols if col in cat_encoder.label_encoders] +\n",
    "                       self.exclude_cols +\n",
    "                       [col for col in df_copy.columns if col not in num_cols + cat_cols])\n",
    "        df_processed = pd.DataFrame(result, columns=all_cols, index=df_copy.index)\n",
    "        logger.info(f\"Preprocessing complete, final shape: {df_processed.shape}\")\n",
    "        return df_processed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    data_train = {\n",
    "        'SK_ID_CURR': [1, 2, 3, 4, 5, 6],\n",
    "        'Feature_Num1': [10.0, 20.0, np.nan, 40.0, 50.0, 60.0],\n",
    "        'Feature_Num2': [1.0, np.inf, 3.0, 4.0, -np.inf, 6.0],\n",
    "        'Feature_Cat1': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
    "        'Feature_Cat2': ['Yes', 'No', 'Yes', 'No', np.nan, 'Yes'],\n",
    "        'Feature_Cat3': ['X', 'Y', 'X', 'Z', 'Y', 'X'],\n",
    "        'Some_Other_Col': ['val1', 'val2', 'val3', 'val4', 'val5', 'val6'],\n",
    "        'TARGET': [0, 1, 0, 1, 0, 1]\n",
    "    }\n",
    "    df_train = pd.DataFrame(data_train)\n",
    "\n",
    "    data_test = {\n",
    "        'SK_ID_CURR': [7, 8, 9, 10],\n",
    "        'Feature_Num1': [70.0, np.nan, 90.0, 100.0],\n",
    "        'Feature_Num2': [7.0, 8.0, np.inf, 10.0],\n",
    "        'Feature_Cat1': ['B', 'D', 'C', 'A'],\n",
    "        'Feature_Cat2': ['No', 'Yes', 'No', 'Yes'],\n",
    "        'Feature_Cat3': ['Y', 'A', 'Z', 'Y'],\n",
    "        'Some_Other_Col': ['val7', 'val8', 'val9', 'val10'],\n",
    "        'TARGET': [0, 0, 1, 1]\n",
    "    }\n",
    "    df_test = pd.DataFrame(data_test)\n",
    "\n",
    "    preprocessor = Preprocessor()\n",
    "    print(\"\\n--- Training Data Preprocessing ---\")\n",
    "    df_train_processed = preprocessor.preprocess(df_train, is_train=True)\n",
    "    print(df_train_processed.head())\n",
    "    print(f\"Training set shape: {df_train_processed.shape}\")\n",
    "    print(df_train_processed.info(verbose=True, show_counts=True))\n",
    "\n",
    "    print(\"\\n--- Test Data Preprocessing ---\")\n",
    "    df_test_processed = preprocessor.preprocess(df_test, is_train=False)\n",
    "    print(df_test_processed.head())\n",
    "    print(f\"Test set shape: {df_test_processed.shape}\")\n",
    "    print(df_test_processed.info(verbose=True, show_counts=True))"
   ],
   "id": "a8ca0741a50d5411"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
